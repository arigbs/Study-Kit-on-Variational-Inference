<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quick Start Guide for Variational Inference</title>
    <style>
        body {
            font-family: 'Helvetica Neue', Arial, sans-serif; /* Clean, modern sans-serif */
            line-height: 1.7;
            margin: 0 auto;
            padding: 25px; /* Overall padding */
            max-width: 950px; /* Slightly wider for better content flow */
            background-color: #fcfcfc; /* Very light grey, almost white */
            color: #383838; /* Dark grey for primary text */
        }

        /* --- Headings --- */
        h1.main-title {
            color: #2a3b4d; /* Deep, muted blue */
            font-size: 2.6em;
            text-align: center;
            margin-top: 1em;
            margin-bottom: 1em;
            font-weight: 500; /* Medium weight */
            text-decoration: underline solid #2a3b4d;
	        text-decoration-thickness: 3px;
        }

        h2.section-title { /* Preface, Prerequisite Knowledge etc. */
            color: #3a506b; /* Slightly lighter deep blue */
            font-size: 2.1em;
            border-bottom: 2px solid #5c9ead; /* Muted teal border */
            padding-bottom: 0.35em;
            margin-top: 2.5em;
            margin-bottom: 1.2em;
            font-weight: 500;
        }

        h2.glossary-title, h2.exercises-title { /* For "Glossary", "Bonus Exercises" etc. */
            color: #1b4965; /* Darker, more distinct blue for major sections */
            font-size: 2.3em;
            text-align: left;
            border-bottom: 2px solid #1b4965;
            padding-bottom: 0.4em;
            margin-top: 3em;
            margin-bottom: 1.5em;
            font-weight: 500;
        }

        h3.glossary-entry-title {
            color: #0077b6; /* Clear, accessible blue for entry titles */
            font-size: 1.8em;
            margin-top: 2.2em; /* More space before each entry */
            margin-bottom: 0.7em;
            font-weight: 500;
            border-bottom: none;
        }

        h3.tutorial-subtitle { /* "SECTION X: Overview..." */
            color: #1d70a2; /* Stronger blue for tutorial sections */
            font-size: 2em;
            margin-top: 2.2em;
            margin-bottom: 1.1em;
            font-weight: 500;
            border-bottom: 2px solid #64b6ac; /* Complementary teal */
            padding-bottom: 0.3em;
        }

        h4.tutorial-part-title { /* "Part X: Building Blocks..." */
            color: #466078; /* Muted blue-grey */
            font-size: 2.0em; /* Increased from 1.6em as per your edit */
            font-weight: 500;
            border-bottom: 1px solid #a9d6e5; /* Light blue border */
            padding-bottom: 0.3em;
            margin-top: 2em;
            margin-bottom: 0.9em;
        }
        
        h3.tutorial-stage-title { /* "Stage X: Decoding..." - Changed from h5 to h3 */
            color: #0b4a6b; /* Darker blue for stage titles */
            font-size: 2.0em; /* Increased from 1.4em as per your edit */
            font-weight: 500;
            margin-top: 1.8em;
            margin-bottom: 0.7em;
            border-bottom: 1px solid #90e0ef; /* Re-added border as per your edit */
            padding-bottom: 0.2em; /* Re-added padding as per your edit */
        }

        /* --- Table of Contents --- */
        .toc {
            background-color: #f4f7f9; /* Light, neutral background */
            border: 1px solid #d8e2e7; /* Soft border */
            padding: 15px 25px; /* Adjusted padding */
            margin-top: 1.5em;
            margin-bottom: 3em; /* Changed from 3.5em */
            border-radius: 6px;
        }
        .toc h2 { /* TOC specific title */
            color: #2a3b4d;
            font-size: 1.6em; /* Slightly smaller TOC title */
            margin-top: 0;
            margin-bottom: 0.8em; /* Reduced space after TOC title */
            border-bottom: 1px solid #cad3db;
            padding-bottom: 0.4em;
            font-weight: 500;
        }
        .toc ul {
            list-style-type: none;
            padding-left: 0;
            margin-bottom: 0; /* Remove bottom margin from ul to control spacing via li */
        }
        .toc ul ul { /* Nested lists in TOC */
            padding-left: 20px; /* Slightly reduced indentation for nested lists */
            margin-top: 0.2em; /* Tighter spacing for nested lists */
        }
        .toc li {
            margin-bottom: 0.35em; /* Significantly reduced space between TOC items */
        }
        .toc a {
            text-decoration: none;
            color: #005f8ddc;
            font-weight: 400;
            transition: color 0.2s ease-in-out;
            display: block; /* Helps with click area if padding is added to 'a' */
            padding: 2px 0; /* Minimal vertical padding for 'a' if needed, or remove */
        }
        .toc a:hover {
            text-decoration: none;
            color: #003c5a;
        }
        .toc > ul > li > a { /* Top-level TOC items */
            font-weight: 500;
            font-size: 0.95em; /* Reduced font size for top-level items */
        }
        .toc ul ul a { /* Nested links */
            font-size: 0.88em; /* Further reduced font size for nested items */
            font-weight: 400; /* Keep nested items regular weight */
        }


        /* --- Entry Specifics & General Text --- */
        .entry-section-title, .tutorial-entry-section-title {
            font-weight: 600; /* Bolder for these sub-titles */
            color: #4a5568;
            display: block;
            margin-top: 1.2em; /* More space above */
            margin-bottom: 0.5em;
            font-size: 1.05em; /* Slightly larger */
        }
        p, li {
            font-size: 1em;
            margin-bottom: 0.8em; /* Consistent spacing */
            color: #454f5b; /* Slightly softer text color */
        }
        ul, ol { /* General ul, ol styling */
            margin-left: 22px;
            margin-bottom: 1.2em;
        }
        ol li, ul li { /* Applies to list items not specifically overridden by TOC or reference-list */
            color: #454f5b;
            margin-bottom: 0.6em; /* Spacing between list items */
        }
        /* Specific ol for exercises within tutorial stages */
        .tutorial-content ol {
            margin-left: 25px; /* Ensure exercises in tutorial are indented */
        }
        .tutorial-content ol li {
             margin-bottom: 0.8em; /* Spacing for exercise list items */
        }


        /* --- Code and Preformatted Text --- */
        code { /* Inline code */
            font-family: 'Menlo', 'Consolas', 'Liberation Mono', Courier, monospace; /* Prioritize common monospace fonts */
            background-color: #eef2f5; /* Light grey background */
            padding: 0.2em 0.45em;
            border-radius: 4px;
            font-size: 0.92em; /* Slightly smaller, more refined */
            color: #2d3748;
            border: 1px solid #dfe6eb; /* Subtle border for inline code */
        }
        pre { /* Code blocks */
            font-family: 'Menlo', 'Consolas', 'Liberation Mono', Courier, monospace;
            background-color: #282c34; /* Common dark theme for code */
            color: #abb2bf; /* Common light text for dark theme */
            padding: 16px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            border-radius: 5px;
            margin-top: 0.8em;
            margin-bottom: 1.2em;
            font-size: 0.95em;
            line-height: 1.6; /* Improved line height for code */
            border: 1px solid #21252b; /* Darker border for code block */
        }

        /* --- Links --- */
        .reference-list ul {
            list-style-type: none;
            padding-left: 0;
        }
        .reference-list li {
            margin-bottom: 0.4em;
        }
        /* General link styling in content */
        a {
            color: #006ba6; /* A clear, professional blue */
            text-decoration: none;
            font-weight: 500;
            transition: color 0.2s ease, text-decoration 0.2s ease;
        }
        a:hover {
            color: #004a73; /* Darker blue on hover */
            text-decoration: underline;
        }
        /* Links in bonus exercises might be less prominent if they are just for reference */
        #bonus-exercises a, #bonus-exercises-answers a {
            color: #006ba6; /* Consistent link color */
            font-weight: 400; /* Regular weight to blend better if they are frequent */
        }
        #bonus-exercises a:hover, #bonus-exercises-answers a:hover {
            color: #004a73;
            text-decoration: underline;
        }


        /* --- Notes and Dividers --- */
        .prerequisite-note {
            background-color: #e9f5f9; /* Lighter, calmer blue */
            border-left: 5px solid #64b6ac; /* Teal accent for contrast */
            padding: 18px 22px;
            margin-top: 1.8em;
            margin-bottom: 1.8em;
            border-radius: 4px;
        }
        .prerequisite-note p {
            margin-bottom: 0.4em;
            color: #334e68; /* Readable text within note */
        }
        .prerequisite-note p:last-child {
            margin-bottom: 0;
        }
        .prerequisite-note strong {
            color: #1b4965; /* Darker blue for emphasis in note */
            font-weight: 600;
        }

        hr.tutorial-divider {
            border: 0;
            height: 1px;
            background-color: #d1d9e0; /* Simple light grey divider */
            margin-top: 2.5em;
            margin-bottom: 2.5em;
        }

        /* --- Bold and Italic from Tutorial --- */
        strong.tutorial-strong {
            font-weight: 600;
            color: #2a3b4d; /* Consistent dark blue */
        }
        i.tutorial-italic {
            font-style: italic;
            color: #5a728a; /* Muted italic color */
        }

        /* --- Bonus Exercises Spacing --- */
        #bonus-exercises ol li, #bonus-exercises-answers ol li {
            margin-bottom: 1em;
            font-size: 1em;
            line-height: 1.7;
        }

    </style>
</head>
<body>
<div class="main-container">

    <h1 class="main-title">Quick Start Guide for Variational Inference</h1>

    <div class="toc">
        <h2>Table of Contents</h2>
        <ul>
            <li><a href="#preface">Preface</a>
                <ul>
                    <li><a href="#how-to-use-guide">How to Use This Quick Start Guide</a></li>
                    <li><a href="#how-to-use-exercises">How to Use the Exercises</a></li>
                </ul>
            </li>
            <li><a href="#prerequisite-knowledge">Prerequisite Knowledge</a></li>
            <li><a href="#how-to-approach-guide">How to Approach this Guide as a Learner</a></li>
            <li><a href="#gentle-stroll-tutorial">A Gentle Stroll Through the Math of Generative Models (Tutorial)</a>
                <ul>
                    <li><a href="#gentle-stroll-overview">SECTION 1: Overview of Main Concepts</a>
                        <ul>
                            <li><a href="#gentle-stroll-part1">Part 1: The Building Blocks</a></li>
                            <li><a href="#gentle-stroll-part2">Part 2: Key Mathematical Operations</a></li>
                            <li><a href="#gentle-stroll-part3">Part 3: Putting it Together</a></li>
                            <li><a href="#gentle-stroll-part4">Part 4: Tips for Grokking Math</a></li>
                        </ul>
                    </li>
                    <li><a href="#gentle-stroll-staged-ascent">SECTION 2: The Path: Staged Ascent to Mathematical Fluency (Exercises)</a>
                        <ul>
                            <li><a href="#gentle-stroll-stage0">Stage 0: Pre-Flight Check</a></li>
                            <li><a href="#gentle-stroll-stage1">Stage 1: Decoding the Alphabet & Basic Grammar</a></li>
                            <li><a href="#gentle-stroll-stage2">Stage 2: Forming Simple Sentences</a></li>
                            <li><a href="#gentle-stroll-stage3">Stage 3: Following Simple Paragraphs</a></li>
                            <li><a href="#gentle-stroll-stage4">Stage 4: Deconstructing Core Arguments</a></li>
                            <li><a href="#gentle-stroll-stage5">Stage 5: Starting to Write</a></li>
                            <li><a href="#gentle-stroll-stage6">Stage 6: Fluency and Creative Construction</a></li>
                            <li><a href="#gentle-stroll-stage7">Stage 7: Mathematical Artistry & Innovation</a></li>
                            <li><a href="#gentle-stroll-stage8">Stage 8: Mentorship, Leadership, and Shaping the Field</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="#glossary-terms-heading">Glossary of Terms</a></li>
            <li><a href="#reference-list-title">Reference List (Index of Glossary Terms)</a></li>
            <li><a href="#bonus-exercises">Bonus Reinforcement Exercises</a></li>
            <li><a href="#bonus-exercises-answers">Answers to Bonus Reinforcement Exercises</a></li>
        </ul>
    </div>

    <h2 class="section-title" id="preface">Preface</h2>
    <p>This Quick Start Guide is a foundational resource for learners venturing into the realms of probabilistic generative models and variational inference. It combines a detailed <a href="#glossary-terms-heading">Glossary of Terms</a> with a comprehensive tutorial, <a href="#gentle-stroll-tutorial">"A Gentle Stroll Through the Math of Generative Models,"</a> which includes staged exercises. This combined resource is designed to help you build the essential vocabulary, conceptual understanding, and mathematical intuition needed to follow along in a course on generative models / variational inference and similar advanced studies in machine learning.</p>

    <p>This Guide was generated with the use of Large Language Models by <b>Ayodele Arigbabu</b>, based on notes taken while attending the PhD course: <b>Generative Models</b> (<a href="https://programmeinfo.bi.no/nb/course/DRE-7053/" target="_blank">DRE 7053</a>) at BI Norwegian Business School, taught by <b>Dr. Rogelio Andrade Mancisidor</b> in Spring 2025 in Oslo. It therefore may serve as a useful preparatory document for students taking the course, or similar courses. The material may be useful for those with basic backgrounds in mathematics and statistics who need to brush up in a targeted way to be able to follow along with the derivations pertinent to variable inference and also get some familiarity with the core concepts behind Variational Autoencoders. The material could also be useful for students more grounded in mathematics and statistics, for getting a quick overview of the 'terrain', especially the core concepts behind Variational Autoencoders; and perhaps refreshing some of their previous knowledge. Beyond preparation, it could also be a good revision tool, for reinforceing learning from the course. This document does not cover python programming though it is a typical requirement for working with Variational Autoencoders.</p>
    <p>This Guide may serve as complementary learning material to the <a href="Tensorflow_Tutorials_For_Variational_Inference.html" target="_blank">Tensorflow Tutorials for Variational Inference</a>, a tutorial series designed to ground the knowledge gained from this guide, in hands-on, practical implementation through practical application using Tensorflow and python coding.</p>
    <p>The document may be freely shared, remixed and used as found appropriate but no guarantees are to be assumed or implied about its usefulness 😊. </p>

    <span class="entry-section-title">What are generative models?</span>
    <p>Generative models are algorithms that learn the underlying patterns of data so well that they can create new, realistic data samples—like generating new images, texts, or sounds. They are central to modern AI, powering applications from image synthesis to data augmentation.</p>

    <span class="entry-section-title">Why are VAEs important?</span>
    <p>Variational autoencoders (VAEs) are a type of generative model that use the mathematics of probability to learn compressed, meaningful representations of data (called latent variables). VAEs are powerful because they combine neural networks with probabilistic reasoning, allowing for both efficient data generation and principled handling of uncertainty.</p>

    <span class="entry-section-title">The role of probabilistic statistics:</span>
    <p>All of this is built on a foundation of probability theory and statistics. Concepts like probability distributions, expectations, and likelihoods are not just mathematical formalities—they are the language that allows us to reason about data, uncertainty, and learning in a rigorous way. The <a href="#gentle-stroll-tutorial">tutorial section</a> aims to make this language more accessible.</p>

    <span class="entry-section-title">There may be errors:</span>
    <p>This Quick Start Guide has been generated with large language models, errors may exist in its content, users should double check with other sources if anything here seems inconsistent or inaccurate.</p>

    <h3 class="tutorial-subtitle" id="how-to-use-guide">How to Use This Quick Start Guide</h3>
    <p>This guide is structured to support your learning journey progressively:</p>
    <ul>
        <li><strong>Start with the <a href="#gentle-stroll-tutorial">"Gentle Stroll" Tutorial:</a></strong> If you are new to the mathematical notation or want to build intuition, begin with this section. It breaks down core mathematical concepts and notation and includes staged exercises to build your skills.</li>
        <li><strong>Use the <a href="#glossary-terms-heading">Glossary of Terms:</a></strong> As you work through your main learning material or the tutorial, use the glossary as both a reference for unfamiliar terms and a study guide to reinforce your learning and clarify new ideas. For each entry, you’ll find a concise definition, a brief explanation, and a more detailed discussion.</li>
        <li><strong>Engage with the <a href="#gentle-stroll-staged-ascent">Staged Exercises</a> within the Tutorial:</strong> These are designed to incrementally build your mathematical fluency, from basic symbol recognition to understanding and even performing simple derivations.</li>
        <li><strong>Challenge yourself with the <a href="#bonus-exercises">Bonus Reinforcement Exercises:</a></strong> After gaining some familiarity, use these to test and reinforce your understanding of various concepts from both the tutorial and the glossary. Answers are provided for self-assessment.</li>
    </ul>

    <h3 class="tutorial-subtitle" id="how-to-use-exercises">How to Use the Exercises</h3>
    <p>The exercises in this guide are designed to be an active learning tool. Here’s how to make the most of them:</p>
    <ul>
        <li><strong class="tutorial-strong">Work Sequentially:</strong> The <a href="#gentle-stroll-staged-ascent">staged exercises</a> within the "Gentle Stroll" tutorial are designed to build skills incrementally. It's recommended to go through them in order. The <a href="#bonus-exercises">bonus exercises</a> can be tackled more randomly once you have a foundational understanding.</li>
        <li><strong class="tutorial-strong">Pencil and Paper:</strong> Actively work through the problems. Writing things down, manipulating equations, and sketching concepts helps solidify understanding far more than passive reading.</li>
        <li><strong class="tutorial-strong">Refer to the Tutorial and Glossary:</strong> The exercises are designed to be used in conjunction with the main tutorial content and the glossary. If a symbol or concept is unclear, look it up! Hyperlinks are provided in the bonus exercises to relevant glossary terms.</li>
        <li><strong class="tutorial-strong">Don't Aim for Perfection Immediately:</strong> It's okay to get stuck. The process of struggling with a problem and then figuring it out (perhaps after consulting resources) is a crucial part of learning.</li>
        <li><strong class="tutorial-strong">Check Your Answers:</strong> For the <a href="#bonus-exercises">bonus exercises</a>, answers are provided. Use them to check your work and understand any mistakes. For the staged exercises in the tutorial, the goal is often to justify steps or explain concepts, which builds understanding directly.</li>
        <li><strong class="tutorial-strong">Focus on Intuition:</strong> While mathematical correctness is important, always try to connect the symbols and derivations back to the underlying concepts and intuitions. What story is the math telling?</li>
    </ul>

    <h2 class="section-title" id="prerequisite-knowledge">Prerequisite Knowledge for Using this Guide</h2>
    <p>The prerequisites listed below are intended to ensure you can understand the definitions, explanations, and exercises <em>within this guide itself</em>. </p>
    <div class="prerequisite-note">
        <p><strong>Important Note on Course Prerequisites:</strong> While the knowledge below is sufficient for this guide, successfully following along and deeply understanding a full course on generative models and variational inference will typically require additional foundational knowledge. This often includes a more solid grounding in calculus (especially multivariate), linear algebra, advanced probability and statistical methods, and the basics of artificial intelligence, machine learning concepts (e.g., optimization, model training), and neural networks.</p>
    </div>

    <ol>
        <li><strong>Basic Probability and Statistics Vocabulary</strong>
            <ul>
                <li>Understanding what a random variable is (e.g., the outcome of a dice roll).</li>
                <li>Familiarity with the idea of probability as a measure of how likely something is.</li>
                <li>Knowing the difference between discrete (e.g., coin flips) and continuous (e.g., height) variables.</li>
            </ul>
        </li>
        <li><strong>High School-Level Algebra</strong>
            <ul>
                <li>Comfort with algebraic expressions, basic equations, and manipulating terms.</li>
                <li>Familiarity with exponents and logarithms (knowing that a logarithm is the inverse of an exponent).</li>
            </ul>
        </li>
        <li><strong>Basic Concepts of Functions</strong>
            <ul>
                <li>Understanding what a function is (input-output relationship).</li>
                <li>Knowing what it means to evaluate a function at a point.</li>
            </ul>
        </li>
        <li><strong>Introductory Calculus (Nice to Have for Guide, More Important for Course)</strong>
            <ul>
                <li>Some exposure to the idea of integration (area under a curve) and differentiation (rate of change), though not required in depth for understanding the guide's explanations.</li>
                <li>Recognizing the notation for integrals (∫) and derivatives.</li>
            </ul>
        </li>
        <li><strong>General Scientific Literacy</strong>
            <ul>
                <li>Comfort with reading structured definitions, explanations, and following logical arguments.</li>
                <li>Familiarity with scientific notation and basic mathematical symbols (e.g., ∑ for sum, P(x) for probability of x), etc.</li>
            </ul>
        </li>
        <li><strong>Awareness of What a Model Is</strong>
            <ul>
                <li>Knowing that a model is a simplified mathematical or computational representation of a real-world process.</li>
            </ul>
        </li>
    </ol>

    <span class="entry-section-title">What You Do Not Need (for this Guide)</span>
    <ul>
        <li>You do not need to know how to implement models in code to use this guide effectively, though coding is essential for practical application.</li>
        <li>You do not need an <em>existing</em> advanced background in statistics, deep learning, or Bayesian inference for the guide itself – key foundational concepts from these fields are introduced and explained as needed.</li>
        <li>You do not need to be able to derive all formulas from scratch to understand the explanations and work through many of the exercises; the focus is on building intuition and the ability to follow derivations.</li>
    </ul>

    <h2 class="section-title" id="how-to-approach-guide">How to Approach this Guide as a Learner</h2>
    <ul>
        <li><strong>Start with the <a href="#gentle-stroll-tutorial">Tutorial</a>:</strong> If the math feels intimidating, the "Gentle Stroll" section is designed to ease you in.</li>
        <li><strong>Iterate Between Sections:</strong> Don’t feel you need to master one section before moving to another. You might read a bit of the tutorial, then look up terms in the glossary, then try some exercises.</li>
        <li><strong>Focus on Concepts:</strong> Especially when first encountering terms in the <a href="#glossary-terms-heading">glossary</a> or equations in the <a href="#gentle-stroll-tutorial">tutorial</a>, prioritize understanding the underlying intuition and the relationships between concepts over memorizing every detail of a formula.</li>
        <li><strong>Use the “Further Explanation” Sections in the Glossary:</strong> These are designed to bridge the gap between basic knowledge and the more advanced concepts you might encounter in a course.</li>
        <li><strong>Supplement as Needed:</strong> If you find a term or concept confusing even after consulting this guide, a quick online search for alternative explanations, videos, or examples can often provide additional clarity.</li>
    </ul>
    <p><strong>In summary:</strong> A learner with high school-level math, basic probability vocabulary, and general scientific literacy will be able to use and benefit from this guide. It is designed to build up more advanced understanding as you go, so you don’t need to be an expert to start using it effectively.</p>

    <h2 class="section-title" id="gentle-stroll-tutorial">A Gentle Stroll Through the Math of Generative Models (Tutorial)</h2>
    <p><strong class="tutorial-strong">Purpose:</strong> To help you feel more comfortable reading and understanding the mathematical notation and basic derivations common in materials about generative models and variational inference. We'll focus on <i class="tutorial-italic">intuition</i> and <i class="tutorial-italic">how to read the symbols</i>, not on rigorous proofs.</p>
    <p><strong class="tutorial-strong">Assumed Starting Point:</strong></p>
    <ul>
        <li><p>You know what a variable is (like x in algebra).</p></li>
        <li><p>You're familiar with basic arithmetic (+, -, ×, ÷).</p></li>
        <li><p>You have a vague idea of probability (e.g., a coin flip has a 50% chance of heads).</p></li>
        <li><p>You've seen functions like f(x) = 2x before.</p></li>
        <li><p>High-school algebra (exponents, maybe a little bit about logarithms).</p></li>
    </ul>
    <p><strong class="tutorial-strong">Let's Begin!</strong></p>
    <hr class="tutorial-divider" />

<div class="tutorial-content"> <!-- Added a wrapper for tutorial content specific styling -->
    <h3 class="tutorial-subtitle" id="gentle-stroll-overview">SECTION 1: Overview of Main Concepts</h3>
    <hr class="tutorial-divider" />

    <h4 class="tutorial-part-title" id="gentle-stroll-part1">Part 1: The Building Blocks - Variables, Parameters, and Basic Notation</h4>
    <ol>
        <li><p><strong class="tutorial-strong">Variables (The Things That Change):</strong></p>
        <ul>
            <li><p><code class="tutorial-code">x</code>: Often represents <strong class="tutorial-strong">observed data</strong>. Think of it as a single data point (e.g., one image, one sentence, one customer's height).</p></li>
            <li><p><code class="tutorial-code">z</code>: Often represents <strong class="tutorial-strong">latent (hidden) variables</strong>. These are things we <i class="tutorial-italic">don't</i> observe directly but believe influence our data x (e.g., the "style" of an image, the "topic" of a sentence).</p></li>
            <li><p><strong class="tutorial-strong">Random Variables:</strong> When x or z can take on different values with certain probabilities, they are called random variables. The outcome of a dice roll is a random variable.</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Parameters (The Knobs of Our Model):</strong></p>
        <ul>
            <li><p><code class="tutorial-code">θ</code> (theta), <code class="tutorial-code">ϕ</code> (phi): These Greek letters often represent <strong class="tutorial-strong">parameters</strong> of our models. Think of parameters as settings or knobs that we "tune" during learning to make our model fit the data well.</p>
            <ul>
                <li><p>Example: If we model heights with a bell curve (Gaussian distribution), θ might include the <i class="tutorial-italic">mean</i> (average height) and <i class="tutorial-italic">standard deviation</i> (how spread out the heights are).</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Probability Notation (The Language of Chance):</strong></p>
        <ul>
            <li><p><code class="tutorial-code">P(A)</code>: "The probability of event A happening."</p>
            <ul>
                <li><p>Example: P(Rain) = 0.3 means there's a 30% chance of rain.</p></li>
            </ul>
            </li>
            <li><p><code class="tutorial-code">P(X=x)</code> or <code class="tutorial-code">P(x)</code>: "The probability that the random variable X takes on the specific value x."</p>
            <ul>
                <li><p>Example: For a fair die, P(DieRoll = 3) = 1/6.</p></li>
            </ul>
            </li>
            <li><p><code class="tutorial-code">p(x)</code> (lowercase 'p'): Often used for <strong class="tutorial-strong">probability density functions (PDFs)</strong> when x is a continuous variable (like height). It's not exactly "probability <i class="tutorial-italic">at</i> a point" but "probability <i class="tutorial-italic">density</i> around a point." Higher p(x) means values around x are more likely.</p></li>
            <li><p><code class="tutorial-code">P(A | B)</code>: "The probability of event A happening, <strong class="tutorial-strong">given that</strong> event B has already happened." This is a <strong class="tutorial-strong">conditional probability</strong>.</p>
            <ul>
                <li><p>Example: P(Umbrella | Rain) is the probability you have an umbrella, given that it's raining (likely high!). P(Umbrella | Sunny) is likely low.</p></li>
                <li><p>In models: p(x | z, θ) could mean "the probability (density) of observing data x, given that the latent variable is z and our model parameters are θ."</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Distributions (The Recipes for Generating Data):</strong></p>
        <ul>
            <li><p>A probability distribution tells us how likely all the different outcomes of a random variable are.</p></li>
            <li><p><strong class="tutorial-strong">Gaussian (Normal) Distribution N(μ, σ²)</strong>: The famous "bell curve."</p>
            <ul>
                <li><p>μ (mu): The mean (center of the bell).</p></li>
                <li><p>σ² (sigma squared): The variance (how spread out the bell is; σ is the standard deviation).</p></li>
                <li><p>Notation: If we say z ~ N(0, 1), it means "the latent variable z is drawn from a Gaussian distribution with a mean of 0 and a variance of 1." This is a common "prior" belief for z.</p></li>
            </ul>
            </li>
            <li><p><strong class="tutorial-strong">Categorical Distribution</strong>: Used when a variable can take one of K discrete categories (e.g., a die roll has K=6 categories). It's defined by the probabilities of each category: (p_1, p_2, ..., p_K).</p></li>
        </ul>
        </li>
    </ol>
    <hr class="tutorial-divider" />

    <h4 class="tutorial-part-title" id="gentle-stroll-part2">Part 2: Key Mathematical Operations &amp; Why They're Used</h4>
    <ol>
        <li><p><strong class="tutorial-strong">Logarithms (Especially Natural Log, ln or log):</strong></p>
        <ul>
            <li><p><strong class="tutorial-strong">Why logs? They're everywhere!</strong></p>
            <ol>
                <li><p><strong class="tutorial-strong">Turning Products into Sums:</strong> log(a × b) = log(a) + log(b).</p>
                <ul>
                    <li><p>Probabilities of <i class="tutorial-italic">independent</i> events are multiplied. If you have many data points, their joint probability is P(x1) × P(x2) × ... × P(xn). This can become a tiny number!</p></li>
                    <li><p>Taking the log gives: log(P(x1)) + log(P(x2)) + ... + log(P(xn)). Sums are easier to work with mathematically and numerically more stable (they don't become super tiny as easily).</p></li>
                </ul>
                </li>
                <li><p><strong class="tutorial-strong">Simplifying Calculations:</strong> The derivatives (for optimization) of log functions are often simpler.</p></li>
                <li><p><strong class="tutorial-strong">Measuring Information:</strong> In information theory, logs are fundamental (e.g., bits, nats).</p></li>
            </ol>
            </li>
            <li><p><strong class="tutorial-strong">log p(x)</strong>: This is the "log-probability" or "log-density." If p(x) is the likelihood, then log p(x) is the <strong class="tutorial-strong">log-likelihood</strong>. We often try to maximize this.</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Exponentials (exp( ) or e^( )):</strong></p>
        <ul>
            <li><p>The inverse of the natural logarithm (ln). So exp(ln(a)) = a and ln(exp(a)) = a.</p></li>
            <li><p>Often used to convert log-probabilities back into probabilities or to ensure something is positive (since exp(anything) is always positive).</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Summation (∑ - Sigma):</strong></p>
        <ul>
            <li><p>Means "add things up."</p></li>
            <li><p>∑_{i=1}^{N} x_i = x_1 + x_2 + ... + x_N (Sum x_i from i=1 up to N).</p></li>
            <li><p>Used for discrete random variables:</p>
            <ol>
                <li><p>To ensure total probability is 1: ∑ P(X=x) = 1 (sum over all possible values x).</p></li>
                <li><p>To calculate <strong class="tutorial-strong">expectation</strong> (see below).</p></li>
            </ol>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Integration (∫ - Integral):</strong></p>
        <ul>
            <li><p>The continuous version of summation. Think of it as finding the "area under a curve."</p></li>
            <li><p>∫ p(x) dx (Integral of p(x) with respect to x).</p></li>
            <li><p>Used for continuous random variables:</p>
            <ol>
                <li><p>To ensure total probability (density) integrates to 1: ∫ p(x) dx = 1 (integrate over all possible values x).</p></li>
                <li><p>To calculate <strong class="tutorial-strong">expectation</strong> (see below).</p></li>
            </ol>
            </li>
            <li><p><strong class="tutorial-strong">Don't panic if you haven't done calculus!</strong> For now, just understand that when you see ∫, it means "summing up" in a continuous way, often over all possible values of a variable.</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Expectation (E[ ]):</strong></p>
        <ul>
            <li><p>The "average" or "expected value" of a random variable, weighted by its probabilities.</p></li>
            <li><p>If X is a random variable:</p>
            <ol>
                <li><p>Discrete: E[X] = ∑ x * P(X=x) (multiply each value x by its probability, then sum them all up).</p></li>
                <li><p>Continuous: E[X] = ∫ x * p(x) dx (multiply each value x by its probability density, then integrate).</p></li>
            </ol>
            </li>
            <li><p><strong class="tutorial-strong">E_{x ~ p(x)}[f(x)] or E_{p(x)}[f(x)]</strong>: This looks scarier but means: "The expected value of the function f(x), where the values of x are drawn according to the probability distribution p(x)."</p>
            <ol>
                <li><p>It's just E[f(X)] if X follows p(x).</p></li>
                <li><p>So, discrete: ∑ f(x) * P(X=x). Continuous: ∫ f(x) * p(x) dx.</p></li>
                <li><p><strong class="tutorial-strong">Intuition:</strong> If you were to draw many samples of x from p(x), calculate f(x) for each, and then average all those f(x) values, you'd get E_{p(x)}[f(x)].</p></li>
            </ol>
            </li>
        </ul>
        </li>
    </ol>
    <hr class="tutorial-divider" />

    <h4 class="tutorial-part-title" id="gentle-stroll-part3">Part 3: Putting it Together - Core Concepts in Math Form</h4>
    <ol>
        <li><p><strong class="tutorial-strong">Likelihood P(data | θ) or p(data | θ):</strong></p>
        <ul>
            <li><p>"How likely is the data we observed, given a specific choice of model parameters θ?"</p></li>
            <li><p>If we have N independent data points x_1, ..., x_N:</p>
            <ul>
                <li><p>P(data | θ) = P(x_1 | θ) × P(x_2 | θ) × ... × P(x_N | θ) = Π_{i=1}^{N} P(x_i | θ)</p></li>
                <li><p>(Π - capital Pi means "product," similar to ∑ for sum).</p></li>
            </ul>
            </li>
            <li><p><strong class="tutorial-strong">Log-Likelihood log P(data | θ):</strong></p>
            <ul>
                <li><p>log P(data | θ) = ∑_{i=1}^{N} log P(x_i | θ) (See? Product became a sum!).</p></li>
                <li><p><strong class="tutorial-strong">Goal of learning (often):</strong> Find θ that <i class="tutorial-italic">maximizes</i> this log-likelihood. This is called <strong class="tutorial-strong">Maximum Likelihood Estimation (MLE)</strong>.</p></li>
                <li><p>argmax_θ log P(data | θ) means "find the θ that makes log P(data | θ) as large as possible."</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Bayes' Theorem (Conceptual View):</strong></p>
        <ul>
            <li><p>P(θ | data) = [P(data | θ) * P(θ)] / P(data)</p></li>
            <li><p>In words: <strong class="tutorial-strong">Posterior = (Likelihood × Prior) / Evidence</strong></p>
            <ul>
                <li><p>P(θ): <strong class="tutorial-strong">Prior</strong>. Our belief about θ <i class="tutorial-italic">before</i> seeing data.</p></li>
                <li><p>P(data | θ): <strong class="tutorial-strong">Likelihood</strong>. How well θ explains the data.</p></li>
                <li><p>P(θ | data): <strong class="tutorial-strong">Posterior</strong>. Our updated belief about θ <i class="tutorial-italic">after</i> seeing data.</p></li>
                <li><p>P(data): <strong class="tutorial-strong">Evidence (or Marginal Likelihood)</strong>. P(data) = ∫ P(data | θ)P(θ) dθ. This is often very hard to calculate!</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Objective Functions (What We Optimize):</strong></p>
        <ul>
            <li><p>A function we want to maximize or minimize to train our model.</p></li>
            <li><p>The log-likelihood is a common objective function (we want to maximize it).</p></li>
            <li><p>In VAEs, the <strong class="tutorial-strong">ELBO (Evidence Lower Bound)</strong> is the objective function.</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">KL Divergence D_KL(P || Q):</strong></p>
        <ul>
            <li><p>"A measure of how different distribution Q is from distribution P."</p></li>
            <li><p>D_KL(P || Q) = E_{x ~ P(x)}[log(P(x) / Q(x))] = ∫ P(x) log(P(x) / Q(x)) dx (for continuous).</p></li>
            <li><p>It's always ≥ 0. It's 0 only if P and Q are identical.</p></li>
            <li><p><strong class="tutorial-strong">Not symmetric:</strong> D_KL(P || Q) ≠ D_KL(Q || P).</p></li>
            <li><p><strong class="tutorial-strong">In VAEs:</strong> You'll see terms like D_KL(q(z|x) || p(z)).</p>
            <ul>
                <li><p>q(z|x) is our "approximate posterior" (what we think z is, given x).</p></li>
                <li><p>p(z) is our "prior" (what we thought z was before seeing x, often N(0,1)).</p></li>
                <li><p>This term tries to make our learned q(z|x) not too different from our simple prior p(z). It acts as a <strong class="tutorial-strong">regularizer</strong>.</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">The ELBO (Evidence Lower Bound) - A Glimpse:</strong></p>
        <ul>
            <li><p>The log-likelihood log p(x) (also called log evidence) is often intractable.</p></li>
            <li><p>The ELBO is a <i class="tutorial-italic">lower bound</i> on it: log p(x) ≥ ELBO.</p></li>
            <li><p>ELBO = E_{q(z|x)}[log p(x|z)] - D_KL(q(z|x) || p(z))</p></li>
            <li><p>Let's break it down intuitively:</p>
            <ul>
                <li><p>E_{q(z|x)}[log p(x|z)]: The <strong class="tutorial-strong">"reconstruction term."</strong></p>
                <ul>
                    <li><p>q(z|x): Our encoder. Given data x, it gives us a distribution for latent z.</p></li>
                    <li><p>p(x|z): Our decoder. Given a latent z, it gives us a distribution for data x.</p></li>
                    <li><p>So, log p(x|z) is how well z reconstructs x.</p></li>
                    <li><p>The expectation E_{q(z|x)}[...] means "on average, drawing z from our encoder's output, how well does it reconstruct x?" We want this to be high.</p></li>
                </ul>
                </li>
                <li><p>D_KL(q(z|x) || p(z)): The <strong class="tutorial-strong">"regularization term."</strong></p>
                <ul>
                    <li><p>We want this to be <i class="tutorial-italic">low</i>, meaning our learned q(z|x) (from the encoder) shouldn't stray too far from our simple prior p(z).</p></li>
                </ul>
                </li>
            </ul>
            </li>
            <li><p>By maximizing the ELBO, we push log p(x) up, and we also make q(z|x) a good approximation of the true (but intractable) posterior p(z|x).</p></li>
        </ul>
        </li>
    </ol>
    <hr class="tutorial-divider" />

    <h4 class="tutorial-part-title" id="gentle-stroll-part4">Part 4: Tips for Grokking Math in Papers/Courses</h4>
    <ol>
        <li><p><strong class="tutorial-strong">Don't Try to Understand Everything at Once:</strong> It's okay if some parts are confusing. Focus on the main ideas first.</p></li>
        <li><p><strong class="tutorial-strong">Look for the "Story":</strong> What are the authors trying to model? What's the input, what's the output? What are they optimizing?</p></li>
        <li><p><strong class="tutorial-strong">Break Down Equations:</strong></p>
        <ul>
            <li><p>Identify the variables (x, z, θ).</p></li>
            <li><p>Identify the operations (∑, ∫, log, E[]).</p></li>
            <li><p>Identify known distributions (N(μ, σ²)).</p></li>
            <li><p>Try to "read" the equation like a sentence. E_{q(z|x)}[log p(x|z)] can be read as "the expectation, with respect to z drawn from q given x, of the log probability of x given z."</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Pay Attention to Subscripts and Superscripts:</strong> They often tell you <i class="tutorial-italic">what</i> something depends on or <i class="tutorial-italic">what distribution</i> an expectation is over.</p></li>
        <li><p><strong class="tutorial-strong">Connect to the Glossary:</strong> When you see a term, look it up!</p></li>
        <li><p><strong class="tutorial-strong">Work Through Simple Examples:</strong> If you see E[X], think about a dice roll: (1*1/6) + (2*1/6) + ... + (6*1/6) = 3.5.</p></li>
        <li><p><strong class="tutorial-strong">It's Iterative:</strong> You might read something, not get it, read more, then come back and it makes more sense.</p></li>
    </ol>
    <hr class="tutorial-divider" />
    <p><strong class="tutorial-strong">This is a starting point.</strong> The key is not to be intimidated. Mathematical notation is just a concise language. With practice, and by connecting symbols to concepts, it will become more familiar. Good luck!</p>

    <h3 class="tutorial-subtitle" id="gentle-stroll-staged-ascent">SECTION 2: The Path: Staged Ascent to Mathematical Fluency (Exercises)</h3>
    <p><strong class="tutorial-strong">The Grand Vision: From Novice to Derivation Dynamo</strong></p>
    <p>Alright, let's embark on this journey! My goal isn't just to teach you, but to transform you into someone who sees mathematical notation not as a barrier, but as a powerful, precise language for expressing beautiful ideas. We'll go from "What does that squiggly line mean?" to "Ah, I see how they derived that, and I could probably do it too!"</p>
    <p>This plan is designed to be a self-guided expedition. Your companions will be:</p>
    <ol>
        <li><p><strong class="tutorial-strong">This Tutorial Path.</strong></p></li>
        <li><p><strong class="tutorial-strong">The Glossary</strong> (your dictionary and concept map).</p></li>
        <li><p><strong class="tutorial-strong">Pencil and Paper</strong> (your thinking tools – actively working things out is crucial!).</p></li>
        <li><p><strong class="tutorial-strong">Resilience and Curiosity</strong> (your fuel).</p></li>
    </ol>
    <p><strong class="tutorial-strong">Your Mindset: The Resilient Explorer</strong></p>
    <ul>
        <li><p><strong class="tutorial-strong">Embrace Confusion:</strong> Confusion is not failure; it's the state <i class="tutorial-italic">before</i> understanding. It's a sign you're wrestling with something new.</p></li>
        <li><p><strong class="tutorial-strong">Celebrate Small Wins:</strong> Every symbol deciphered, every line of a derivation understood, is progress.</p></li>
        <li><p><strong class="tutorial-strong">Be Patient:</strong> This isn't a race. Deep understanding takes time and repetition.</p></li>
        <li><p><strong class="tutorial-strong">Active Engagement is Key:</strong> Don't just read. Write, recalculate, question.</p></li>
    </ul>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage0">Stage 0: Pre-Flight Check (Mental Prep &amp; Tool Familiarization)</h3>
    <p><i class="tutorial-italic">(This stage is about mindset and getting comfortable with the idea of engaging with math)</i></p>
    <ul>
        <li><p><strong class="tutorial-strong">Objective:</strong> To feel ready and equipped to start, and to understand that math is a language.</p></li>
        <li><p><strong class="tutorial-strong">Activities:</strong></p>
        <ol>
            <li><p><strong class="tutorial-strong">Re-read:</strong> "A Gentle Stroll Through the Math..." (the previous tutorial). Don't worry about mastering it, just re-familiarize.</p></li>
            <li><p><strong class="tutorial-strong">Glossary Scan:</strong> Flip through your glossary. Pick 3-5 terms that look interesting or completely alien. Read their "Quick Description." Don't try to understand fully, just expose yourself to the vocabulary.</p></li>
            <li><p><strong class="tutorial-strong">Affirmation:</strong> Tell yourself: "I can learn this. Math is a language, and I can learn new languages."</p></li>
        </ol>
        </li>
        <li><p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> Many people feel "math anxiety." Recognize it if you have it, and know it's common and conquerable. We're not judging your past math experiences; we're building new, positive ones.</p></li>
    </ul>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage1">Stage 1: Decoding the Alphabet &amp; Basic Grammar</h3>
    <p><i class="tutorial-italic">(Focus: Recognizing symbols and their most basic, literal meanings)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To be able to look at a simple mathematical expression and identify individual symbols and their general roles (variable, parameter, operation).</p>
    <p><strong class="tutorial-strong">Core Concepts for this Stage (from "Gentle Stroll" Part 1 &amp; Glossary):</strong></p>
    <ul>
        <li>Variables: x (data), z (latent), y (labels/targets)</li>
        <li>Parameters: θ, φ, w, b (weights, biases)</li>
        <li>Distributions: P(), p(), N(μ, σ²), Ber(p) (Bernoulli)</li>
        <li>Operations: ∑ (sum), log (logarithm), exp (exponential), E[] (expectation)</li>
        <li>Relationships: ~ (sampled from), = (equals), ∝ (proportional to)</li>
    </ul>
    <p><strong class="tutorial-strong">Exercises (Pencil &amp; Paper!):</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong">Symbol Inventory:</strong></p>
        <ul>
            <li><p>From the expression: L(θ, φ) = E_{z ~ q_φ(z|x)}[log p_θ(x|z)] - D_{KL}(q_φ(z|x) || p(z))</p>
            <ul>
                <li><p>(a) List all unique Roman letters used as variables (e.g., x, z).</p></li>
                <li><p>(b) List all unique Greek letters used as parameters (e.g., θ, φ).</p></li>
                <li><p>(c) List all mathematical operators or functions (e.g., E, log, D_KL).</p></li>
                <li><p>(d) What does ~ generally signify in expressions like z ~ q_φ(z|x)? (Consult Glossary: <a href="#random-variable-glossary">Random Variable</a> or "Gentle Stroll")</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">"Translate to English" - Literal Meaning:</strong></p>
        <ul>
            <li><p>(a) p(y|x, w) -&gt; _________________________________________________</p></li>
            <li><p>(b) w_new = w_old - α * ∇L(w_old) (α is learning rate, ∇ is gradient) -&gt; _________________________</p></li>
            <li><p>(c) x_i ~ Ber(p) -&gt; __________________________________________________ (Ber is Bernoulli)</p></li>
            <li><p>(d) log P(A, B) -&gt; __________________________________________________</p></li>
            <li><p>(e) E_{z ~ N(0,I)}[f(z)] -&gt; ___________________________________________</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Match the Notation to the Concept:</strong><br />
            <pre>
| Notation             | Concept                                   |
| :------------------- | :---------------------------------------- |
| 1. θ, w, b           | A. A specific probability distribution    |
| 2. x, y, z           | B. An operation to find an average value  |
| 3. N(μ, σ²), Ber(p)  | C. The "settings" of a model we learn   |
| 4. E[...]            | D. Things that can take on different values |
| 5. ∑                 | E. An operation to add up a series of terms |
            </pre>
            <i class="tutorial-italic">(Your Task: Fill in the matches, e.g., 1-C, 2-D, etc.)</i>
        </p></li>
        <li><p><strong class="tutorial-strong">Identify the "Main Subject" (Random Variable of Interest):</strong></p>
        <ul>
            <li><p>In p(x|z), the main random variable whose probability is being described is ____.</p></li>
            <li><p>In q(z|x), the main random variable whose probability is being described is ____.</p></li>
            <li><p>In E_{x~P(x)}[h(x)], the expectation is being taken over values of ____.</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Parameters vs. Variables:</strong></p>
        <ul>
            <li><p>Consider the Gaussian PDF: p(x | μ, σ²) = (1 / sqrt(2πσ²)) * exp(-(x-μ)² / (2σ²))</p>
            <ul>
                <li><p>(a) Which symbol represents the data point (a variable)?</p></li>
                <li><p>(b) Which symbols represent the parameters that define this specific Gaussian distribution?</p></li>
                <li><p>(c) If we are <i class="tutorial-italic">learning</i> a Gaussian model from data, which symbols would we be trying to estimate?</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Spot the Distribution:</strong></p>
        <ul>
            <li><p>z_i ~ N(μ_i, Σ_i): What type of distribution is z_i sampled from? What do μ_i and Σ_i (Sigma, for covariance matrix) represent? (See: <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>)</p></li>
            <li><p>y_i ~ Categorical(π): What type of distribution is y_i sampled from? What does π (pi) likely represent here? (See: <a href="#categorical-distribution-glossary">Categorical Distribution</a>)</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Basic Logic - "If... then...":</strong></p>
        <ul>
            <li><p>If log a = b, then a = exp(b). True or False?</p></li>
            <li><p>If a = exp(b), then log a = b. True or False?</p></li>
            <li><p>If ∑_{i=1}^{K} p_i = 1 and each p_i ≥ 0, what could p_i represent? (Hint: probabilities of discrete outcomes, see <a href="#categorical-distribution-glossary">Categorical Distribution</a>).</p></li>
        </ul>
        </li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> At this stage, don't worry about <i class="tutorial-italic">why</i> the equation is written that way. Just focus on identifying the parts. Think of it like learning the letters of a new alphabet and a few basic words.</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage2">Stage 2: Forming Simple Sentences - Connecting Symbols to Concepts</h3>
    <p><i class="tutorial-italic">(Focus: Understanding what basic mathematical "phrases" mean conceptually)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To connect sequences of symbols to the probabilistic and statistical concepts they represent.</p>
    <p><strong class="tutorial-strong">Core Concepts for this Stage:</strong></p>
    <ul>
        <li><a href="#likelihood-glossary">Likelihood</a>, <a href="#log-likelihood-glossary">Log-Likelihood</a> (and why we use logs)</li>
        <li><a href="#prior-distribution-glossary">Prior</a>, <a href="#posterior-distribution-glossary">Posterior</a> (<a href="#bayesian-inference-glossary">Bayes' Theorem</a> components conceptually)</li>
        <li><a href="#expectation-e-glossary">Expectation</a> as a weighted average and its properties (e.g., linearity E[aX+b] = aE[X]+b)</li>
        <li><a href="#conditional-distribution-glossary">Conditional</a> vs. <a href="#marginal-distribution-glossary">Marginal</a> vs. <a href="#joint-distribution-glossary">Joint</a> probabilities (conceptual differences)</li>
        <li>Basic model components: encoder, decoder (conceptual, see <a href="#variational-autoencoder-vae-glossary">VAE</a>)</li>
    </ul>
    <p><strong class="tutorial-strong">Exercises (Pencil &amp; Paper!):</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong">Explaining the "Why":</strong></p>
        <ul>
            <li><p>(a) You have 1000 data points x_1, ..., x_1000. The probability of observing all of them (assuming independence) is P(x_1, ..., x_{1000} | θ) = P(x_1|θ) * P(x_2|θ) * ... * P(x_{1000}|θ). If each P(x_i|θ) is around 0.1, what problem might you encounter when calculating the joint probability directly? How does taking the <a href="#logarithm-log-natural-logarithm-ln-glossary">log</a> solve this?</p></li>
            <li><p>(b) Explain argmax_θ <a href="#log-likelihood-glossary">log P(data|θ)</a> in plain English. What is the goal here?</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Bayes' Theorem Storytelling:</strong>
            <p>Imagine you're a doctor. You have these terms:
            P(Disease): Your initial belief (prior) that a random person has a specific disease.
            P(PositiveTest | Disease): The probability a sick person tests positive.
            P(PositiveTest | NoDisease): The probability a healthy person tests positive (false positive).
            P(Disease | PositiveTest): What you really want to know – the probability the person has the disease after they test positive.
            </p>
            <p><strong class="tutorial-strong">Task:</strong> Using Bayes' Theorem <code>P(A|B) = P(B|A)P(A) / P(B)</code>, identify what A, B, P(B|A), P(A), and P(B) correspond to in this scenario to calculate P(Disease | PositiveTest). (Don't calculate, just map. Note: P(PositiveTest) would be the evidence, calculated using law of total probability: P(PositiveTest | Disease)P(Disease) + P(PositiveTest | NoDisease)P(NoDisease)). (See <a href="#bayesian-inference-glossary">Bayesian Inference</a>)</p>
        </li>
        <li><p><strong class="tutorial-strong">Expectation Playground:</strong></p>
        <ul>
            <li><p>Let X be a random variable that takes value 1 with probability p and value 0 with probability 1-p (a Bernoulli trial).</p>
            <ul>
                <li><p>(a) Write down the expression for <a href="#expectation-e-glossary">E[X]</a> using ∑. Calculate it. (This is the mean of a Bernoulli).</p></li>
                <li><p>(b) Let Y = 5X + 2. Calculate E[Y] using your result from (a) and the linearity property of expectation (E[aX+b] = aE[X]+b).</p></li>
                <li><p>(c) Verify your result in (b) by calculating E[Y] directly from the definition: ∑ y * P(Y=y).</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Conditional vs. Marginal vs. Joint:</strong></p>
        <ul>
            <li><p>Consider two dice, D1 (6-sided) and D2 (4-sided). Let X be the outcome of D1, and Y be the outcome of D2.</p>
            <ul>
                <li><p>(a) What is P(X=3)? (<a href="#marginal-distribution-glossary">Marginal probability</a>)</p></li>
                <li><p>(b) What is P(X=3, Y=2)? (<a href="#joint-distribution-glossary">Joint probability</a>, assuming dice are independent)</p></li>
                <li><p>(c) What is P(X=3 | Y=2)? (<a href="#conditional-distribution-glossary">Conditional probability</a>. How does knowing Y=2 change your belief about X=3, if at all, for independent dice?)</p></li>
                <li><p>(d) If p(z) is the <a href="#prior-distribution-glossary">prior</a> over latent codes and p(x|z) is the decoder output distribution, what kind of probability is p(x,z) = p(x|z)p(z)?</p></li>
            </ul>
            </li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Model Components in Math (referencing <a href="#variational-autoencoder-vae-glossary">VAE</a>):</strong></p>
        <ul>
            <li><p>In a VAE, the <i class="tutorial-italic">encoder</i> is often written as q_φ(z|x). What does this mean? It takes _______ as input and produces a distribution over _______. The parameters of the encoder are _______.</p></li>
            <li><p>The <i class="tutorial-italic">decoder</i> is often written as p_θ(x|z). What does this mean? It takes _______ as input and produces a distribution over _______. The parameters of the decoder are _______.</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">From Words to Math (and vice-versa):</strong></p>
        <ul>
            <li><p>(a) "The probability density of observing data point x_n given parameters θ." -&gt; ______________ (See: <a href="#likelihood-glossary">Likelihood</a>)</p></li>
            <li><p>(b) ∑_{k=1}^{K} π_k N(x | μ_k, Σ_k) -&gt; (In words) "This represents a _________________ of K ___________ distributions, where π_k are the _________ weights." (See: <a href="#mixture-model-glossary">Mixture Model</a>, <a href="#gaussian-mixture-model-gmm-glossary">Gaussian Mixture Model</a>)</p></li>
            <li><p>(c) "The evidence, or <a href="#marginal-likelihood-glossary">marginal likelihood</a> of the data x." (Two ways to write this, one involving z) -&gt; _________ or _________</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Interpreting argmax and argmin:</strong> (See: <a href="#objective-function-glossary">Objective Function</a>)</p>
        <ul>
            <li><p>θ* = argmax_θ L(θ) means θ* is the value of θ that ____________ L(θ).</p></li>
            <li><p>z* = argmin_z ||x - g(z)||^2 means z* is the value of z that ____________ the squared distance between x and g(z). What might g(z) represent in a generative model? (Hint: decoder/generator).</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Prior Intuition:</strong></p>
        <ul>
            <li><p>In VAEs, a common <a href="#prior-distribution-glossary">prior</a> is p(z) = N(z | 0, I). What does this choice imply about our initial beliefs for the <a href="#latent-variable-glossary">latent codes</a> z before seeing any data? (e.g., are they centered? are different dimensions of z correlated? are they spread out or compact? See <a href="#isotropic-gaussian-glossary">Isotropic Gaussian</a>).</p></li>
        </ul>
        </li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> We're now moving from just reading words to understanding simple sentences. The goal is to see log P(data|θ) and immediately think "how well the parameters explain the data, in a numerically friendly way."</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage3">Stage 3: Following Simple Paragraphs - Tracing Basic Derivations</h3>
    <p><i class="tutorial-italic">(Focus: Understanding how one line of math leads to the next through valid algebraic/probabilistic manipulations)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To follow and verify the steps in a short, explicit derivation.</p>
    <p><strong class="tutorial-strong">Core Concepts for this Stage:</strong></p>
    <ul>
        <li>Properties of <a href="#logarithm-log-natural-logarithm-ln-glossary">logarithms</a>: log(ab) = log a + log b, log(a/b) = log a - log b, log(a^k) = k log a</li>
        <li>Properties of <a href="#expectation-e-glossary">expectation</a>: E[c] = c (c is constant), E[X+Y] = E[X]+E[Y], E[cX] = cE[X]</li>
        <li>Definition of <a href="#conditional-distribution-glossary">conditional probability</a>: P(A|B) = P(A,B) / P(B) =&gt; P(A,B) = P(A|B)P(B)</li>
        <li><a href="#marginal-distribution-glossary">Marginalization</a>: P(A) = ∑_B P(A,B) or p(a) = ∫ p(a,b) db</li>
        <li>Multiplying by 1 in a fancy way (e.g., X = X * (Y/Y))</li>
    </ul>
    <p><strong class="tutorial-strong">Exercises (Pencil &amp; Paper!):</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong">Log Gymnastics - Justify Each Step:</strong><br />
        <code>log (a * b<sup>2</sup> / c)</code><br />
        <code>= log(a) + log(b<sup>2</sup>) - log(c)</code>   <i class="tutorial-italic">(Justification: _______________ and _______________)</i><br />
        <code>= log(a) + 2log(b) - log(c)</code>   <i class="tutorial-italic">(Justification: _______________)</i></p></li>
        <li><p><strong class="tutorial-strong">Expectation Shuffle - Justify Each Step:</strong><br />
        <code>E<sub>x~P(x)</sub> [ax + b + c log y]</code>  (Assume <code>a, b, c, y</code> are constants with respect to <code>x</code>)<br />
        <code>= E<sub>x~P(x)</sub>[ax] + E<sub>x~P(x)</sub>[b] + E<sub>x~P(x)</sub>[c log y]</code> <i class="tutorial-italic">(Justification: _______________)</i><br />
        <code>= a E<sub>x~P(x)</sub>[x] + b + c log y</code> <i class="tutorial-italic">(Justification: _______________ and _______________)</i></p></li>
        <li><p><strong class="tutorial-strong">Conditional Probability Unpacking:</strong></p>
        <ul>
            <li><p>Start: p(x,y,z) (joint probability of three variables)</p></li>
            <li><p>Step 1: = p(x | y,z) p(y,z) <i class="tutorial-italic">(Justification: Definition of _______________)</i></p></li>
            <li><p>Step 2: = p(x | y,z) p(y | z) p(z) <i class="tutorial-italic">(Justification: Definition of _______________ applied to p(y,z))</i></p></li>
            <li><p>This is called the "chain rule of probability." Can you write the chain rule for p(a,b,c,d)? (See <a href="#markov-process-markov-chain-glossary">Markov Process</a> for related concept of conditional dependence)</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Introducing q(z) - Fill in the Gaps:</strong><br />
        We want to analyze log p(x).<br />
        <code>log p(x) = log ∫ p(x,z) dz</code>                                 <i class="tutorial-italic">(Rule: Marginalization)</i><br />
        <code>= log ∫ q(z) [p(x,z) / q(z)] dz</code>                         <i class="tutorial-italic">(Rule: Multiply by 1, where 1 = _________)</i><br />
        <code>= log E<sub>z ~ q(z)</sub> [ p(x,z) / q(z) ]</code>                       <i class="tutorial-italic">(Rule: Definition of _______________)</i><br />
        <i class="tutorial-italic">(Note: this q(z) is a generic distribution. In VAEs it's q(z|x)). This sequence is part of deriving the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>.</i></p></li>
        <li><p><strong class="tutorial-strong">Deriving <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a> from Scratch (Simpler Form):</strong><br />
        Recall <code>D<sub>KL</sub>(P||Q) = E<sub>x~P(x)</sub>[log(P(x)/Q(x))]</code>.</p>
        <ul>
            <li><p>(a) Expand log(P(x)/Q(x)) using log properties.</p></li>
            <li><p>(b) Distribute the expectation using E[A-B] = E[A] - E[B].</p></li>
            <li><p>(c) You should get <code>D<sub>KL</sub>(P||Q) = E<sub>x~P(x)</sub>[log P(x)] - E<sub>x~P(x)</sub>[log Q(x)]</code>.<br />
            The first term <code>E<sub>x~P(x)</sub>[log P(x)]</code> is also known as the negative __________ of distribution P. (Consult Glossary: Entropy, if available, or search online. See <a href="#nat-glossary">Nat</a> for units).</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong"><a href="#evidence-lower-bound-elbo-glossary">ELBO</a> Step-by-Step (Focus on <code>log p(x,z) / q(z|x)</code> part):</strong><br />
        From <code>E<sub>z~q(z|x)</sub> [log (p(x,z) / q(z|x))]</code></p>
        <ul>
            <li><p>(a) Apply log(A/B) rule to <code>log (p(x,z) / q(z|x))</code>. -&gt; <code>E<sub>z~q(z|x)</sub> [_______________ - _______________]</code></p></li>
            <li><p>(b) Replace <code>p(x,z)</code> with <code>p(x|z)p(z)</code> (using definition of conditional probability). -&gt; <code>E<sub>z~q(z|x)</sub> [log(____________ * ____________) - log q(z|x)]</code></p></li>
            <li><p>(c) Apply log(AB) rule. -&gt; <code>E<sub>z~q(z|x)</sub> [log p(x|z) + _______________ - log q(z|x)]</code></p></li>
            <li><p>(d) Distribute the Expectation E[] over the three terms.<br />
            -&gt; <code>E<sub>z~q(z|x)</sub>[log p(x|z)] + E<sub>z~q(z|x)</sub>[log p(z)] - E<sub>z~q(z|x)</sub>[log q(z|x)]</code></p></li>
            <li><p>(e) Recognize the last two terms as <code>-D<sub>KL</sub>(q(z|x) || p(z))</code> based on the result from exercise 5c.<br />
            -&gt; <code>E<sub>z~q(z|x)</sub>[log p(x|z)] - D<sub>KL</sub>(q(z|x) || p(z))</code> (This is the ELBO!)</p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Maximum Likelihood for a Gaussian (Conceptual Start):</strong></p>
        <ul>
            <li><p>Data: x_1, ..., x_N. Assume they are i.i.d. (independent and identically distributed) from <code>N(μ, σ²)</code>.</p></li>
            <li><p>(a) Write the <a href="#likelihood-glossary">likelihood</a> P(x_1, ..., x_N | μ, σ²). (It's a product).</p></li>
            <li><p>(b) Write the <a href="#log-likelihood-glossary">log-likelihood</a> L(μ, σ²) = log P(x_1, ..., x_N | μ, σ²). (It's a sum).</p></li>
            <li><p>(c) For a <i class="tutorial-italic">single</i> data point x_i, write out <a href="#log-density-of-a-gaussian-glossary">log p(x_i | μ, σ²)</a>, explicitly substituting the Gaussian PDF formula: <code>p(x_i | μ, σ²) = (1 / sqrt(2πσ²)) * exp(-(x_i-μ)² / (2σ²))</code>. Simplify using log rules.<br />
            <i class="tutorial-italic">(Actually finding the argmax w.r.t μ and σ² involves calculus - derivatives - which we'll touch on later, but setting it up is key).</i></p></li>
        </ul>
        </li>
        <li><p><strong class="tutorial-strong">Variable Swapping (Careful Reading of Expectations):</strong><br />
        Consider the expression <code>E<sub>a~P(a)</sub> [log Q(a,b)]</code>.</p>
        <ul>
            <li><p>Which variable is being averaged over in this <a href="#expectation-e-glossary">Expectation</a>?</p></li>
            <li><p>Which variable(s) would still be present in the expression <i class="tutorial-italic">after</i> the expectation is notionally computed?</p></li>
            <li><p>If Q(a,b) = a + b^2, what would the expression evaluate to? (Assume <code>E<sub>a~P(a)</sub>[a] = μ_a</code>).</p></li>
        </ul>
        </li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> Derivations are just sequences of logical steps. Your job is to be a detective, verifying each step's validity. Use your pencil! Write out the intermediate forms if it helps. The "Fill in the Blank" exercise is crucial – it's the first step towards doing it yourself.</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage4">Stage 4: Deconstructing Core Arguments - Understanding Model-Specific Math</h3>
    <p><i class="tutorial-italic">(Focus: Grasping the meaning and derivation of key equations like the ELBO in VAEs or loss functions in other generative models, including the "why" behind certain mathematical steps like Jensen's Inequality or reparameterization.)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To understand the "story" told by a more complex equation like the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>, including the role of each term and the intuition behind its derivation.</p>
    <p><strong class="tutorial-strong">Core Concepts for this Stage:</strong></p>
    <ul>
        <li><a href="#jensens-inequality-glossary">Jensen's Inequality</a> (and why it's used to create a lower bound)</li>
        <li>Full <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> derivation (multiple forms, if possible)</li>
        <li><a href="#reparameterization-trick-glossary">Reparameterization Trick</a> (why and how it's used for gradients)</li>
        <li>Understanding how <a href="#objective-function-glossary">loss function</a> terms drive model behavior</li>
        <li>Basics of gradients (conceptual: ∇f points in direction of steepest ascent of f)</li>
    </ul>
    <p><strong class="tutorial-strong">Exercises (Pencil &amp; Paper!):</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong"><a href="#jensens-inequality-glossary">Jensen's Inequality</a> Intuition &amp; Application:</strong> (Task: Sketch log(x), demonstrate the inequality for concave functions, explain why it creates a lower bound in the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> derivation by identifying the function and expectation involved).</p></li>
        <li><p><strong class="tutorial-strong"><a href="#evidence-lower-bound-elbo-glossary">ELBO</a>: The Other Formulation (<code>log p(x) - D<sub>KL</sub>(q(z|x) || p(z|x))</code>):</strong> (Task: Starting from the definition of <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a>, use <a href="#bayesian-inference-glossary">Bayes' rule</a> for <code>p(z|x)</code> and manipulate terms to show this form is equivalent to the reconstruction + regularization form. Explain what happens when <code>q(z|x)</code> perfectly approximates <code>p(z|x)</code>.)</p></li>
        <li><p><strong class="tutorial-strong">The <a href="#reparameterization-trick-glossary">Reparameterization Trick</a> - Why and How:</strong> (Task: Explain why direct differentiation of an <a href="#expectation-e-glossary">expectation</a> with respect to parameters of the sampling distribution is problematic. Detail how the trick, e.g., for a Gaussian <code>z = μ_φ(x) + σ_φ(x) * ε</code>, moves parameter dependence to a deterministic function, enabling gradient flow.)</p></li>
        <li><p><strong class="tutorial-strong">Loss Function Detective (VAE Loss):</strong> (Task: Relate the typical VAE minimization loss to the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> (maximization). If reconstruction term <code>log p_θ(x|z)</code> uses a Gaussian (<a href="#log-density-of-a-gaussian-glossary">Log Density of a Gaussian</a>), explain its connection to squared error. Analyze the effect of a large/small <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL term</a> on the <a href="#latent-variable-glossary">latent space</a> and <a href="#prior-distribution-glossary">prior</a> matching).</p></li>
        <li><p><strong class="tutorial-strong">Gradients and Model Updates (Conceptual):</strong> (Task: Explain what <code>∇_θ L(θ)</code> represents for an <a href="#objective-function-glossary">objective function</a> <code>L(θ)</code>. Why subtract in gradient descent? For <code>ELBO = R(θ,φ) - K(φ)</code>, identify which parameters (<code>θ</code> or <code>φ</code>) affect <code>R</code> and <code>K</code> when taking gradients).</p></li>
        <li><p><strong class="tutorial-strong">Analyzing a Simplified <a href="#generative-model-glossary">Generative Model</a> (e.g., Factor Analysis - Linear Gaussian Model):</strong> (Task: Given <code>z ~ N(0, I)</code>, <code>x = Wz + μ + ε</code>, where <code>ε ~ N(0, Ψ)</code>. What is <code>p(x|z)</code>? What is <code>p(x)</code> (conceptually)? What parameters (<code>W, μ, Ψ</code>) would be learned? See <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>).</p></li>
        <li><p><strong class="tutorial-strong">Interpreting <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a> Terms in Different Contexts:</strong> (Task: Explain how the VAE's KL term <code>D<sub>KL</sub>(q(z|x) || p(z))</code> can be viewed as an "information cost" or a measure of complexity for the encoding <code>q(z|x)</code> relative to the <a href="#prior-distribution-glossary">prior</a> <code>p(z)</code>).</p></li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> Complex equations tell a story. The ELBO tells a story of trying to explain the data (log p(x|z)) while keeping the learned latent representations (q(z|x)) close to a simple prior (p(z)), all because directly optimizing log p(x) is too hard.</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage5">Stage 5: Starting to Write - Simple Derivations &amp; Pattern Recognition</h3>
    <p><i class="tutorial-italic">(Focus: Actively attempting to derive simple results or fill in larger gaps in derivations, and recognizing common mathematical "moves" and "tricks" like reparameterization.)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To gain confidence in manipulating equations and to begin internalizing common derivational patterns.</p>
    <p><strong class="tutorial-strong">Core Concepts for this Stage:</strong></p>
    <ul>
        <li>Fluency with algebraic manipulation of probabilistic expressions.</li>
        <li>Deriving gradients of simple functions.</li>
        <li>Applying <a href="#reparameterization-trick-glossary">reparameterization</a> in new (simple) contexts.</li>
        <li>Modifying existing derivations for slight changes in assumptions.</li>
        <li>Writing out the <a href="#objective-function-glossary">objective function</a> for a slightly modified model.</li>
    </ul>
    <p><strong class="tutorial-strong">Exercises (Pencil &amp; Paper!):</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong"><a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a> for Univariate Gaussians:</strong> (Task: Given the formula for KL between two univariate Gaussians <code>q(z)=N(μ,σ²)</code> and <code>p(z)=N(0,1)</code>, verify it yields 0 if <code>μ=0, σ=1</code>. Optionally, use the general formula for KL between <code>N(μ<sub>1</sub>,σ<sub>1</sub>²)</code> and <code>N(μ<sub>2</sub>,σ<sub>2</sub>²)</code> and verify. See also <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>, <a href="#log-density-of-a-gaussian-glossary">Log Density of a Gaussian</a>).</p></li>
        <li><p><strong class="tutorial-strong"><a href="#reparameterization-trick-glossary">Reparameterization</a> for a Different Distribution (Exponential):</strong> (Task: Given inverse transform sampling for an Exponential distribution <code>z = -log(1-u)/λ</code> where <code>u~Uniform(0,1)</code>, rewrite <code>E<sub>z~Exp(λ_φ)</sub>[g(z)]</code> using this reparameterization so the expectation is over <code>u</code>. Explain its utility for gradient calculation w.r.t. <code>φ</code> if <code>λ_φ</code> is parameterized by <code>φ</code>).</p></li>
        <li><p><strong class="tutorial-strong">Deriving a Simple Gradient:</strong> (Task: For loss <code>L(w) = (y - wx)²</code>, calculate <code>∂L/∂w</code>. Write the gradient descent update rule for <code>w</code>. Relevant for optimizing <a href="#objective-function-glossary">Objective Function</a>).</p></li>
        <li><p><strong class="tutorial-strong">Modifying the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> (Conceptual - β-VAE):</strong> (Task: For ELBO with a weighted KL term <code>E[log p(x|z)] - β * D<sub>KL</sub>(q(z|x) || p(z))</code>, analyze the effect of <code>β > 1</code> and <code>0 < β < 1</code> on the emphasis on prior matching vs. reconstruction, and on the learned <a href="#latent-variable-glossary">latent space</a>).</p></li>
        <li><p><strong class="tutorial-strong">Outline the Derivation - GAN Loss (Conceptual):</strong> (Task: For the GAN minimax objective <code>min<sub>G</sub> max<sub>D</sub> V(D,G) = E<sub>x~p<sub>data</sub></sub>[log D(x)] + E<sub>z~p<sub>z</sub></sub>[log(1 - D(G(z)))]</code>, explain what <code>D(x)</code> and <code>G(z)</code> represent, and how each term encourages D and G to achieve their respective goals. See <a href="#generative-model-glossary">Generative Model</a>, <a href="#likelihood-free-objective-function-glossary">Likelihood-Free Objective Function</a>).</p></li>
        <li><p><strong class="tutorial-strong">"What if" Derivation - Simple Change (Bernoulli Likelihood):</strong> (Task: Start with <a href="#log-likelihood-glossary">log-likelihood</a> for i.i.d. data <code>L(θ) = ∑ log p(x<sub>i</sub>|θ)</code>. If <code>p(x<sub>i</sub>|θ)</code> is Bernoulli <code>θ<sup>x<sub>i</sub></sup>(1-θ)<sup>1-x<sub>i</sub></sup></code>, substitute and simplify <code>L(θ)</code>. Challenge: find MLE for <code>θ</code>. See <a href="#probability-distribution-glossary">Probability Distribution</a>, <a href="#likelihood-glossary">Likelihood</a>).</p></li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> This is where the training wheels start to come off. It's okay to wobble or fall (get stuck). The key is to try, consult resources when stuck, and then try again. You're learning the common "recipes" for mathematical arguments in this field.</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage6">Stage 6: Fluency and Creative Construction - Reading, Critiquing, and Creating</h3>
    <p><i class="tutorial-italic">(Focus: Confidently navigating complex papers, understanding the nuances of derivations, and being able to formulate your own mathematical arguments for new model variations or problems.)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> Achieve a level of comfort where you can read cutting-edge papers, understand their mathematical underpinnings, critique them, and, when necessary, derive mathematical components for novel ideas.</p>
     <p><strong class="tutorial-strong">This stage is more about <i class="tutorial-italic">projects</i> and <i class="tutorial-italic">deep dives</i> than discrete exercises.</strong></p>
    <p><strong class="tutorial-strong">Tasks/Projects:</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong">Full Paper Deconstruction:</strong> (Task: Pick a foundational <a href="#generative-model-glossary">generative model</a> paper like original <a href="#variational-autoencoder-vae-glossary">VAE</a>, GAN, or <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">DDPM</a>. Re-derive key mathematical results. Optionally implement. Write a critical review of its mathematical formulation, assumptions, and limitations).</p></li>
        <li><p><strong class="tutorial-strong">"Propose and Derive" a Model Variation:</strong>
            <ul>
            <li>(a) (Task: Modify the <a href="#prior-distribution-glossary">prior</a> <code>p(z)</code> in a <a href="#variational-autoencoder-vae-glossary">VAE</a>, e.g., to <code>N(z | 0, diag(σ<sub>j</sub>²))</code> where <code>σ<sub>j</sub>²</code> are learnable. Derive the new <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL term</a> for the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>.)</li>
            <li>(b) (Task: How would you modify the <a href="#log-likelihood-in-vaes-glossary">reconstruction term</a> <code>log p_θ(x|z)</code> in a VAE if some components of <code>x</code> are missing? Write down a modified objective.)</li>
            </ul>
        </p></li>
        <li><p><strong class="tutorial-strong">Explore a Theoretical Connection:</strong> (Task: Research the connection between <a href="#variational-autoencoder-vae-glossary">VAEs</a> and rate-distortion theory or information bottleneck. Understand mathematical arguments linking <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> terms to mutual information, compression, distortion. See <a href="#nat-glossary">Nat</a>, <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a>).</p></li>
        <li><p><strong class="tutorial-strong"><a href="#diffusion-model-glossary">Diffusion Models</a> - Deconstruct the Loss:</strong> (Task: For the simplified <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">DDPM</a> loss <code>E[ ||ε - ε<sub>θ</sub>(x<sub>t</sub>, t)||^2 ]</code>, identify terms (<code>x<sub>0</sub></code>, <code>ε</code>, <code>t</code>, <code>ᾱ<sub>t</sub></code>, <code>ε<sub>θ</sub></code>). Explain what <code>x<sub>t</sub></code> (<code>√ᾱ<sub>t</sub>x<sub>0</sub> + √(1-ᾱ<sub>t</sub>)ε</code>) is. Explain why minimizing this trains the model to denoise. Advanced: Research relation to full variational bound. See <a href="#forward-diffusion-process-glossary">Forward Diffusion Process</a>).</p></li>
        <li><p><strong class="tutorial-strong">Tackle a Challenge Problem from a Course or Textbook:</strong> (Task: Select and solve a derivation-heavy exercise related to <a href="#generative-model-glossary">generative models</a> from advanced ML resources).</p></li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom:</strong> At this stage, you are no longer just a consumer of mathematical derivations; you are becoming a creator and a critical thinker. You'll start seeing the elegance and power in the math, and how it enables precise expression of complex ideas. This is a lifelong learning journey, and the joy comes from continuous exploration and discovery.</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage7">Stage 7: Mathematical Artistry &amp; Innovation in Generative Modeling</h3>
    <p><i class="tutorial-italic">(Focus: Moving beyond understanding existing work to synthesizing knowledge, identifying open problems, formulating novel mathematical frameworks, and rigorously evaluating them. This is about becoming a research-level contributor.)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To develop the intuition, skills, and knowledge base to propose, formalize, and analyze novel <a href="#generative-model-glossary">generative models</a> or inference techniques, and to critically engage with the forefront of research.</p>
    <p><strong class="tutorial-strong">Paths for Exploration &amp; Development (Less "Exercises," More "Quests"):</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong">Deep Dive into Advanced Theoretical Topics:</strong>
            <ul>
            <li>Information Geometry (using <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a>, Fisher Information).</li>
            <li>Optimal Transport & Wasserstein Distances (for GANs, <a href="#likelihood-free-objective-function-glossary">Likelihood-Free Objectives</a>).</li>
            <li>Stochastic Differential Equations (SDEs) & Score-Based Models (continuous-time <a href="#diffusion-model-glossary">Diffusion Models</a>).</li>
            <li>Causal Inference & <a href="#generative-model-glossary">Generative Models</a>.</li>
            <li>Advanced <a href="#variational-inference-vi-glossary">Variational Inference</a> Techniques (Amortized, Non-Amortized, Implicit VI, Normalizing Flows for richer <a href="#posterior-distribution-glossary">posteriors</a>/<a href="#prior-distribution-glossary">priors</a>).</li>
            <li>Bayesian Nonparametrics (e.g., Dirichlet Processes, Gaussian Processes as priors).</li>
            </ul>
        </p></li>
        <li><p><strong class="tutorial-strong">The "Why Does it Work (Or Not Work)?" Quest:</strong> (Task: Mathematically analyze SOTA <a href="#generative-model-glossary">generative models</a> or common failure modes like posterior collapse in <a href="#variational-autoencoder-vae-glossary">VAEs</a> or mode collapse in GANs. Identify reasons in objectives or assumptions).</p></li>
        <li><p><strong class="tutorial-strong">The "Unifying Framework" Quest:</strong> (Task: Explore research connecting diverse <a href="#generative-model-glossary">generative model</a> families, e.g., how <a href="#variational-autoencoder-vae-glossary">VAEs</a> and <a href="#diffusion-model-glossary">Diffusion Models</a> relate via <a href="#hierarchical-vae-glossary">Hierarchical VAEs</a> or variational perspectives).</p></li>
        <li><p><strong class="tutorial-strong">The "Invent a Toy Problem" Quest:</strong> (Task: Design a simple generative task. Invent/modify a mathematical <a href="#objective-function-glossary">objective</a> or model structure. Work through its properties).</p></li>
        <li><p><strong class="tutorial-strong">The "Cross-Pollination" Quest:</strong> (Task: Read papers from other quantitative fields. Can their mathematical ideas/methods inspire solutions in generative modeling?).</p></li>
        <li><p><strong class="tutorial-strong">The "Formalize an Intuition" Quest:</strong> (Task: Translate a high-level modeling goal, like "disentangled <a href="#latent-variable-glossary">latent variables</a>," into a precise mathematical <a href="#objective-function-glossary">objective</a> or constraint).</p></li>
        <li><p><strong class="tutorial-strong">The "Critique and Improve" Quest (On Your Own Ideas):</strong> (Task: Rigorously analyze your novel mathematical ideas for models/objectives regarding failure modes, tractability, theoretical guarantees, and comparison to existing methods).</p></li>
        <li><p><strong class="tutorial-strong">Engage with the Research Community:</strong> (Task: Read arXiv pre-prints, attend seminars/conferences, attempt to reproduce results, contribute to open-source implementations).</p></li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom for Stage 7:</strong> Embrace ambiguity in research. Develop a "taste" for elegant and powerful mathematical ideas. Collaborate. Don't fear being wrong; learn from "failures." Remember math is a tool. Perfection can be the enemy of progress; find balance between rigor and empirical utility.</p>
    <hr class="tutorial-divider" />

    <h3 class="tutorial-stage-title" id="gentle-stroll-stage8">Stage 8: Mentorship, Leadership, and Shaping the Field</h3>
    <p><i class="tutorial-italic">(Focus: Transitioning from individual excellence to amplifying impact through guiding others, setting research agendas, fostering collaborations, and contributing to the ethical and responsible development of AI.)</i></p>
    <p><strong class="tutorial-strong">Objective:</strong> To leverage deep mathematical and conceptual understanding to lead research efforts, mentor the next generation, and contribute to the broader intellectual and societal impact of generative AI.</p>
    <p><strong class="tutorial-strong">Paths for Development &amp; Contribution:</strong></p>
    <ol>
        <li><p><strong class="tutorial-strong">Mentorship &amp; Education:</strong> (Task: Mentor junior researchers. Develop advanced educational materials to make difficult concepts accessible, e.g., a "Gentle Stroll" for Information Geometry).</p></li>
        <li><p><strong class="tutorial-strong">Defining Research Agendas:</strong> (Task: Identify fundamental open questions in generative AI requiring mathematical breakthroughs. Write "perspectives" papers suggesting promising directions).</p></li>
        <li><p><strong class="tutorial-strong">Building Bridges &amp; Fostering Collaboration:</strong> (Task: Organize interdisciplinary workshops. Identify where mathematical tools from one field can be transformative for another, e.g., category theory for model compositionality, PDE theory for <a href="#diffusion-model-glossary">diffusion models</a>).</p></li>
        <li><p><strong class="tutorial-strong">Championing Rigor and Reproducibility:</strong> (Task: Advocate for high mathematical standards in publications. Develop guidelines for reproducibility of mathematical claims).</p></li>
        <li><p><strong class="tutorial-strong">Ethical &amp; Societal Implications – The Mathematical Lens:</strong> (Task: Formally analyze and mitigate biases, misuse potentials of <a href="#generative-model-glossary">generative models</a>. Develop mathematical definitions for fairness, privacy, accountability relevant to them).</p></li>
        <li><p><strong class="tutorial-strong">Developing New Mathematical Tools &amp; Languages:</strong> (Task: Contribute new mathematical abstractions or notations if existing ones are inadequate for new AI ideas).</p></li>
        <li><p><strong class="tutorial-strong">Visionary Leadership:</strong> (Task: Synthesize trends, foresee challenges, articulate how foundational mathematical understanding can unlock future AI capabilities).</p></li>
    </ol>
    <p><strong class="tutorial-strong">"BEST Teacher" Wisdom for Stage 8:</strong> Legacy is built through others. Maintain intellectual humility. Formulate the right questions. Balance breadth and depth. Cultivate endurance and a long-term vision for transformative mathematical contributions.</p>
    <hr class="tutorial-divider" />
</div> <!-- End of tutorial-content wrapper -->

    <h2 class="glossary-title" id="glossary-terms-heading">Glossary of Terms</h2>
    <!-- Actual glossary entries will be dynamically inserted here by the script -->
    <!-- For example: -->
    <!-- <h3 class="glossary-entry-title" id="probability-distribution-glossary">1. Probability Distribution</h3> ... -->

    <h3 class="glossary-entry-title" id="backward-diffusion-process-glossary">Backward Diffusion Process (Reverse Process)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#diffusion-model-glossary">diffusion models</a>, the learned, generative process that aims to reverse the <a href="#forward-diffusion-process-glossary">forward diffusion</a>. It starts from pure noise (<code>x<sub>T</sub></code>) and iteratively removes noise step-by-step to generate a clean data sample (<code>x<sub>0</sub></code>).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The backward process is modeled by a neural network (parameterized by <code>θ</code>) that learns the conditional <a href="#probability-distribution-glossary">distributions</a> <code>p<sub>θ</sub>(x<sub>t-1</sub> | x<sub>t</sub>)</code> for each denoising step. This network effectively learns to predict and subtract the noise added at each stage of the forward process.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The backward diffusion process, also known as the reverse process, is the core generative component of <a href="#diffusion-model-glossary">diffusion models</a>. It is responsible for synthesizing new data samples by "undoing" the noising operations of the fixed <a href="#forward-diffusion-process-glossary">forward process</a>.
    The goal is to learn the sequence of reverse conditional <a href="#probability-distribution-glossary">distributions</a> <code>p<sub>θ</sub>(x<sub>t-1</sub> | x<sub>t</sub>)</code> for <code>t = T, T-1, ..., 1</code>, starting from a sample <code>x<sub>T</sub></code> drawn from a simple noise distribution (usually <code>N(0, I)</code>).
    The true reverse conditional <code>q(x<sub>t-1</sub> | x<sub>t</sub>, x<sub>0</sub>)</code> (which depends on the unknown <code>x<sub>0</sub></code> when generating) can be shown to be Gaussian when the forward process noise <code>β<sub>t</sub></code> is small. This motivates parameterizing the learned reverse transitions <code>p<sub>θ</sub>(x<sub>t-1</sub> | x<sub>t</sub>)</code> also as Gaussians:
    <pre>p<sub>θ</sub>(x<sub>t-1</sub> | x<sub>t</sub>) = N(x<sub>t-1</sub>; μ<sub>θ</sub>(x<sub>t</sub>, t), Σ<sub>θ</sub>(x<sub>t</sub>, t))</pre>
    A neural network, typically a <a href="#u-net-architecture-glossary">U-Net architecture</a> with parameters <code>θ</code>, is trained to predict the mean <code>μ<sub>θ</sub>(x<sub>t</sub>, t)</code> and often a fixed or learned variance <code>Σ<sub>θ</sub>(x<sub>t</sub>, t)</code> of these reverse steps. The network takes the current noisy state <code>x<sub>t</sub></code> and the timestep <code>t</code> as input.
    A common approach is to train the network <code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code> to predict the noise <code>ε</code> that was added to <code>x<sub>0</sub></code> to obtain <code>x<sub>t</sub></code> (using the formula <code>x<sub>t</sub> = √ᾱ<sub>t</sub>x<sub>0</sub> + √(1-ᾱ<sub>t</sub>)ε</code> from the forward process, where ᾱ<sub>t</sub> is part of the <a href="#noise-schedule-glossary">noise schedule</a>). Once <code>ε<sub>θ</sub></code> is predicted, one can derive <code>μ<sub>θ</sub>(x<sub>t</sub>, t)</code>.
    The training involves optimizing an <a href="#objective-function-glossary">objective function</a> (often a simplified version of the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>, see <a href="#diffusion-model-objective-function-glossary">Diffusion Model Objective Function</a>) that encourages <code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code> to match the true noise <code>ε</code>. This is closely related to <a href="#denoising-score-matching-glossary">Denoising Score Matching</a>.
    During sampling (generation), one starts with <code>x<sub>T</sub> ~ N(0,I)</code> and iteratively applies the learned reverse transitions: <code>x<sub>t-1</sub> ~ p<sub>θ</sub>(x<sub>t-1</sub> | x<sub>t</sub>)</code>, until a clean sample <code>x<sub>0</sub></code> is produced. This <a href="#iterative-inference-generation-glossary">iterative inference/generation</a> process allows diffusion models to generate highly realistic and diverse samples.</p>

    <h3 class="glossary-entry-title" id="bayesian-inference-glossary">Bayesian Inference</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A method of statistical inference in which Bayes’ theorem is used to update the probability for a hypothesis (or parameters) as more evidence or data is acquired.</p>
    <span class="entry-section-title">Explanation:</span>
    <p><a href="#bayesian-inference-glossary">Bayesian inference</a> combines a <a href="#prior-distribution-glossary">prior belief</a> about parameters with the <a href="#likelihood-glossary">likelihood</a> of observed data to form an updated <a href="#posterior-distribution-glossary">posterior belief</a> about the parameters. It's central to probabilistic modeling and reasoning under uncertainty.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Bayesian inference is a statistical framework for learning from data by updating our beliefs about unknown quantities (such as model parameters <code>θ</code> or <a href="#latent-variable-glossary">latent variables</a> <code>z</code>) as new evidence (data <code>x</code>) becomes available. It is based on Bayes’ theorem:
    <pre>P(θ|x) = [P(x|θ) * P(θ)] / P(x)</pre>
    where:
    <ul>
        <li><code>P(θ)</code> is the <a href="#prior-distribution-glossary"><strong>prior distribution</strong></a>: Our initial belief about <code>θ</code> before observing any data.</li>
        <li><code>P(x|θ)</code> is the <a href="#likelihood-glossary"><strong>likelihood</strong></a>: The probability of observing data <code>x</code> given parameters <code>θ</code>.</li>
        <li><code>P(θ|x)</code> is the <a href="#posterior-distribution-glossary"><strong>posterior distribution</strong></a>: Our updated belief about <code>θ</code> after considering the evidence <code>x</code>.</li>
        <li><code>P(x)</code> is the <a href="#marginal-likelihood-glossary"><strong>marginal likelihood</strong></a> (or evidence): <code>P(x) = ∫ P(x|θ)P(θ) dθ</code>. It acts as a normalization constant and is crucial for model comparison.</li>
    </ul>
    This approach is powerful because it provides a principled way to handle uncertainty (the posterior is a full distribution, not just a point estimate) and incorporate prior knowledge. In <a href="#generative-model-glossary">generative models</a> and <a href="#variational-autoencoder-vae-glossary">VAEs</a>, Bayesian inference allows us to reason about the hidden causes (latent variables <code>z</code>) of observed data <code>x</code> and to quantify uncertainty in our predictions. However, calculating the exact posterior <code>P(θ|x)</code> or <code>P(z|x)</code> (and the marginal likelihood <code>P(x)</code>) is often intractable in complex models due to high-dimensional integrals. This motivates the use of approximate inference methods like <a href="#variational-inference-vi-glossary">variational inference</a> and Markov Chain Monte Carlo (MCMC).</p>

    <h3 class="glossary-entry-title" id="bernoulli-distribution-glossary">Bernoulli Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A discrete <a href="#probability-distribution-glossary">probability distribution</a> of a <a href="#random-variable-glossary">random variable</a> which takes the value 1 (success) with probability <code>p</code> and the value 0 (failure) with probability <code>1-p</code>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The Bernoulli distribution models a single trial with two possible outcomes, often labeled as success/failure or yes/no. It is a special case of the Binomial distribution with n=1 trial, and also a special case of the <a href="#categorical-distribution-glossary">Categorical distribution</a> with K=2 categories.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Bernoulli distribution is one of the simplest discrete <a href="#probability-distribution-glossary">probability distributions</a>. It describes an experiment with only two outcomes, typically coded as 1 (success) and 0 (failure).
    The probability mass function (PMF) is given by:
    <pre>P(X=k | p) = p<sup>k</sup> (1-p)<sup>1-k</sup>   for k ∈ {0, 1}</pre>
    where <code>p</code> is the probability of success (X=1).
    <ul>
        <li><code>P(X=1 | p) = p</code></li>
        <li><code>P(X=0 | p) = 1-p</code></li>
    </ul>
    The <a href="#expectation-e-glossary">expectation</a> of a Bernoulli random variable is <code>E[X] = p</code>, and its variance is <code>Var(X) = p(1-p)</code>.
    Applications:
    <ul>
        <li>Modeling the outcome of a single coin flip (heads/tails).</li>
        <li>Representing binary features or events.</li>
        <li>In <a href="#variational-autoencoder-vae-glossary">VAEs</a> or other <a href="#generative-model-glossary">generative models</a>, the decoder <code>p(x|z)</code> might output parameters for a Bernoulli distribution for each pixel if the input images are binary (black and white). The <a href="#log-likelihood-glossary">log-likelihood</a> term in the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> would then be the sum of log probabilities of Bernoulli trials, which is equivalent to binary cross-entropy.</li>
    </ul>
    A sequence of independent Bernoulli trials with the same success probability <code>p</code> forms a Binomial distribution (which counts the number of successes in <code>n</code> trials).</p>

    <h3 class="glossary-entry-title" id="canonical-correlation-analysis-cca-glossary">Canonical Correlation Analysis (CCA)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A statistical method used to identify and quantify the linear relationships between two sets of variables, typically representing two different views or modalities of the same underlying entities.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>CCA finds pairs of linear projections (canonical variates), one for each set of variables, such that the correlation between the projected variables in each pair is maximized. These pairs are found sequentially, with each new pair being uncorrelated with previous pairs. It's used to discover shared low-dimensional structure or common underlying factors between different data modalities.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Canonical Correlation Analysis (CCA) is a multivariate statistical technique that explores the interrelationships between two sets of variables. Suppose we have two sets of variables, <strong>X</strong> (with <code>p</code> variables) and <strong>Y</strong> (with <code>q</code> variables), observed for the same set of <code>n</code> individuals. CCA seeks to find pairs of linear combinations:
    <pre>U<sub>k</sub> = <strong>a</strong><sub>k</sub><sup>T</sup><strong>X</strong></pre>
    <pre>V<sub>k</sub> = <strong>b</strong><sub>k</sub><sup>T</sup><strong>Y</strong></pre>
    called canonical variates, such that:
    <ol>
        <li>The correlation between the first pair (<code>U<sub>1</sub></code>, <code>V<sub>1</sub></code>) is maximized. This correlation is the first canonical correlation.</li>
        <li>The correlation between the second pair (<code>U<sub>2</sub></code>, <code>V<sub>2</sub></code>) is maximized, subject to <code>U<sub>2</sub></code> being uncorrelated with <code>U<sub>1</sub></code>, and <code>V<sub>2</sub></code> being uncorrelated with <code>V<sub>1</sub></code>.</li>
        <li>This continues for up to <code>min(p,q)</code> pairs.</li>
    </ol>
    Essentially, CCA finds a sequence of basis vectors for <strong>X</strong> and <strong>Y</strong> such that the correlation between the projections of the variables onto these basis vectors is mutually maximized.
    Applications in <a href="#multimodal-learning-glossary">multimodal learning</a>:
    <ul>
        <li><strong>Shared Representation Learning:</strong> CCA can be used to find a common subspace where representations from two different modalities (e.g., image features and text features) are maximally correlated. This helps in aligning the modalities.</li>
        <li><strong>Dimensionality Reduction:</strong> The canonical variates often provide a lower-dimensional summary of the shared information between the two sets.</li>
        <li><strong>Cross-Modal Retrieval:</strong> By projecting data from different modalities into the shared CCA space, one can retrieve items from one modality based on a query from another.</li>
    </ul>
    Limitations:
    <ul>
        <li><strong>Linearity:</strong> Classical CCA only captures linear relationships.</li>
        <li><strong>Interpretation:</strong> Canonical variates can sometimes be hard to interpret.</li>
    </ul>
    To address the linearity limitation, nonlinear extensions of CCA have been developed, often using kernel methods (Kernel CCA) or deep neural networks (e.g., Deep CCA - DCCA, Deep Canonically Correlated Autoencoders - DCCAE). These deep learning approaches can learn complex nonlinear transformations to find highly correlated representations between modalities. Understanding CCA provides a foundation for these more advanced multimodal alignment techniques.</p>

    <h3 class="glossary-entry-title" id="categorical-distribution-glossary">Categorical Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A <a href="#probability-distribution-glossary">probability distribution</a> over a discrete set of K possible outcomes or categories, where each outcome has a specific probability.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The categorical distribution describes the probabilities of a <a href="#random-variable-glossary">random variable</a> that can take on one of K fixed, mutually exclusive categories (e.g., the outcome of rolling a K-sided die, or choosing one class out of K classes). It's a generalization of the <a href="#bernoulli-distribution-glossary">Bernoulli distribution</a> (K=2).</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The categorical distribution is used to model a discrete <a href="#random-variable-glossary">random variable</a> that can take on one of <code>K</code> possible distinct, unordered outcomes or categories. It is parameterized by a vector of probabilities <code><strong>p</strong> = (p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>K</sub>)</code>, where:
    <ul>
        <li><code>p<sub>k</sub></code> is the probability of the <code>k</code>-th outcome.</li>
        <li>Each <code>p<sub>k</sub> ≥ 0</code>.</li>
        <li>The sum of probabilities is 1: <code>Σ<sup>K</sup><sub>k=1</sub> p<sub>k</sub> = 1</code>.</li>
    </ul>
    If a random variable <code>X</code> follows a categorical distribution with parameter <code><strong>p</strong></code>, then <code>P(X=k) = p<sub>k</sub></code>.
    Examples:
    <ul>
        <li>The outcome of rolling a standard six-sided die (K=6, <code>p<sub>k</sub> = 1/6</code> for all k if fair).</li>
        <li>The class label for an object in a K-class classification problem.</li>
        <li>The choice of a word from a vocabulary of size K.</li>
    </ul>
    When K=2, the categorical distribution reduces to the <a href="#bernoulli-distribution-glossary">Bernoulli distribution</a>.
    In <a href="#generative-model-glossary">generative models</a> and clustering (such as <a href="#gaussian-mixture-model-gmm-glossary">Gaussian Mixture Models</a>), categorical distributions are often used to represent:
    <ul>
        <li>A <a href="#latent-variable-glossary">latent variable</a> indicating which component of a <a href="#mixture-model-glossary">mixture model</a> generated a data point.</li>
        <li>The distribution over discrete choices or actions.</li>
    </ul>
    In <a href="#variational-autoencoder-vae-glossary">VAEs</a> with mixture priors (e.g., a <a href="#gaussian-mixture-model-gmm-glossary">GMM prior</a>), a categorical latent variable might select which Gaussian component of the prior is used to generate the continuous latent variables. Sampling from a categorical distribution involves selecting an outcome based on its assigned probability. Understanding categorical distributions is essential for working with discrete latent variables and for modeling data that is naturally grouped into distinct categories.</p>

    <h3 class="glossary-entry-title" id="conditional-distribution-glossary">Conditional Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#probability-distribution-glossary">probability distribution</a> of one variable given that another variable is fixed at a certain value.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Conditional distributions answer questions like, "What is the probability of event A, given that event B has happened?"</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A conditional distribution describes the probability of one event or variable, assuming that another event or variable is known. For example, if you know it is raining, the probability of carrying an umbrella is different than if you don’t know the weather. Mathematically, the conditional probability of A given B is written as <code>P(A|B)</code>, and for continuous variables, the conditional density is <code>f(x|y)</code>. Conditional distributions are central to <a href="#bayesian-inference-glossary">Bayesian inference</a>, where we are often interested in the distribution of unknown parameters (or <a href="#latent-variable-glossary">latent variables</a>) given observed data. In <a href="#generative-model-glossary">generative models</a>, the conditional distribution is used both in generating new data (e.g., generating an image given a label) and in inference (e.g., inferring the latent variables given observed data). Mastering conditional distributions is essential for understanding how information flows in probabilistic models.</p>

    <h3 class="glossary-entry-title" id="consensus-distribution-glossary">Consensus Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#multimodal-learning-glossary">multimodal learning</a>, a single <a href="#probability-distribution-glossary">probability distribution</a> over shared <a href="#latent-variable-glossary">latent variables</a> that is formed by combining information from multiple "expert" distributions, where each expert distribution is typically derived from a single modality of data.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The goal of a consensus distribution is to synthesize the knowledge or beliefs from different data sources (modalities) into a single, coherent probabilistic representation of the shared underlying <a href="#latent-variable-glossary">latent variables</a>. This is crucial in multimodal <a href="#generative-model-glossary">generative models</a> for robust inference and generation, especially when handling varying subsets of available modalities.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A consensus distribution is a key concept in <a href="#multimodal-learning-glossary">multimodal</a> <a href="#generative-model-glossary">generative models</a>, particularly multimodal <a href="#variational-autoencoder-vae-glossary">VAEs</a>. When data is available from multiple modalities (e.g., image <code>x<sub>I</sub></code>, text <code>x<sub>T</sub></code>), each modality can provide information about a shared <a href="#latent-variable-glossary">latent representation</a> <code>z</code> via its own unimodal inference network (encoder), yielding unimodal approximate posteriors like <code>q(z|x<sub>I</sub>)</code> and <code>q(z|x<sub>T</sub>)</code>.
    A consensus distribution aims to combine these unimodal "expert" opinions into a single, unified distribution that approximates the true <a href="#joint-posterior-distribution-glossary">joint posterior</a> <code>P(z | x<sub>I</sub>, x<sub>T</sub>, ...)</code>, which considers all available evidence simultaneously.
    Forming a consensus distribution is essential for:
    <ul>
        <li><strong>Information Fusion:</strong> Integrating diverse information to obtain a more accurate and robust estimate of the latent variables.</li>
        <li><strong>Handling Missing Modalities:</strong> If some modalities are unavailable at test time, a consensus mechanism can still produce a reasonable posterior based on the present modalities. For instance, if only an image is available, the consensus should ideally approximate <code>q(z|x<sub>I</sub>)</code>. If both image and text are available, it should approximate <code>q(z|x<sub>I</sub>, x<sub>T</sub>)</code>.</li>
        <li><strong>Enabling <a href="#cross-modal-generation-glossary">Cross-Modal Generation</a>:</strong> By inferring <code>z</code> using the consensus from available input modalities, one can then decode <code>z</code> into any target modality.</li>
    </ul>
    Common methods for creating consensus distributions include:
    <ul>
        <li><a href="#product-of-experts-poe-glossary"><strong>Product of Experts (PoE)</strong></a>: Multiplies the (probabilities of) expert distributions. Tends to be sharp but can be overconfident if experts are correlated.</li>
        <li><a href="#mixture-of-experts-moe-glossary"><strong>Mixture of Experts (MoE)</strong></a>: Takes a weighted average of expert distributions. More flexible and robust to outlier experts but can be less sharp.</li>
        <li>More sophisticated dynamic mechanisms, like <a href="#consensus-of-dependent-experts-code-glossary">Consensus of Dependent Experts (CoDE)</a>, that might try to learn how to best combine experts based on their reliability or dependencies.</li>
    </ul>
    The choice of consensus method significantly impacts how well a multimodal model can integrate information and generalize across different combinations of input modalities.</p>

    <h3 class="glossary-entry-title" id="consensus-of-dependent-experts-code-glossary">Consensus of Dependent Experts (CoDE)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An advanced method for forming a <a href="#consensus-distribution-glossary">consensus distribution</a> in <a href="#multimodal-learning-glossary">multimodal settings</a> that explicitly models and accounts for statistical dependencies (correlations) between the different "expert" opinions (unimodal posteriors).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>CoDE aims to improve upon simpler consensus methods like <a href="#product-of-experts-poe-glossary">PoE</a> (which assumes conditional independence) and <a href="#mixture-of-experts-moe-glossary">MoE</a> by acknowledging that information derived from different modalities might be correlated. By modeling these dependencies, CoDE can lead to more accurate and robust <a href="#joint-posterior-distribution-glossary">joint posterior</a> approximations.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Consensus of Dependent Experts (CoDE) addresses a key limitation inherent in simpler methods for forming <a href="#consensus-distribution-glossary">consensus distributions</a> in <a href="#multimodal-learning-glossary">multimodal models</a>. Standard <a href="#product-of-experts-poe-glossary">Product of Experts (PoE)</a> often assumes that the unimodal "expert" posteriors (e.g., <code>q(z|x<sub>image</sub>)</code>, <code>q(z|x<sub>text</sub>)</code>) are conditionally independent given the <a href="#latent-variable-glossary">latent variable</a> <code>z</code>. However, in reality, the information (and errors) from different modalities can be correlated. For instance, if an image is ambiguous, the text caption might also be ambiguous in a related way.
    CoDE methods attempt to explicitly model these dependencies. Instead of just multiplying or averaging the expert distributions, they might:
    <ul>
        <li>Consider the covariance structure between the parameters (e.g., means and variances) predicted by the unimodal encoders.</li>
        <li>Treat the outputs of unimodal encoders as noisy observations of a true underlying latent state <code>z</code>, where the "noise" across modalities can be correlated.</li>
        <li>Employ techniques from multivariate statistics or graphical models to derive a Bayesian posterior for <code>z</code> that incorporates these estimated correlations.</li>
    </ul>
    For example, if experts are Gaussian, CoDE might involve estimating a full covariance matrix for the joint distribution of the means predicted by each expert, rather than assuming these means are independent given <code>z</code>.
    The goal is to achieve a consensus that is:
    <ul>
        <li>More accurate than PoE if experts are positively correlated in their errors (PoE might become too sharp/overconfident).</li>
        <li>Sharper than MoE by properly integrating redundant or complementary information.</li>
        <li>More robust to situations where some experts might be systematically biased or correlated in their mistakes.</li>
    </ul>
    By accounting for dependencies, CoDE aims to provide a more nuanced and reliable approximation of the true <a href="#joint-posterior-distribution-glossary">joint posterior distribution</a>, which is critical for high-performance multimodal inference and generation, especially in complex scenarios with many modalities or varying data quality.</p>

    <h3 class="glossary-entry-title" id="cross-modal-generation-glossary">Cross-Modal Generation</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The task in <a href="#multimodal-learning-glossary">multimodal learning</a> of generating data in one modality (e.g., text) conditioned on input data from a different modality (e.g., an image), or vice-versa.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Cross-modal generation showcases a model's ability to "translate" information between different data types. Prominent examples include generating a caption from an image (image-to-text synthesis), creating an image from a text description (text-to-image synthesis), or synthesizing speech from text (text-to-speech).</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Cross-modal generation is a significant capability and a common benchmark task for advanced <a href="#generative-model-glossary">generative models</a> designed for <a href="#multimodal-learning-glossary">multimodal learning</a>. It involves synthesizing data in a target modality based on input information from one or more source modalities.
    Key aspects and examples:
    <ul>
        <li><strong>Input Modality → Output Modality:</strong>
            <ul>
                <li><strong>Image → Text:</strong> Image Captioning (e.g., generating "a cat sitting on a mat" from an image of a cat).</li>
                <li><strong>Text → Image:</strong> Text-to-Image Synthesis (e.g., generating an image of "an astronaut riding a horse" from that text prompt).</li>
                <li><strong>Text → Speech:</strong> Text-to-Speech (TTS) synthesis.</li>
                <li><strong>Speech → Text:</strong> Automatic Speech Recognition (ASR), though often viewed as discriminative, the underlying acoustic models can have generative components.</li>
                <li><strong>Video → Text:</strong> Video Captioning or Description.</li>
                <li><strong>Text → Video:</strong> Generating video clips from textual descriptions.</li>
                <li><strong>Audio → Image/Video:</strong> E.g., generating visuals that synchronize with music.</li>
            </ul>
        </li>
        <li><strong>Underlying Mechanism:</strong> Successful cross-modal generation typically relies on learning a good shared or coordinated <a href="#latent-variable-glossary">latent space</a> where information from different modalities can be represented and related. The process usually involves:
            <ol>
                <li>Encoding the input modality into this latent space (obtaining <code>z</code>).</li>
                <li>Conditioning a generative model (decoder) for the target modality on this latent representation <code>z</code> to produce the output.</li>
            </ol>
            In multimodal <a href="#variational-autoencoder-vae-glossary">VAEs</a>, this often involves inferring a <a href="#consensus-distribution-glossary">consensus latent representation</a> from the input modality(ies) and then decoding it into the target modality.
        </li>
        <li><strong>Challenges:</strong> Ensuring semantic consistency, coherence, and high fidelity in the generated output, especially for complex modalities like images and video. Handling the "one-to-many" nature of translation (e.g., one image can have many valid captions).</li>
    </ul>
    Cross-modal generation is crucial for a wide array of applications that require AI to understand and interact with the world using multiple senses or types of information, such as advanced user interfaces, content creation tools, robotics, and assistive technologies. It demonstrates a deeper level of "understanding" and flexibility in probabilistic generative models.</p>

    <h3 class="glossary-entry-title" id="denoising-diffusion-probabilistic-models-ddpm-glossary">Denoising Diffusion Probabilistic Models (DDPM)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A specific and influential type of <a href="#diffusion-model-glossary">diffusion model</a> that generates data by stepwise denoising, characterized by a particular parameterization and training objective.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>DDPMs define a <a href="#forward-diffusion-process-glossary">forward noising process</a> that gradually adds Gaussian noise to data, and a learned <a href="#backward-diffusion-process-glossary">backward denoising process</a> (typically a <a href="#u-net-architecture-glossary">U-Net</a>) trained to predict and reverse this noise. They often use a simplified <a href="#objective-function-glossary">objective function</a> based on predicting the added noise at each step.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Denoising Diffusion Probabilistic Models (DDPMs) are a class of <a href="#generative-model-glossary">generative models</a> that have achieved remarkable success, particularly in image synthesis. They are a specific instance of the broader <a href="#diffusion-model-glossary">diffusion model</a> framework.
    Key characteristics of DDPMs include:
    <ol>
        <li><strong>Forward Process (<code>q</code>):</strong> A fixed <a href="#markov-process-markov-chain-glossary">Markov chain</a> that gradually adds Gaussian noise to an initial data sample <code>x<sub>0</sub></code> over <code>T</code> (e.g., 1000) discrete time steps, following a predefined <a href="#noise-schedule-glossary">variance schedule</a> <code>{β<sub>t</sub>}</code>. As described in the <a href="#forward-diffusion-process-glossary">Forward Diffusion Process</a> entry, <code>x<sub>T</sub></code> becomes approximately <code>N(0, I)</code>.</li>
        <li><strong>Backward Process (<code>p<sub>θ</sub></code>):</strong> A learned <a href="#markov-process-markov-chain-glossary">Markov chain</a> that reverses the forward process. It starts with <code>x<sub>T</sub> ~ N(0,I)</code> and iteratively denoises to produce <code>x<sub>0</sub></code>. Each reverse transition <code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> is parameterized as a Gaussian:
        <pre>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>) = N(x<sub>t-1</sub>; μ<sub>θ</sub>(x<sub>t</sub>, t), σ<sup>2</sup><sub>t</sub>I)</pre>
        The variance <code>σ<sup>2</sup><sub>t</sub></code> is often fixed (e.g., to <code>β<sub>t</sub></code> or a related quantity from the <a href="#noise-schedule-glossary">noise schedule</a>). The mean <code>μ<sub>θ</sub>(x<sub>t</sub>, t)</code> is derived from a neural network <code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code> (typically a <a href="#u-net-architecture-glossary">U-Net</a>) that is trained to predict the noise component <code>ε</code> in <code>x<sub>t</sub></code>, where <code>x<sub>t</sub> = √ᾱ<sub>t</sub>x<sub>0</sub> + √(1-ᾱ<sub>t</sub>)ε</code> (ᾱ<sub>t</sub> is from the <a href="#noise-schedule-glossary">noise schedule</a>). Specifically:
        <pre>μ<sub>θ</sub>(x<sub>t</sub>, t) = (1/√α<sub>t</sub>) * (x<sub>t</sub> - (β<sub>t</sub> / √(1-ᾱ<sub>t</sub>)) * ε<sub>θ</sub>(x<sub>t</sub>, t))</pre>
        where <code>α<sub>t</sub> = 1-β<sub>t</sub></code>.</li>
        <li><strong>Training Objective:</strong> DDPMs are trained by optimizing a variational bound on the <a href="#log-likelihood-glossary">log likelihood</a> (see <a href="#diffusion-model-objective-function-glossary">Diffusion Model Objective Function</a>, related to the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>). A common simplification of this objective, related to <a href="#denoising-score-matching-glossary">Denoising Score Matching</a>, leads to a mean squared error loss between the true noise <code>ε</code> (used to generate <code>x<sub>t</sub></code> from <code>x<sub>0</sub></code>) and the noise <code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code> predicted by the network:
        <pre>L<sub>simple</sub>(θ) = E<sub>t~U(1,T), x<sub>0</sub>~data, ε~N(0,I)</sub>[ ||ε - ε<sub>θ</sub>(√ᾱ<sub>t</sub>x<sub>0</sub> + √(1-ᾱ<sub>t</sub>)ε, t)||² ]</pre>
        </li>
    </ol>
    Sampling involves starting with <code>x<sub>T</sub> ~ N(0,I)</code> and then iteratively applying the learned denoising steps using the neural network's prediction <code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code> to compute <code>μ<sub>θ</sub>(x<sub>t</sub>, t)</code> and sample <code>x<sub>t-1</sub></code>. DDPMs are known for their stable training process and their ability to generate exceptionally high-quality and diverse samples, making them a prominent architecture in modern generative AI.</p>

    <h3 class="glossary-entry-title" id="denoising-score-matching-glossary">Denoising Score Matching</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An objective for learning a probability distribution by training a model to predict the "score" (gradient of the log probability density) of data points perturbed by noise. The training often involves predicting the noise itself.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Instead of directly estimating the probability density <code>p(x)</code> or its log, score matching aims to learn <code>∇<sub>x</sub> log p(x)</code>. Denoising Score Matching (DSM) trains a model <code>s<sub>θ</sub>(x̃)</code> to estimate the score of a noisy data distribution <code>p<sub>σ</sub>(x̃)</code> (where <code>x̃</code> is <code>x + noise</code>). This is often equivalent to training the model to denoise <code>x̃</code>, i.e., predict the noise that was added, which is a key idea in <a href="#diffusion-model-glossary">Diffusion Models</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Score matching provides a way to learn an unnormalized statistical model without needing to compute the (often intractable) normalization constant (partition function). The "score" of a data point <code>x</code> with respect to a distribution <code>p(x)</code> is defined as the gradient of the log-density with respect to the data: <code>s(x) = ∇<sub>x</sub> log p(x)</code>.
    The original score matching objective involves minimizing the expected squared difference between the model's score estimate <code>s<sub>θ</sub>(x)</code> and the true score <code>∇<sub>x</sub> log p<sub>data</sub>(x)</code>. However, this requires knowing the true score of the data, which is unknown.
    <strong>Denoising Score Matching (DSM)</strong> offers a more practical alternative. It works by:
    <ol>
        <li>Defining a noise distribution <code>q<sub>σ</sub>(x̃|x)</code> (e.g., Gaussian noise <code>N(x̃; x, σ²I)</code>) that perturbs clean data <code>x</code> to get noisy data <code>x̃</code>.</li>
        <li>Training a model <code>s<sub>θ</sub>(x̃)</code> to estimate the score of the noisy data distribution <code>q<sub>σ</sub>(x̃) = ∫ q<sub>σ</sub>(x̃|x)p<sub>data</sub>(x)dx</code>.</li>
    </ol>
    A key result is that the DSM objective can be simplified to an expected squared error between the model's score output and the score of the noise conditional, or often, more intuitively, by training the model to predict the noise that was added to the clean data to produce the noisy version.
    Specifically, for Gaussian noise, training a denoiser <code>r<sub>θ</sub>(x̃) ≈ x</code> is equivalent to learning the score. If the model <code>ε<sub>θ</sub>(x̃)</code> predicts the noise <code>ε = x̃ - x</code>, then the score can be related to this noise prediction: <code>s<sub>θ</sub>(x̃) ≈ -(x̃ - r<sub>θ</sub>(x̃))/σ² = -ε<sub>θ</sub>(x̃)/σ²</code>.
    The simplified loss often used in <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">DDPMs</a>, <code>E[ ||ε - ε<sub>θ</sub>(x<sub>t</sub>, t)||² ]</code>, where <code>ε<sub>θ</sub></code> predicts the noise <code>ε</code> in the noisy data <code>x<sub>t</sub></code>, is directly inspired by and closely related to Denoising Score Matching principles. This connection allows diffusion models to implicitly learn the score function of the data distribution at various noise levels, which then guides the <a href="#backward-diffusion-process-glossary">backward (generative) process</a>.</p>

    <h3 class="glossary-entry-title" id="diffusion-model-glossary">Diffusion Model</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A type of <a href="#generative-model-glossary">generative model</a> that learns to create data by systematically reversing a predefined, gradual noising process (the <a href="#forward-diffusion-process-glossary">forward diffusion process</a>).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Diffusion models operate in two main stages:
    1. A fixed <a href="#forward-diffusion-process-glossary"><strong>forward process</strong></a> that progressively adds noise (usually Gaussian) to data over many steps until it becomes nearly pure noise.
    2. A learned <a href="#backward-diffusion-process-glossary"><strong>backward (denoising) process</strong></a> that starts from pure noise and iteratively removes the noise, guided by a neural network, to generate new, realistic samples.
    </p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Diffusion models are a powerful class of <a href="#generative-model-glossary">generative models</a> inspired by non-equilibrium thermodynamics and physical diffusion processes. They have demonstrated remarkable success in generating high-fidelity data, especially images.
    The core mechanism involves two processes:
    <ol>
        <li><strong>Forward Diffusion Process (<code>q</code>):</strong> This is a fixed (not learned) <a href="#markov-process-markov-chain-glossary">Markov chain</a> that gradually adds small amounts of Gaussian noise to an initial data sample <code>x<sub>0</sub></code> over a sequence of <code>T</code> discrete time steps.
        <pre>q(x<sub>t</sub> | x<sub>t-1</sub>) = N(x<sub>t</sub>; √(1-β<sub>t</sub>)x<sub>t-1</sub>, β<sub>t</sub>I)</pre>
        The variances <code>β<sub>t</sub></code> (part of the <a href="#noise-schedule-glossary">noise schedule</a>) are chosen such that after <code>T</code> steps, <code>x<sub>T</sub></code> is approximately distributed as a standard normal distribution <code>N(0, I)</code>.</li>
        <li><strong>Backward (Reverse) Diffusion Process (<code>p<sub>θ</sub></code>):</strong> This is the learned generative process that aims to reverse the forward diffusion. It starts with a sample <code>x<sub>T</sub></code> from the noise distribution <code>N(0, I)</code> and iteratively denoises it to produce a sample <code>x<sub>0</sub></code>:
        <pre>p<sub>θ</sub>(x<sub>t-1</sub> | x<sub>t</sub>) = N(x<sub>t-1</sub>; μ<sub>θ</sub>(x<sub>t</sub>, t), Σ<sub>θ</sub>(x<sub>t</sub>, t))</pre>
        A neural network (often a <a href="#u-net-architecture-glossary">U-Net architecture</a>), parameterized by <code>θ</code>, is trained to predict the parameters of these reverse transitions, typically by predicting the noise that was added at step <code>t</code> (<code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code>). This training is related to <a href="#denoising-score-matching-glossary">Denoising Score Matching</a>.</li>
    </ol>
    The training objective for diffusion models is usually derived from a variational lower bound on the data <a href="#log-likelihood-glossary">log-likelihood</a> (see <a href="#diffusion-model-objective-function-glossary">Diffusion Model Objective Function</a>, related to the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>). This often simplifies to a mean squared error loss between the true added noise and the noise predicted by the neural network.
    Diffusion models are known for their stable training and their ability to generate diverse, high-quality samples, often outperforming other generative architectures like GANs and <a href="#variational-autoencoder-vae-glossary">VAEs</a> in terms of sample fidelity. See also <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">DDPM</a> and <a href="#iterative-inference-generation-glossary">Iterative Inference/Generation</a>.</p>

    <h3 class="glossary-entry-title" id="diffusion-model-objective-function-glossary">Diffusion Model Objective Function</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#objective-function-glossary">objective function</a> used to train <a href="#diffusion-model-glossary">diffusion models</a>, typically a variational lower bound (<a href="#evidence-lower-bound-elbo-glossary">ELBO</a>) on the <a href="#log-likelihood-glossary">log likelihood</a> of the data, or a simplified version thereof.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The objective aims to train the <a href="#backward-diffusion-process-glossary">backward (denoising) process</a> <code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> to accurately reverse the <a href="#forward-diffusion-process-glossary">forward noising process</a> <code>q(x<sub>t</sub>|x<sub>t-1</sub>)</code>. A common formulation involves minimizing the difference between the predicted noise and the actual noise added at each step, which is derived from the ELBO.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The training of a <a href="#diffusion-model-glossary">diffusion model</a> aims to maximize the <a href="#log-likelihood-glossary">log likelihood</a> of the observed data <code>x<sub>0</sub></code>. Since this is generally intractable, an <a href="#evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</a> is maximized instead. For diffusion models, the ELBO involves an expectation over the entire noising trajectory <code>x<sub>1:T</sub></code> generated by the <a href="#forward-diffusion-process-glossary">forward process</a> <code>q(x<sub>1:T</sub>|x<sub>0</sub>)</code>:
    <pre>log p<sub>θ</sub>(x<sub>0</sub>) ≥ L<sub>VLB</sub> = E<sub>q(x<sub>1:T</sub>|x<sub>0</sub>)</sub> [ log ( p<sub>θ</sub>(x<sub>0:T</sub>) / q(x<sub>1:T</sub>|x<sub>0</sub>) ) ]</pre>
    where <code>p<sub>θ</sub>(x<sub>0:T</sub>) = p(x<sub>T</sub>) Π<sup>T</sup><sub>t=1</sub> p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> is the joint distribution of the <a href="#backward-diffusion-process-glossary">reverse process</a> and <code>q(x<sub>1:T</sub>|x<sub>0</sub>) = Π<sup>T</sup><sub>t=1</sub> q(x<sub>t</sub>|x<sub>t-1</sub>)</code> is the joint of the forward process.
    This ELBO can be rewritten as a sum of several terms:
    <pre>L<sub>VLB</sub> = E<sub>q</sub>[log p<sub>θ</sub>(x<sub>0</sub>|x<sub>1</sub>)] - Σ<sup>T-1</sup><sub>t=1</sub> E<sub>q</sub>[D<sub>KL</sub>(q(x<sub>t</sub>|x<sub>t+1</sub>,x<sub>0</sub>) || p<sub>θ</sub>(x<sub>t</sub>|x<sub>t+1</sub>))] - E<sub>q</sub>[D<sub>KL</sub>(q(x<sub>T</sub>|x<sub>0</sub>) || p(x<sub>T</sub>))]</pre>
    The terms involve:
    <ul>
        <li>A reconstruction term <code>log p<sub>θ</sub>(x<sub>0</sub>|x<sub>1</sub>)</code> for the final denoising step.</li>
        <li><a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergences</a> between the true reverse conditionals <code>q(x<sub>t</sub>|x<sub>t+1</sub>,x<sub>0</sub>)</code> (which are tractable Gaussians) and the learned reverse conditionals <code>p<sub>θ</sub>(x<sub>t</sub>|x<sub>t+1</sub>)</code>. These terms encourage the learned denoising steps to match the "true" denoising steps.</li>
        <li>A KL divergence matching the final noisy state <code>q(x<sub>T</sub>|x<sub>0</sub>)</code> to the noise prior <code>p(x<sub>T</sub>)</code> (usually <code>N(0,I)</code>). This term has no learnable parameters if the <a href="#noise-schedule-glossary">noise schedule</a> ensures <code>q(x<sub>T</sub>|x<sub>0</sub>)</code> is already <code>N(0,I)</code>.</li>
    </ul>
    For practical training, especially in <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">DDPMs</a>, a simplified objective is often used. If the reverse process variance is fixed, optimizing the KL terms primarily involves matching the means. This leads to training a neural network <code>ε<sub>θ</sub>(x<sub>t</sub>, t)</code> to predict the noise <code>ε</code> that was added to <code>x<sub>0</sub></code> to get <code>x<sub>t</sub></code> (where <code>x<sub>t</sub> = √ᾱ<sub>t</sub>x<sub>0</sub> + √(1-ᾱ<sub>t</sub>)ε</code>). The simplified loss is:
    <pre>L<sub>simple</sub>(θ) = E<sub>t~U(1,T), x<sub>0</sub>~data, ε~N(0,I)</sub>[ ||ε - ε<sub>θ</sub>(x<sub>t</sub>, t)||² ]</pre>
    This objective is closely related to <a href="#denoising-score-matching-glossary">Denoising Score Matching</a>. Optimizing this simplified loss has been shown to be effective in training high-quality diffusion models.</p>

    <h3 class="glossary-entry-title" id="evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An <a href="#objective-function-glossary">objective function</a> central to <a href="#variational-inference-vi-glossary">variational inference</a>. Maximizing the ELBO simultaneously improves the approximation to the true <a href="#posterior-distribution-glossary">posterior</a> and provides a lower bound on the <a href="#log-likelihood-glossary">log</a> <a href="#marginal-likelihood-glossary">marginal likelihood</a> (log evidence).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The ELBO, <code>L(φ, θ)</code>, is a tractable quantity that is maximized during the training of models like <a href="#variational-autoencoder-vae-glossary">VAEs</a>. It has two common interpretations:
    1. <code>ELBO = E<sub>q<sub>φ</sub>(z|x)</sub>[log P<sub>θ</sub>(x|z)] - KL(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z))</code> (Reconstruction + Regularization)
    2. <code>ELBO = log P<sub>θ</sub>(x) - KL(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z|x))</code> (Log evidence - KL to true posterior)
    </p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Evidence Lower Bound (ELBO) is a crucial quantity in <a href="#variational-inference-vi-glossary">variational inference (VI)</a> and models like <a href="#variational-autoencoder-vae-glossary">Variational Autoencoders (VAEs)</a> and <a href="#diffusion-model-glossary">Diffusion Models</a> (see <a href="#diffusion-model-objective-function-glossary">Diffusion Model Objective Function</a>). The <a href="#marginal-likelihood-glossary">log marginal likelihood</a> (or log evidence), <code>log P<sub>θ</sub>(x)</code>, is often intractable to compute directly. The ELBO provides a tractable lower bound to this quantity:
    <pre>log P<sub>θ</sub>(x) ≥ ELBO(φ, θ)</pre>
    where <code>q<sub>φ</sub>(z|x)</code> is the approximate <a href="#posterior-distribution-glossary">posterior</a> (parameterized by <code>φ</code>) and <code>P<sub>θ</sub>(x,z)</code> is the joint distribution of data and <a href="#latent-variable-glossary">latent variables</a> (parameterized by <code>θ</code>).

    There are several equivalent ways to write the ELBO, each offering different insights:

    1.  <strong>Reconstruction + Regularization Form:</strong>
        <pre>ELBO = E<sub>q<sub>φ</sub>(z|x)</sub>[log P<sub>θ</sub>(x|z)] - D<sub>KL</sub>(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z))</pre>
        <ul>
            <li><code>E<sub>q<sub>φ</sub>(z|x)</sub>[log P<sub>θ</sub>(x|z)]</code>: The expected <a href="#log-likelihood-glossary">log-likelihood</a> of the data <code>x</code> given latent variables <code>z</code> sampled from the approximate posterior <code>q<sub>φ</sub>(z|x)</code>. This term encourages the model (decoder <code>P<sub>θ</sub>(x|z)</code>) to accurately reconstruct the data.</li>
            <li><code>D<sub>KL</sub>(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z))</code>: The <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a> between the approximate posterior <code>q<sub>φ</sub>(z|x)</code> (encoder output) and the <a href="#prior-distribution-glossary">prior</a> <code>P<sub>θ</sub>(z)</code> over latent variables. This term acts as a regularizer, encouraging the learned latent representations to stay close to the prior. It is subtracted, so maximizing the ELBO aims to minimize this KL divergence.</li>
        </ul>

    2.  <strong>Accuracy + Complexity Form (related to the definition of VI):</strong>
        <pre>ELBO = E<sub>q<sub>φ</sub>(z|x)</sub>[log P<sub>θ</sub>(x,z)] - E<sub>q<sub>φ</sub>(z|x)</sub>[log q<sub>φ</sub>(z|x)]</pre>
        <ul>
            <li><code>E<sub>q<sub>φ</sub>(z|x)</sub>[log P<sub>θ</sub>(x,z)]</code>: Expected log joint probability under the approximate posterior. Encourages <code>q</code> to put mass on <code>z</code> values that make the data and latent variables jointly probable under the model.</li>
            <li><code>-E<sub>q<sub>φ</sub>(z|x)</sub>[log q<sub>φ</sub>(z|x)]</code>: The entropy of the approximate posterior <code>q<sub>φ</sub>(z|x)</code>. This term encourages <code>q</code> to be more spread out (higher entropy), preventing it from collapsing to a point.</li>
        </ul>

    The relationship <code>log P<sub>θ</sub>(x) = ELBO(φ, θ) + D<sub>KL</sub>(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z|x))</code> shows that maximizing the ELBO with respect to <code>φ</code> is equivalent to minimizing the KL divergence between the approximate posterior <code>q<sub>φ</sub>(z|x)</code> and the true (but intractable) posterior <code>P<sub>θ</sub>(z|x)</code>. Simultaneously, maximizing the ELBO with respect to <code>θ</code> improves the model's fit to the data. In VAEs, <code>φ</code> represents encoder parameters and <code>θ</code> represents decoder parameters.</p>

    <h3 class="glossary-entry-title" id="expectation-e-glossary">Expectation (E)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The average or mean value of a <a href="#random-variable-glossary">random variable</a>, weighted by its probability.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Also called the expected value, this is what you would get if you could repeat a random process infinitely many times and average the results.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The expectation (or expected value) of a <a href="#random-variable-glossary">random variable</a> is a central concept in probability and statistics. It provides a single number summarizing the "center" of a <a href="#probability-distribution-glossary">probability distribution</a>. For a discrete random variable, the expectation is calculated by summing over all possible values, each weighted by its probability. For a continuous random variable, it’s computed as an <a href="#integration-glossary">integral</a> over all possible values, weighted by the probability density. In mathematical notation, the expectation of a function <code>f(X)</code> with respect to the random variable X is written as <code>E[f(X)]</code>. In machine learning and <a href="#generative-model-glossary">generative modeling</a>, expectations are everywhere: they appear in loss functions (like the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>), in the computation of averages over data and <a href="#latent-variable-glossary">latent variables</a>, and in the derivation of key results. Understanding expectations is crucial for interpreting model behavior and for designing algorithms that learn from data, often estimated via Monte Carlo methods when direct computation is intractable (see <a href="#stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent</a>).</p>

    <h3 class="glossary-entry-title" id="expectation-maximization-em-algorithm-glossary">Expectation-Maximization (EM) Algorithm</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An iterative method to find Maximum <a href="#likelihood-glossary">Likelihood</a> or Maximum A Posteriori (MAP) estimates of parameters in statistical models with unobserved <a href="#latent-variable-glossary">latent variables</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The EM algorithm alternates between an Expectation step (E-step), which computes the <a href="#expectation-e-glossary">expectation</a> of the <a href="#log-likelihood-glossary">log-likelihood</a> with respect to the <a href="#conditional-distribution-glossary">conditional distribution</a> of latent variables given current parameter estimates, and a Maximization step (M-step), which updates parameters to maximize this expected log-likelihood.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Expectation-Maximization (EM) algorithm is a widely used approach for parameter estimation in models where some variables are unobserved or latent (e.g., mixture components in a <a href="#gaussian-mixture-model-gmm-glossary">GMM</a>, hidden states in an HMM). Direct optimization of the <a href="#likelihood-glossary">likelihood</a> is often difficult due to these hidden variables. EM provides an iterative two-step process:
    <ol>
        <li><strong>E-step (Expectation):</strong> Given the current estimate of the model parameters <code>θ<sup>(t)</sup></code>, compute the <a href="#expectation-e-glossary">expected value</a> of the complete-data <a href="#log-likelihood-glossary">log-likelihood</a>, where the expectation is taken with respect to the <a href="#conditional-distribution-glossary">conditional distribution</a> of the <a href="#latent-variable-glossary">latent variables</a> <code>Z</code> given the observed data <code>X</code> and current parameters <code>θ<sup>(t)</sup></code>. This essentially involves "filling in" or estimating the contribution of the latent variables.
        Functionally, calculate: <code>Q(θ | θ<sup>(t)</sup>) = E<sub>Z|X,θ<sup>(t)</sup></sub>[log P(X,Z|θ)]</code>.</li>
        <li><strong>M-step (Maximization):</strong> Find the parameters <code>θ<sup>(t+1)</sup></code> that maximize the Q-function computed in the E-step:
        <code>θ<sup>(t+1)</sup> = argmax<sub>θ</sub> Q(θ | θ<sup>(t)</sup>)</code>.</li>
    </ol>
    This process is repeated until convergence (e.g., when the change in log-likelihood or parameters is small). Each iteration of EM is guaranteed to increase the log-likelihood (or keep it the same). <a href="#variational-inference-vi-glossary">Variational inference</a> can be seen as a generalization of the EM algorithm, particularly when the E-step is intractable; the E-step in VI involves optimizing an approximate <a href="#posterior-distribution-glossary">posterior</a> over latent variables.</p>

    <h3 class="glossary-entry-title" id="exponent-exponential-function-glossary">Exponent (Exponential Function)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A mathematical operation indicating repeated multiplication; e.g., <code>e<sup>x</sup></code>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Exponential functions describe growth or decay processes and are the mathematical inverse of <a href="#logarithm-log-natural-logarithm-ln-glossary">logarithms</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Exponentiation is the process of raising a number (the base) to a power (the exponent). The exponential function with base e (Euler's number), written as <code>e<sup>x</sup></code> or <code>exp(x)</code>, is especially important in mathematics, probability, and statistics. Exponential functions model processes that grow or decay at rates proportional to their current value, such as population growth, radioactive decay, and compound interest. In probability, the exponential function appears in the definitions of many distributions (like the Gaussian, Laplace, Exponential distributions), in the normalization of probabilities (e.g., in the softmax function), and in the transformation of log-likelihoods back to <a href="#likelihood-glossary">likelihoods</a>. Exponentials and <a href="#logarithm-log-natural-logarithm-ln-glossary">logarithms</a> are inverse operations: applying one after the other gets you back to your original value (<code>exp(ln(x)) = x</code> for x > 0, and <code>ln(exp(x)) = x</code>). Understanding exponentials is essential for working with probability densities, likelihoods, and the mathematics of <a href="#generative-model-glossary">generative models</a>.</p>

    <h3 class="glossary-entry-title" id="forward-diffusion-process-glossary">Forward Diffusion Process</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#diffusion-model-glossary">diffusion models</a>, the predefined, fixed process of gradually adding noise to an initial data sample (<code>x<sub>0</sub></code>) over many discrete time steps, eventually transforming it into a sample from a simple, known <a href="#probability-distribution-glossary">distribution</a> (usually Gaussian noise <code>x<sub>T</sub></code>).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The forward process systematically corrupts data <code>x<sub>0</sub></code> by adding small amounts of Gaussian noise at each step <code>t</code> according to a predefined variance schedule <code>β<sub>t</sub></code> (part of the <a href="#noise-schedule-glossary">noise schedule</a>). This process is a <a href="#markov-process-markov-chain-glossary">Markov chain</a> and is not learned by the model.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The forward diffusion process is a crucial, non-learned component of <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">Denoising Diffusion Probabilistic Models (DDPMs)</a> and other <a href="#diffusion-model-glossary">diffusion-based generative models</a>. It defines how an original data sample <code>x<sub>0</sub></code> (e.g., a clean image) is progressively transformed into noise over <code>T</code> discrete time steps.
    At each step <code>t</code> from 1 to <code>T</code>, a small amount of Gaussian noise is added to <code>x<sub>t-1</sub></code> to produce <code>x<sub>t</sub></code>. The conditional <a href="#probability-distribution-glossary">distribution</a> <code>q(x<sub>t</sub> | x<sub>t-1</sub>)</code> is defined as a Gaussian:
    <pre>q(x<sub>t</sub> | x<sub>t-1</sub>) = N(x<sub>t</sub>; √(1-β<sub>t</sub>)x<sub>t-1</sub>, β<sub>t</sub>I)</pre>
    where <code>{β<sub>t</sub>}<sup>T</sup><sub>t=1</sub></code> is a predefined variance schedule (part of the <a href="#noise-schedule-glossary">noise schedule</a>; a set of small positive constants, typically increasing or constant with <code>t</code>). This sequence of transitions forms a <a href="#markov-process-markov-chain-glossary">Markov chain</a>: <code>x<sub>0</sub> → x<sub>1</sub> → ... → x<sub>T</sub></code>.
    A key property of this process is that any noisy sample <code>x<sub>t</sub></code> can be expressed directly in terms of the original sample <code>x<sub>0</sub></code> and a single Gaussian noise term:
    <pre>q(x<sub>t</sub> | x<sub>0</sub>) = N(x<sub>t</sub>; √ᾱ<sub>t</sub>x<sub>0</sub>, (1-ᾱ<sub>t</sub>)I)</pre>
    where <code>α<sub>t</sub> = 1-β<sub>t</sub></code> and <code>ᾱ<sub>t</sub> = Π<sup>t</sup><sub>s=1</sub>α<sub>s</sub></code> (these are also part of the <a href="#noise-schedule-glossary">noise schedule</a>).
    The variance schedule <code>β<sub>t</sub></code> is designed such that <code>ᾱ<sub>T</sub></code> is close to zero, meaning <code>x<sub>T</sub></code> (the state at the final timestep) is almost entirely noise, approximately distributed as <code>N(0, I)</code>, regardless of the initial <code>x<sub>0</sub></code>.
    The forward process provides the "target" for the learned <a href="#backward-diffusion-process-glossary">backward (denoising) process</a>. The model doesn't learn this forward process; it's a given part of the model definition. Understanding its mathematical properties is essential for deriving the training objective for the reverse process.</p>

    <h3 class="glossary-entry-title" id="gaussian-mixture-model-gmm-glossary">Gaussian Mixture Model (GMM)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A type of <a href="#mixture-model-glossary">mixture model</a> where each component <a href="#probability-distribution-glossary">distribution</a> is a <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian (Normal) distribution</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>GMMs are widely used for clustering and density estimation. They model the data as a weighted sum of several Gaussian-shaped "blobs" or clusters, each with its own mean, covariance, and weight.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Gaussian Mixture Model (GMM) is a popular and versatile probabilistic model where the overall <a href="#probability-distribution-glossary">probability distribution</a> of a data point <code>x</code> is a weighted sum of <code>K</code> Gaussian components:
    <pre>P(x | {μ<sub>k</sub>, Σ<sub>k</sub>, π<sub>k</sub>}<sup>K</sup><sub>k=1</sub>) = Σ<sup>K</sup><sub>k=1</sub> π<sub>k</sub> N(x | μ<sub>k</sub>, Σ<sub>k</sub>)</pre>
    where:
    <ul>
        <li><code>K</code> is the number of Gaussian components.</li>
        <li><code>π<sub>k</sub></code> is the mixing coefficient for component <code>k</code> (<code>π<sub>k</sub> ≥ 0</code>, <code>Σ π<sub>k</sub> = 1</code>).</li>
        <li><code>N(x | μ<sub>k</sub>, Σ<sub>k</sub>)</code> is the PDF of the <code>k</code>-th <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian component</a> with mean vector <code>μ<sub>k</sub></code> and covariance matrix <code>Σ<sub>k</sub></code>.</li>
    </ul>
    GMMs can model complex, multimodal (multiple peaks) data distributions by combining simpler Gaussian shapes. They are extensively used for:
    <ul>
        <li><strong>Clustering:</strong> Each Gaussian component can represent a distinct cluster in the data. Data points can be assigned to the cluster (component) that most likely generated them (soft assignment based on posterior probabilities).</li>
        <li><strong>Density Estimation:</strong> Approximating the underlying probability density of the data.</li>
    </ul>
    The parameters of a GMM (<code>μ<sub>k</sub>, Σ<sub>k</sub>, π<sub>k</sub></code> for all <code>k</code>) are typically learned from data using the <a href="#expectation-maximization-em-algorithm-glossary">Expectation-Maximization (EM) algorithm</a>. The E-step calculates the "responsibilities" (posterior probability of each component having generated each data point), and the M-step updates the parameters to maximize the expected <a href="#log-likelihood-glossary">log-likelihood</a> given these responsibilities.
    In the context of <a href="#variational-autoencoder-vae-glossary">VAEs</a> and advanced <a href="#generative-model-glossary">generative models</a>, GMMs can serve as flexible <a href="#prior-distribution-glossary">priors</a> in the <a href="#latent-variable-glossary">latent space</a> (e.g., a GMM prior <code>P(z)</code>), enabling the model to learn more structured or multi-modal latent representations compared to a single Gaussian prior.</p>

    <h3 class="glossary-entry-title" id="generative-model-glossary">Generative Model</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A type of statistical model that learns the underlying <a href="#probability-distribution-glossary">probability distribution</a> of a dataset and can then generate new data points similar to those in the training set.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Generative models aim to understand "how data is generated." They can create new, realistic samples (like images, text, or audio) by sampling from the learned data distribution, often involving <a href="#latent-variable-glossary">latent variables</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Generative models are a class of machine learning models focused on learning the <a href="#joint-distribution-glossary">joint probability distribution</a> <code>P(X,Y)</code> of observed data <code>X</code> and (if applicable) labels <code>Y</code>, or more commonly, the distribution of the data itself <code>P(X)</code>. Often, this involves introducing <a href="#latent-variable-glossary">latent variables</a> <code>Z</code> and learning the joint distribution <code>P(X,Z)</code>.
    Unlike discriminative models, which learn to predict labels or outputs from inputs (i.e., <code>P(Y|X)</code>), generative models aim to capture the underlying process of data generation. This allows them to:
    <ul>
        <li><strong>Generate new data samples:</strong> By sampling from the learned distribution <code>P(X)</code> (or by sampling <code>z ~ P(Z)</code> and then <code>x ~ P(X|Z)</code>).</li>
        <li><strong>Perform density estimation:</strong> Estimate the probability of observing a given data point.</li>
        <li><strong>Handle missing data:</strong> Impute missing values based on the learned data distribution.</li>
        <li><strong>Feature learning:</strong> The latent variables often capture meaningful, compressed representations of the data.</li>
    </ul>
    Examples include:
    <ul>
        <li><a href="#gaussian-mixture-model-gmm-glossary">Gaussian Mixture Models (GMMs)</a></li>
        <li>Hidden Markov Models (HMMs)</li>
        <li><a href="#variational-autoencoder-vae-glossary">Variational Autoencoders (VAEs)</a></li>
        <li>Generative Adversarial Networks (GANs) - which use a <a href="#likelihood-free-objective-function-glossary">likelihood-free objective</a></li>
        <li><a href="#diffusion-model-glossary">Diffusion Models</a></li>
        <li>Autoregressive models (e.g., PixelCNN, GPT)</li>
        <li>Normalizing Flows</li>
    </ul>
    The ability to sample new data, fill in missing values, and model uncertainty makes generative models powerful tools in modern AI, with applications in image/text/audio synthesis, data augmentation, drug discovery, and scientific simulation.</p>

    <h3 class="glossary-entry-title" id="hierarchical-vae-glossary">Hierarchical VAE (Hierarchical Latent Spaces)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A <a href="#variational-autoencoder-vae-glossary">Variational Autoencoder (VAE)</a> that employs multiple layers of <a href="#latent-variable-glossary">latent variables</a> organized in a hierarchy, designed to capture complex data structures at different levels of abstraction.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Hierarchical VAEs extend the standard VAE by introducing a sequence of latent layers (e.g., <code>z<sub>1</sub>, z<sub>2</sub>, ..., z<sub>L</sub></code>). This allows the model to learn representations where higher-level latents (e.g., <code>z<sub>L</sub></code>) often capture more global, abstract features, while lower-level latents (e.g., <code>z<sub>1</sub></code>, closer to the data) capture finer details.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A hierarchical VAE (HVAE) introduces multiple stochastic <a href="#latent-variable-glossary">latent layers</a>, creating a deeper <a href="#generative-model-glossary">generative process</a> and inference model compared to a standard <a href="#variational-autoencoder-vae-glossary">VAE</a> with a single latent layer. These layers form hierarchical latent spaces.
    The generative process typically looks like:
    <pre>P(x, z<sub>1</sub>, ..., z<sub>L</sub>) = P(x|z<sub>1</sub>) [ Π<sup>L-1</sup><sub>i=1</sub> P(z<sub>i</sub>|z<sub>i+1</sub>) ] P(z<sub>L</sub>)</pre>
    Here, <code>z<sub>L</sub></code> is the top-most latent layer, usually sampled from a simple <a href="#prior-distribution-glossary">prior</a> (e.g., <code>N(0,I)</code>). Each subsequent layer <code>z<sub>i</sub></code> is generated conditioned on the layer above it <code>z<sub>i+1</sub></code>, and finally, the data <code>x</code> is generated from the bottom-most latent layer <code>z<sub>1</sub></code>.
    The inference model (encoder) also becomes hierarchical, attempting to approximate the true <a href="#posterior-distribution-glossary">posterior</a> <code>P(z<sub>1</sub>, ..., z<sub>L</sub> | x)</code>, often with a factorized approximation like:
    <pre>q(z<sub>1</sub>, ..., z<sub>L</sub> | x) = q(z<sub>1</sub>|x) [ Π<sup>L-1</sup><sub>i=1</sub> q(z<sub>i+1</sub>|z<sub>i</sub>) ]</pre>
    (Other factorization or inference schemes exist).
    Benefits of HVAEs:
    <ul>
        <li><strong>Increased Expressive Power:</strong> The hierarchical structure allows the model to learn more complex and structured <a href="#probability-distribution-glossary">probability distributions</a> over data. Different layers can capture features at different scales or levels of abstraction.</li>
        <li><strong>Improved Representations:</strong> Can lead to more disentangled or semantically meaningful latent representations, especially in the higher layers.</li>
        <li><strong>Better Generative Quality:</strong> Often results in higher quality generated samples.</li>
    </ul>
    Challenges:
    <ul>
        <li><strong>Training Complexity:</strong> Training can be more difficult due to the increased number of parameters and stochastic layers (vanishing gradients, harder optimization of the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>).</li>
        <li><strong>Inference Complexity:</strong> Designing effective inference networks for multiple latent layers is more involved.</li>
    </ul>
    The <a href="#iterative-inference-generation-glossary">iterative refinement process</a> in <a href="#diffusion-model-glossary">Diffusion Models</a> can be seen as analogous to a very deep hierarchical latent variable model where each denoising step corresponds to inferring a slightly cleaner version of the data (a latent state) based on the previous, noisier state. The sequence of noisy images <code>x<sub>T</sub>, ..., x<sub>1</sub></code> in a diffusion model can be thought of as a specific kind of hierarchical latent space.</p>

    <h3 class="glossary-entry-title" id="integration-glossary">Integration</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A mathematical operation that finds the total accumulation (such as area under a curve), often used to compute probabilities or <a href="#expectation-e-glossary">expectations</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>In probability, integration is used to sum up (for continuous variables) all possible values weighted by their probabilities.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Integration is a core concept in calculus and probability theory. For continuous <a href="#random-variable-glossary">random variables</a>, probabilities and <a href="#expectation-e-glossary">expectations</a> are computed using integrals. For example, to find the probability that a variable X falls within a certain range, you integrate its <a href="#probability-distribution-glossary">probability density function</a> over that range. Similarly, the expectation (mean) of a function of X is computed by integrating the product of the function and the probability density over all possible values. In <a href="#generative-model-glossary">generative modeling</a>, integration appears in many places: when calculating <a href="#marginal-distribution-glossary">marginal probabilities</a>, when defining the <a href="#likelihood-glossary">likelihood</a> of observed data (see <a href="#marginal-likelihood-glossary">Marginal Likelihood</a>), and when working with expectations in <a href="#variational-inference-vi-glossary">variational inference</a>. However, integrals can be difficult or impossible to compute exactly in complex models (high-dimensional integrals), which is why approximate methods like Monte Carlo sampling and <a href="#variational-inference-vi-glossary">variational inference</a> are so important in practice.</p>

    <h3 class="glossary-entry-title" id="iterative-inference-generation-glossary">Iterative Inference/Generation (Context of Diffusion)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A characteristic of certain <a href="#generative-model-glossary">generative models</a>, particularly <a href="#diffusion-model-glossary">diffusion models</a>, where the generation of a data sample (or inference of latent states) occurs through a sequence of multiple, gradual refinement steps.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Instead of a single-pass transformation from noise/input to output, these models employ an iterative process. In <a href="#diffusion-model-glossary">diffusion models</a>, the <a href="#backward-diffusion-process-glossary">backward process</a> generates data by progressively denoising a sample over many timesteps. This contrasts with some VAEs or GANs that might generate an output in one forward pass of a neural network.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Iterative inference and generation refer to processes that proceed in a sequence of steps, where each step refines the output based on the previous step's result. This is a defining feature of <a href="#diffusion-model-glossary">diffusion models</a>:
    <ul>
        <li><strong>Forward Process:</strong> Data is iteratively noised over <code>T</code> steps. This is an iterative process by definition (<a href="#forward-diffusion-process-glossary">Forward Diffusion Process</a>).</li>
        <li><strong>Backward (Generative) Process:</strong> A sample starts as pure noise <code>x<sub>T</sub></code> and is iteratively denoised over <code>T</code> steps by the learned model <code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> to produce <code>x<sub>0</sub></code> (<a href="#backward-diffusion-process-glossary">Backward Diffusion Process</a>). Each step makes a small change to the sample.</li>
    </ul>
    This iterative refinement has parallels with:
    <ul>
        <li><strong><a href="#hierarchical-vae-glossary">Hierarchical VAEs</a>:</strong> Where information is processed through multiple layers of latent variables, with each layer potentially refining the representation. The sequence of states <code>x<sub>T</sub>, ..., x<sub>0</sub></code> in a diffusion model can be viewed as a very deep hierarchy of latent variables.</li>
        <li><strong>Iterative Optimization Algorithms:</strong> Like gradient descent (<a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>) or the <a href="#expectation-maximization-em-algorithm-glossary">EM algorithm</a>, which iteratively update parameters or estimates.</li>
        <li><strong>MCMC methods:</strong> Which iteratively explore a state space to converge to a target distribution.</li>
    </ul>
    The advantages of iterative generation in diffusion models include the ability to model very complex distributions by breaking down the generation into many simple steps, often leading to very high-quality samples. The disadvantage can be slower sampling speed due to the need for many sequential steps. The iterative nature is also fundamental to the mathematical formulation of their <a href="#diffusion-model-objective-function-glossary">objective functions</a>.</p>

    <h3 class="glossary-entry-title" id="isotropic-gaussian-glossary">Isotropic Gaussian</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A <a href="#multivariate-gaussian-normal-distribution-glossary">multivariate Gaussian distribution</a> where all variables have the same variance and are uncorrelated.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>An isotropic Gaussian has a covariance matrix that is a scalar multiple of the identity matrix (<strong>Σ</strong> = σ²<strong>I</strong>). Its probability density contours are hyperspheres, making it simple and a common choice for a <a href="#prior-distribution-glossary">prior</a> in <a href="#variational-autoencoder-vae-glossary">VAEs</a> (e.g., <code>N(0, I)</code>).</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The term “isotropic” means “uniform in all directions.” In the context of <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussians</a>, an isotropic Gaussian is one whose covariance matrix <strong>Σ</strong> is of the form σ²<strong>I</strong>, where σ² is a scalar variance and <strong>I</strong> is the identity matrix. This implies:
    <ul>
        <li>All variables have the same variance σ².</li>
        <li>All variables are uncorrelated (the off-diagonal elements of <strong>Σ</strong> are zero).</li>
    </ul>
    This results in a probability distribution whose level sets (contours of equal probability density) are circles in 2D, spheres in 3D, and hyperspheres in higher dimensions. The distribution is perfectly symmetrical around its mean.
    Isotropic Gaussians are frequently used as <a href="#prior-distribution-glossary">prior distributions</a> <code>P(z)</code> over <a href="#latent-variable-glossary">latent variables</a> <code>z</code> in <a href="#variational-autoencoder-vae-glossary">VAEs</a>, most commonly the standard normal distribution <code>N(0, I)</code> (which is isotropic with μ=0 and σ²=1). This choice offers several advantages:
    <ul>
        <li><strong>Simplicity:</strong> It's a simple distribution to define and sample from.</li>
        <li><strong>Mathematical Tractability:</strong> The <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a> term in the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> between another Gaussian (the approximate posterior <code>q(z|x)</code>) and an isotropic Gaussian prior often has a closed-form analytical solution, simplifying calculations.</li>
        <li><strong>Regularization:</strong> It encourages the learned latent space to be somewhat uniform and centered, which can help with disentanglement of latent factors and improve generalization.</li>
    </ul>
    While real-world data projected into a latent space might not naturally be isotropic, using an isotropic prior provides a useful regularizing effect and makes sampling for generation straightforward.</p>

    <h3 class="glossary-entry-title" id="jensens-inequality-glossary">Jensen’s Inequality</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A mathematical inequality relating convex (or concave) functions and <a href="#expectation-e-glossary">expectations</a>, foundational in <a href="#variational-inference-vi-glossary">variational inference</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>For a convex function <code>f</code>, <code>f(E[X]) ≤ E[f(X)]</code>. For a concave function <code>f</code> (like <code>log</code>), <code>f(E[X]) ≥ E[f(X)]</code>. This is key to deriving variational bounds like the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Jensen’s Inequality is a powerful result in mathematics that relates the value of a convex (or concave) function of an <a href="#expectation-e-glossary">expectation</a> to the expectation of the function's values.
    <ul>
        <li><strong>For a convex function <code>f</code>:</strong> <code>f(E[X]) ≤ E[f(X)]</code>. Geometrically, for a convex function, the line segment connecting any two points on its graph lies above or on the graph. The "function of the average" is less than or equal to the "average of the function."</li>
        <li><strong>For a concave function <code>f</code>:</strong> <code>f(E[X]) ≥ E[f(X)]</code>. The inequality is reversed. The <a href="#logarithm-log-natural-logarithm-ln-glossary">logarithm function</a> is a key example of a concave function.</li>
    </ul>
    In <a href="#variational-inference-vi-glossary">variational inference</a> and <a href="#variational-autoencoder-vae-glossary">VAEs</a>, Jensen’s Inequality (specifically for the concave log function) is crucial for deriving the <a href="#evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</a>. Starting with the <a href="#marginal-likelihood-glossary">log marginal likelihood</a> <code>log P(x) = log ∫ P(x,z) dz</code>, we introduce an auxiliary distribution <code>q(z|x)</code> and rewrite it as <code>log P(x) = log E<sub>q(z|x)</sub>[P(x,z)/q(z|x)]</code>. Applying Jensen's inequality for the concave log function (<code>log(E[Y]) ≥ E[log(Y)]</code> where <code>Y = P(x,z)/q(z|x)</code>) yields:
    <pre>log P(x) ≥ E<sub>q(z|x)</sub>[log (P(x,z)/q(z|x))]</pre>
    The right-hand side is the ELBO. This inequality shows that the ELBO is indeed a lower bound on the log marginal likelihood. Maximizing the ELBO thus pushes up this lower bound, indirectly optimizing the true objective.</p>

    <h3 class="glossary-entry-title" id="joint-distribution-glossary">Joint Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#probability-distribution-glossary">probability distribution</a> describing two or more <a href="#random-variable-glossary">random variables</a> together.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Joint distributions capture the <a href="#likelihood-glossary">likelihood</a> of combinations of outcomes for multiple variables at once.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A joint distribution gives us a complete picture of how two or more <a href="#random-variable-glossary">random variables</a> behave together. For example, if you’re interested in the relationship between students’ heights and weights, the joint distribution describes the probability of seeing any particular combination of height and weight in your data. In mathematical terms, for two variables X and Y, the joint distribution is written as <code>P(X, Y)</code> for discrete variables or as a joint probability density function for continuous variables. Joint distributions are essential for understanding dependencies and correlations between variables. In <a href="#generative-model-glossary">generative models</a>, the joint distribution often involves both observed data (like images or text) and <a href="#latent-variable-glossary">latent variables</a> (hidden factors that explain the data, e.g., <code>P(x,z)</code>). Many inference and learning tasks require working with joint distributions, such as marginalizing (summing or <a href="#integration-glossary">integrating</a> out) some variables to focus on others (to get a <a href="#marginal-distribution-glossary">marginal distribution</a>), or conditioning on observed values to make predictions (to get a <a href="#conditional-distribution-glossary">conditional distribution</a>).</p>

    <h3 class="glossary-entry-title" id="joint-posterior-distribution-glossary">Joint Posterior Distribution (in Multimodal Learning)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#multimodal-learning-glossary">multimodal learning</a> scenarios, this refers to the <a href="#posterior-distribution-glossary">posterior probability distribution</a> over a shared <a href="#latent-variable-glossary">latent variable</a> <code>z</code>, conditioned on observations from <em>all</em> available modalities (e.g., <code>P(z | x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>M</sub>)</code> for M modalities).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The joint posterior aims to synthesize information from all observed data types (modalities) to provide a unified and comprehensive inference about the shared underlying latent factors <code>z</code> that are assumed to generate data across these modalities.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In <a href="#multimodal-learning-glossary">multimodal learning</a>, a common goal is to learn a shared <a href="#latent-variable-glossary">latent representation</a> <code>z</code> that captures the common underlying factors explaining data from multiple modalities (e.g., <code>x<sub>image</sub></code>, <code>x<sub>text</sub></code>, <code>x<sub>audio</sub></code>). The joint posterior distribution, <code>P(z | x<sub>image</sub>, x<sub>text</sub>, x<sub>audio</sub>, ...)</code>, represents our belief about this shared latent variable <code>z</code> after observing data from all available modalities.
    This joint posterior is central because:
    <ul>
        <li>It embodies the <strong>fusion of information</strong> from all modalities. A well-formed joint posterior should be more informed (e.g., have lower variance or uncertainty) than any unimodal posterior (e.g., <code>P(z | x<sub>image</sub>)</code> alone).</li>
        <li>It serves as the basis for <strong>coherent generation and inference</strong>. If we can sample from (an approximation of) the joint posterior, we can then decode <code>z</code> into any desired modality, enabling <a href="#cross-modal-generation-glossary">cross-modal generation</a> or reconstruction.</li>
    </ul>
    However, directly parameterizing an inference network (encoder) for the true joint posterior that can gracefully handle any subset of available modalities at input is challenging. For <code>M</code> modalities, there are <code>2<sup>M</sup>-1</code> possible subsets of observations.
    Therefore, approximate methods are often used to construct a <a href="#consensus-distribution-glossary"><strong>consensus distribution</strong></a> that approximates the joint posterior. Common strategies include:
    <ul>
        <li><a href="#product-of-experts-poe-glossary"><strong>Product of Experts (PoE)</strong></a>: Combines unimodal posteriors (e.g., <code>q(z|x<sub>image</sub>)</code>, <code>q(z|x<sub>text</sub>)</code>) by multiplying them. Assumes conditional independence of modalities given <code>z</code>.</li>
        <li><a href="#mixture-of-experts-moe-glossary"><strong>Mixture of Experts (MoE)</strong></a>: Combines unimodal posteriors by taking a weighted average.</li>
        <li>More sophisticated methods like <a href="#consensus-of-dependent-experts-code-glossary">Consensus of Dependent Experts (CoDE)</a> that try to model dependencies between modalities.</li>
    </ul>
    The quality of the approximation to the joint posterior significantly impacts the performance of multimodal <a href="#generative-model-glossary">generative models</a>, influencing their ability to fuse information effectively and generate consistent multimodal outputs.</p>

    <h3 class="glossary-entry-title" id="kl-divergence-kullback-leibler-divergence-glossary">KL Divergence (Kullback-Leibler Divergence)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A non-symmetric measure of how one <a href="#probability-distribution-glossary">probability distribution</a> (P) differs from a second, reference <a href="#probability-distribution-glossary">probability distribution</a> (Q).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>KL divergence, often written as <code>D<sub>KL</sub>(P || Q)</code> or <code>KL(P || Q)</code>, quantifies the "information lost" or "inefficiency" when using distribution Q as an approximation for distribution P. It is always non-negative, and <code>D<sub>KL</sub>(P || Q) = 0</code> if and only if P and Q are identical. It's a cornerstone in information theory and <a href="#variational-inference-vi-glossary">variational inference</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Kullback-Leibler (KL) divergence is a fundamental concept used to measure the dissimilarity between two <a href="#probability-distribution-glossary">probability distributions</a>, P and Q, defined over the same space.
    <ul>
        <li>For <strong>discrete distributions</strong> P and Q over a variable X:
        <pre>D<sub>KL</sub>(P || Q) = Σ<sub>x</sub> P(x) log (P(x) / Q(x)) = Σ<sub>x</sub> P(x) [log P(x) - log Q(x)]</pre></li>
        <li>For <strong>continuous distributions</strong> p and q with densities over a variable x:
        <pre>D<sub>KL</sub>(p || q) = ∫ p(x) log (p(x) / q(x)) dx = ∫ p(x) [log p(x) - log q(x)] dx</pre></li>
    </ul>
    Key properties of KL divergence:
    <ul>
        <li><strong>Non-negativity:</strong> <code>D<sub>KL</sub>(P || Q) ≥ 0</code>.</li>
        <li><strong>Identity of Indiscernibles:</strong> <code>D<sub>KL</sub>(P || Q) = 0</code> if and only if P = Q (almost everywhere).</li>
        <li><strong>Asymmetry:</strong> In general, <code>D<sub>KL</sub>(P || Q) ≠ D<sub>KL</sub>(Q || P)</code>. Thus, it's not a true distance metric. The choice of which distribution is P (the "true" or target) and which is Q (the approximation) matters for its behavior.
            <ul>
                <li>Minimizing <code>D<sub>KL</sub>(P || Q)</code> with respect to Q (forward KL) tends to find a Q that covers all modes of P, potentially being more diffuse.</li>
                <li>Minimizing <code>D<sub>KL</sub>(Q || P)</code> with respect to Q (reverse KL, used in VI) tends to find a Q that focuses on one mode of P if P is multimodal, often being less diffuse.</li>
            </ul>
        </li>
    </ul>
    In the context of <a href="#variational-autoencoder-vae-glossary">VAEs</a> and <a href="#variational-inference-vi-glossary">variational inference</a>:
    <ol>
        <li>The term <code>D<sub>KL</sub>(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z))</code> in the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> measures the divergence of the approximate <a href="#posterior-distribution-glossary">posterior</a> <code>q<sub>φ</sub>(z|x)</code> from the <a href="#prior-distribution-glossary">prior</a> <code>P<sub>θ</sub>(z)</code>. Minimizing this (by maximizing ELBO) acts as a regularizer.</li>
        <li>The entire VI optimization process is equivalent to minimizing <code>D<sub>KL</sub>(q<sub>φ</sub>(z|x) || P<sub>θ</sub>(z|x))</code>, the divergence of the approximate posterior from the true (intractable) posterior.</li>
    </ol>
    Understanding KL divergence is crucial for interpreting the <a href="#objective-function-glossary">objective functions</a> and behavior of many probabilistic <a href="#generative-model-glossary">generative models</a>. It's typically measured in <a href="#nat-glossary">nats</a> if natural logarithm is used, or bits if log base 2 is used.</p>

    <h3 class="glossary-entry-title" id="latent-variable-glossary">Latent Variable</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A hidden variable that is not directly observed in data but is inferred by a model to explain patterns or structure in the observed variables.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Latent variables (often denoted <code>z</code>) aim to capture underlying, unobserved factors that generate or influence the data we see. Examples include the "topic" of a document, the "style" of an image, or underlying "clusters" in a dataset.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Latent variables are variables that are not directly measured or observed in the data but are postulated by a statistical model to explain the observed patterns, dependencies, or variations. They are a key feature of many probabilistic <a href="#generative-model-glossary">generative models</a>, including <a href="#variational-autoencoder-vae-glossary">VAEs</a>, <a href="#gaussian-mixture-model-gmm-glossary">mixture models</a>, factor analysis, and Hidden Markov Models.
    The idea is that the observed data <code>x</code> is generated from these simpler, often lower-dimensional, latent variables <code>z</code> through a mapping or generative process <code>p(x|z)</code>.
    For example:
    <ul>
        <li>In a <a href="#mixture-model-glossary">mixture of Gaussians</a>, the latent variable indicates which Gaussian component a data point belongs to.</li>
        <li>In <a href="#variational-autoencoder-vae-glossary">VAEs</a>, latent variables represent compressed, abstract features (e.g., semantic attributes of an image like "smiling" or "wearing glasses") that capture the essential characteristics of the data. These form <a href="#hierarchical-vae-glossary">hierarchical latent spaces</a> in more complex VAEs.</li>
        <li>In topic models, latent variables represent the underlying topics present in a collection of documents.</li>
    </ul>
    These variables allow generative models to learn complex, high-dimensional data distributions by modeling simpler distributions in a lower-dimensional latent space. Inference about latent variables—determining their likely values or distribution given the observed data (i.e., finding the <a href="#posterior-distribution-glossary">posterior distribution</a> <code>p(z|x)</code>)—is a central task and often requires approximate methods like <a href="#variational-inference-vi-glossary">variational inference</a> or MCMC due to computational challenges.</p>

    <h3 class="glossary-entry-title" id="likelihood-glossary">Likelihood</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The probability (or probability density) of observed data given specific model parameters.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Likelihood, <code>L(θ|x) = P(x|θ)</code>, tells us how well a set of parameters <code>θ</code> explains the observed data <code>x</code>. It is central to statistical inference, particularly Maximum Likelihood Estimation.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In statistics and machine learning, the likelihood function is a fundamental concept for model fitting and parameter estimation. For a given model and a set of parameters <code>θ</code>, the likelihood <code>L(θ|x)</code> quantifies how probable the observed data <code>x</code> is under those parameters. It is numerically equal to the <a href="#probability-distribution-glossary">probability (or density)</a> of the data given the parameters, <code>P(x|θ)</code>, but viewed as a function of <code>θ</code> with <code>x</code> fixed. For example, in a Gaussian model, the likelihood of observing a data point is given by the Gaussian PDF evaluated at that point, with the current mean and variance as parameters. In practice, we often want to find the parameter values that maximize the likelihood of the observed data—this is called Maximum Likelihood Estimation (MLE). In <a href="#generative-model-glossary">generative models</a>, the likelihood (or more commonly, the <a href="#log-likelihood-glossary">log-likelihood</a>) is used to measure how well the model can explain or generate the data. In <a href="#variational-autoencoder-vae-glossary">VAEs</a>, the reconstruction term in the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>, <code>E<sub>q(z|x)</sub>[log P(x|z,θ)]</code>, is an expected conditional likelihood of the data given latent variables <code>z</code> and model parameters <code>θ</code>.</p>

    <h3 class="glossary-entry-title" id="likelihood-free-objective-function-glossary">Likelihood-Free Objective Function</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An <a href="#objective-function-glossary">objective function</a> used to train <a href="#generative-model-glossary">generative models</a> without requiring explicit computation or knowledge of the model's <a href="#likelihood-glossary">likelihood function</a> <code>P(x|θ)</code>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Some generative models define the data generation process implicitly (e.g., through a complex simulator or a neural network that transforms noise into data) where the likelihood <code>P(x|θ)</code> is unknown or too computationally expensive to evaluate. These methods optimize alternative objectives that assess the similarity between generated and real data distributions without direct likelihood evaluation.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Likelihood-free inference (LFI) approaches, also known as implicit inference, are essential when the <a href="#likelihood-glossary">likelihood function</a> <code>P(x|θ)</code> of observing data <code>x</code> given model parameters <code>θ</code> is intractable or cannot be expressed analytically. This is common in models where the generative process is a complex simulation or a deep neural network transformation.
    Instead of maximizing likelihood, these methods rely on objectives that typically involve:
    <ul>
        <li><strong>Comparing Distributions:</strong> Measuring a divergence or distance between the distribution of data generated by the model and the distribution of real data.
            <ul>
                <li><strong>Generative Adversarial Networks (GANs):</strong> A prime example. GANs use a minimax game between a generator (that produces samples) and a discriminator (that tries to distinguish real from generated samples). The discriminator's loss implicitly defines a divergence (e.g., Jensen-Shannon divergence in original GANs, Wasserstein distance in WGANs).</li>
                <li><strong>Minimum Divergence Estimation:</strong> Directly minimizing statistical distances like Maximum Mean Discrepancy (MMD) or Wasserstein distance between empirical distributions of real and generated samples.</li>
            </ul>
        </li>
        <li><strong>Ratio Estimation:</strong> Some methods try to estimate the ratio of likelihoods, which can be easier than estimating individual likelihoods.</li>
        <li><strong>Approximate Bayesian Computation (ABC):</strong> For parameter inference, ABC methods simulate data from the model with given parameters, compare summary statistics of simulated data with those of observed data, and accept parameters that produce simulations "close enough" to the observed data.</li>
    </ul>
    These likelihood-free objectives allow training of very flexible and powerful generative models capable of producing highly realistic samples, even if their underlying probability densities are unknown. This contrasts with models like <a href="#variational-autoencoder-vae-glossary">VAEs</a> which optimize a bound on the (log) likelihood, even if the <a href="#marginal-likelihood-glossary">marginal likelihood</a> itself is intractable.</p>

    <h3 class="glossary-entry-title" id="log-density-of-a-gaussian-glossary">Log Density of a Gaussian</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#logarithm-log-natural-logarithm-ln-glossary">natural logarithm</a> of the probability density function (PDF) of a <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian (Normal) distribution</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Working with log densities is common in probabilistic modeling for numerical stability (avoiding underflow/overflow from multiplying many small probabilities) and mathematical convenience (products become sums, simplifying differentiation). This is frequently used in <a href="#likelihood-glossary">likelihood</a> calculations and the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The log density of a Gaussian distribution is a key component in many probabilistic models.
    For a <strong>univariate Gaussian</strong> with mean μ and variance σ², the PDF is:
    <pre>f(x | μ, σ²) = (1 / √(2πσ²)) * exp(-(x-μ)² / (2σ²))</pre>
    Its log density (using natural logarithm, ln) is:
    <pre>log f(x | μ, σ²) = log( (1 / √(2πσ²)) * exp(-(x-μ)² / (2σ²)) )</pre>
    <pre>                = log(1 / √(2πσ²)) + log(exp(-(x-μ)² / (2σ²)))</pre>
    <pre>                = -log(√(2πσ²)) - (x-μ)² / (2σ²)</pre>
    <pre>                = -1/2 log(2πσ²) - (x-μ)² / (2σ²)</pre>
    <pre>                = -1/2 log(2π) - 1/2 log(σ²) - (x-μ)² / (2σ²)</pre>

    For a <strong>D-dimensional <a href="#multivariate-gaussian-normal-distribution-glossary">multivariate Gaussian</a></strong> with mean vector <strong>μ</strong> and covariance matrix <strong>Σ</strong>, the PDF is:
    <pre>f(<strong>x</strong> | <strong>μ</strong>, <strong>Σ</strong>) = (1 / ((2π)<sup>D/2</sup> |<strong>Σ</strong>|<sup>1/2</sup>)) * exp(-1/2 (<strong>x</strong>-<strong>μ</strong>)<sup>T</sup><strong>Σ</strong><sup>-1</sup>(<strong>x</strong>-<strong>μ</strong>))</pre>
    Its log density is:
    <pre>log f(<strong>x</strong> | <strong>μ</strong>, <strong>Σ</strong>) = -D/2 log(2π) - 1/2 log|<strong>Σ</strong>| - 1/2 (<strong>x</strong>-<strong>μ</strong>)<sup>T</sup><strong>Σ</strong><sup>-1</sup>(<strong>x</strong>-<strong>μ</strong>)</pre>
    where <code>|<strong>Σ</strong>|</code> is the determinant of the covariance matrix and <code><strong>Σ</strong><sup>-1</sup></code> is its inverse (the <a href="#precision-in-gaussians-glossary">precision matrix</a>). The term <code>(<strong>x</strong>-<strong>μ</strong>)<sup>T</sup><strong>Σ</strong><sup>-1</sup>(<strong>x</strong>-<strong>μ</strong>)</code> is known as the Mahalanobis distance squared.

    Using log densities is vital because:
    <ul>
        <li><strong>Numerical Stability:</strong> Products of many small probabilities (common in <a href="#likelihood-glossary">likelihoods</a> of i.i.d. data) can underflow. Sums of log probabilities are more stable.</li>
        <li><strong>Mathematical Simplicity:</strong> Products become sums (<code>log(ab) = log a + log b</code>), and exponents become multipliers (<code>log(a<sup>b</sup>) = b log a</code>). This simplifies differentiation needed for optimization.</li>
    </ul>
    In <a href="#variational-autoencoder-vae-glossary">VAEs</a>, Gaussian distributions are often used for the <a href="#prior-distribution-glossary">prior</a> <code>p(z)</code>, the approximate <a href="#posterior-distribution-glossary">posterior</a> <code>q(z|x)</code>, and/or the data <a href="#likelihood-glossary">likelihood</a> <code>p(x|z)</code> (if data is continuous). Their log densities are therefore essential components of the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> calculation.</p>

    <h3 class="glossary-entry-title" id="log-likelihood-glossary">Log Likelihood</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#logarithm-log-natural-logarithm-ln-glossary">natural logarithm</a> of the <a href="#likelihood-glossary">likelihood function</a>, used for mathematical convenience and numerical stability.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Log likelihood, <code>log L(θ|x)</code>, simplifies the multiplication of probabilities (common in likelihoods of independent data) into addition, making calculations more tractable and stable, especially for optimization.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>When working with datasets where data points are assumed to be independent, the total <a href="#likelihood-glossary">likelihood</a> of observing all the data is the product of the individual likelihoods for each data point: <code>L(θ|x<sub>1</sub>,...,x<sub>N</sub>) = Π<sub>i</sub> P(x<sub>i</sub>|θ)</code>. This product can quickly become very small (leading to numerical underflow) or very large, making it difficult to handle mathematically. By taking the <a href="#logarithm-log-natural-logarithm-ln-glossary">natural logarithm</a> of the likelihood, we turn this product into a sum: <code>log L(θ|x<sub>1</sub>,...,x<sub>N</sub>) = ∑<sub>i</sub> log P(x<sub>i</sub>|θ)</code>. This sum is much easier to work with, especially when optimizing model parameters using calculus (derivatives of sums are sums of derivatives). Since the logarithm is a monotonically increasing function, maximizing the log likelihood is equivalent to maximizing the likelihood itself. In practice, almost all modern statistical and machine learning models optimize the log likelihood rather than the likelihood. In <a href="#variational-autoencoder-vae-glossary">VAEs</a> and other <a href="#generative-model-glossary">generative models</a>, the log likelihood (or a bound on it like the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>) is a key component of the <a href="#objective-function-glossary">objective function</a>, and maximizing it means making the model better at explaining or generating the observed data.</p>

    <h3 class="glossary-entry-title" id="log-likelihood-in-vaes-glossary">Log Likelihood in VAEs</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In the context of <a href="#variational-autoencoder-vae-glossary">VAEs</a>, this typically refers to the <a href="#log-likelihood-glossary">log</a> <a href="#marginal-likelihood-glossary">marginal likelihood</a> of the observed data under the VAE's <a href="#generative-model-glossary">generative model</a>, <code>log p<sub>θ</sub>(x)</code>. This quantity is usually intractable to compute directly and is lower-bounded by the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>For a VAE, <code>log p<sub>θ</sub>(x)</code> represents how well the decoder <code>p<sub>θ</sub>(x|z)</code> and prior <code>p(z)</code> can, on average over all <code>z</code>, explain the observed data <code>x</code>. During training, the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> is maximized as a proxy for maximizing this true (but intractable) log likelihood. The ELBO is also often used as an evaluation metric, although tighter bounds or alternative evaluation methods exist.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In a <a href="#variational-autoencoder-vae-glossary">Variational Autoencoder (VAE)</a>, the ultimate <a href="#objective-function-glossary">objective</a> we'd ideally like to maximize is the <a href="#log-likelihood-glossary">log</a> <a href="#marginal-likelihood-glossary">marginal likelihood</a> of the observed data <code>x</code> under the <a href="#generative-model-glossary">generative model</a> parameterized by <code>θ</code> (decoder parameters). This is written as:
    <pre>log p<sub>θ</sub>(x) = log ∫ p<sub>θ</sub>(x,z) dz = log ∫ p<sub>θ</sub>(x|z)p(z) dz</pre>
    This integral is over all possible configurations of the <a href="#latent-variable-glossary">latent variables</a> <code>z</code>. It measures how well the model (consisting of the decoder <code>p<sub>θ</sub>(x|z)</code> and the <a href="#prior-distribution-glossary">prior</a> <code>p(z)</code>) can explain the observed data <code>x</code>.
    However, this integral is generally intractable for complex decoders (like neural networks) and high-dimensional latent spaces.
    This is where <a href="#variational-inference-vi-glossary">variational inference</a> and the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> come in. The ELBO provides a tractable lower bound to this intractable log likelihood:
    <pre>ELBO(φ, θ) ≤ log p<sub>θ</sub>(x)</pre>
    The gap between the ELBO and the true log marginal likelihood is precisely the <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a> between the approximate <a href="#posterior-distribution-glossary">posterior</a> <code>q<sub>φ</sub>(z|x)</code> (encoder) and the true posterior <code>p<sub>θ</sub>(z|x)</code>:
    <pre>log p<sub>θ</sub>(x) - ELBO(φ, θ) = D<sub>KL</sub>(q<sub>φ</sub>(z|x) || p<sub>θ</sub>(z|x))</pre>
    So, by maximizing the ELBO, we are:
    <ol>
        <li>Pushing up a lower bound on <code>log p<sub>θ</sub>(x)</code>, thus making the model better at explaining the data.</li>
        <li>Minimizing the KL divergence between our approximate posterior and the true posterior, making our inference (encoder) more accurate.</li>
    </ol>
    While the ELBO is used for training, evaluating the true <code>log p<sub>θ</sub>(x)</code> for a trained VAE remains challenging. Methods like Importance Weighted Autoencoders (IWAE) can provide tighter bounds on <code>log p<sub>θ</sub>(x)</code> and are often used for more accurate evaluation of the model's generative performance in terms of likelihood. The "reconstruction log likelihood" term within the ELBO, <code>E<sub>q<sub>φ</sub>(z|x)</sub>[log p<sub>θ</sub>(x|z)]</code>, is also an important component, measuring how well data is reconstructed given an encoding.</p>

    <h3 class="glossary-entry-title" id="logarithm-log-natural-logarithm-ln-glossary">Logarithm (log) & Natural Logarithm (ln)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The inverse of <a href="#exponent-exponential-function-glossary">exponentiation</a>; the natural log uses base e (≈2.718).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Logarithms turn multiplication into addition, making complex probability calculations easier; natural logs are especially common in statistics and machine learning.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Logarithms are mathematical functions that answer the question: “To what power must the base be raised to get a certain number?” The natural logarithm (ln) uses the special base e (Euler's number), which is about 2.718. In probability and statistics, logs are used to simplify products of probabilities into sums, which are easier to handle mathematically and numerically. For example, the <a href="#likelihood-glossary">likelihood</a> of observing a dataset is often the product of many probabilities, but the <a href="#log-likelihood-glossary">log-likelihood</a> is a sum, which is easier to optimize and less prone to numerical errors (underflow/overflow). Natural logs are particularly common because many <a href="#probability-distribution-glossary">probability distributions</a> and statistical formulas (like those involving entropy or the <a href="#exponent-exponential-function-glossary">exponential function</a>) are most naturally expressed using base e. When information or entropy is measured using natural logs, the units are called “<a href="#nat-glossary">nats</a>.” Understanding logarithms is crucial for interpreting likelihoods, entropy, and divergence measures (like <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a>) in probabilistic models.</p>

    <h3 class="glossary-entry-title" id="marginal-distribution-glossary">Marginal Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#probability-distribution-glossary">distribution</a> of one variable in a dataset, ignoring all others.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Marginal distributions show how often each value of a variable occurs, regardless of the values of other variables—like looking at the row or column totals in a table.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In probability theory, the marginal distribution of a subset of variables is obtained by summing (for discrete variables) or <a href="#integration-glossary">integrating</a> (for continuous variables) over the other variables in the <a href="#joint-distribution-glossary">joint distribution</a>. For example, if you have a joint distribution over two variables, X and Y, <code>P(X,Y)</code>, the marginal distribution of X is found by summing or integrating out Y: <code>P(X) = Σ<sub>y</sub> P(X,Y=y)</code> (for discrete Y) or <code>P(X) = ∫ P(X,y) dy</code> (for continuous Y). Marginal distributions are important because they allow us to focus on a single variable of interest, even when we know the full joint distribution over several variables. In <a href="#generative-model-glossary">generative models</a>, computing marginal distributions is often necessary when making predictions or evaluating the <a href="#likelihood-glossary">likelihood</a> of observed data (see <a href="#marginal-likelihood-glossary">Marginal Likelihood</a>). However, in complex models with many <a href="#latent-variable-glossary">latent variables</a>, marginalization can become computationally challenging, which is one reason why approximate inference methods like <a href="#variational-inference-vi-glossary">variational inference</a> are so important.</p>

    <h3 class="glossary-entry-title" id="marginal-likelihood-glossary">Marginal Likelihood (Model Evidence)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The <a href="#probability-distribution-glossary">probability</a> of the observed data under a specific model, after all <a href="#latent-variable-glossary">latent variables</a> and/or parameters have been integrated (or summed) out.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Marginal likelihood, often denoted <code>P(x|M)</code> or just <code>P(x)</code>, quantifies how well a model <code>M</code> explains the data <code>x</code> on average, considering all possible configurations of its unobserved components. It's a key quantity for <a href="#bayesian-inference-glossary">Bayesian model comparison</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In <a href="#bayesian-inference-glossary">Bayesian statistics</a> and <a href="#generative-model-glossary">generative modeling</a>, the marginal likelihood (also known as model evidence or integrated likelihood) is the probability of the observed data <code>x</code> given a model <code>M</code>. If the model involves <a href="#latent-variable-glossary">latent variables</a> <code>z</code> (and parameters <code>θ</code> which might also be integrated out if we are fully Bayesian), it is computed by:
    <pre>P(x|M) = ∫ P(x,z|M) dz = ∫ P(x|z,M)P(z|M) dz</pre>
    If we are considering parameters <code>θ</code> as well:
    <pre>P(x|M) = ∫ P(x|θ,M)P(θ|M) dθ</pre>
    The marginal likelihood is crucial for:
    <ul>
        <li><strong>Bayesian Model Selection/Comparison:</strong> It allows comparison of different models. Models that assign higher marginal likelihood to the observed data are considered better. It naturally incorporates Occam's Razor: simpler models that explain the data well are favored over overly complex models, as complex models spread their probability mass over a wider range of possible datasets.</li>
        <li><strong>Denominator in Bayes' Theorem:</strong> It serves as the normalization constant in Bayes' theorem for calculating the <a href="#posterior-distribution-glossary">posterior distribution</a> of parameters: <code>P(θ|x,M) = [P(x|θ,M)P(θ|M)] / P(x|M)</code>.</li>
    </ul>
    However, in most interesting and complex models (such as <a href="#variational-autoencoder-vae-glossary">VAEs</a>, deep <a href="#generative-model-glossary">generative models</a>, and <a href="#diffusion-model-glossary">diffusion models</a>), the marginal likelihood is intractable to compute exactly due to the high-dimensional <a href="#integration-glossary">integral</a> over latent variables or parameters. This intractability motivates the use of:
    <ul>
        <li>Approximate inference techniques like <a href="#variational-inference-vi-glossary">variational inference</a>, which optimize a lower bound on the log marginal likelihood (the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>).</li>
        <li>Sampling methods like Annealed Importance Sampling (AIS) to estimate it.</li>
    </ul></p>

    <h3 class="glossary-entry-title" id="markov-process-markov-chain-glossary">Markov Process / Markov Chain</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A stochastic process (a sequence of <a href="#random-variable-glossary">random variables</a>) where the future state depends only on the current state, not on the sequence of events that preceded it (the "Markov property").</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Markov processes are "memoryless": given the present, the future is conditionally independent of the past. A Markov chain is a Markov process with discrete time steps and often discrete states. They are foundational in <a href="#diffusion-model-glossary">diffusion models</a> and many generative processes involving sequences.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A Markov process is a type of random process where the future evolution of the system depends solely on its current state, and not on how it arrived at that current state. This "memoryless" characteristic is known as the Markov property.
    Mathematically, for a sequence of <a href="#random-variable-glossary">random variables</a> <code>X<sub>0</sub>, X<sub>1</sub>, X<sub>2</sub>, ...</code> (representing the state at different time steps):
    <pre>P(X<sub>t+1</sub> = x<sub>t+1</sub> | X<sub>t</sub> = x<sub>t</sub>, X<sub>t-1</sub> = x<sub>t-1</sub>, ..., X<sub>0</sub> = x<sub>0</sub>) = P(X<sub>t+1</sub> = x<sub>t+1</sub> | X<sub>t</sub> = x<sub>t</sub>)</pre>
    A <strong>Markov Chain</strong> is a specific type of Markov process that has discrete time steps and typically (though not always) a discrete state space.
    Key concepts associated with Markov chains include:
    <ul>
        <li><strong>State Space:</strong> The set of all possible values the random variables can take.</li>
        <li><strong>Transition Probabilities:</strong> The probabilities of moving from one state to another, <code>P(X<sub>t+1</sub>=j | X<sub>t</sub>=i)</code>.</li>
        <li><strong>Stationary Distribution:</strong> A probability distribution over states that remains unchanged over time if the process starts in that distribution.</li>
    </ul>
    Markov processes are widely used in:
    <ul>
        <li>Time series analysis</li>
        <li>Natural language processing (e.g., n-gram models as first-order Markov chains)</li>
        <li>Reinforcement learning (Markov Decision Processes)</li>
        <li><a href="#generative-model-glossary">Generative modeling</a>:
            <ul>
                <li>In <a href="#diffusion-model-glossary">diffusion models</a>, both the <a href="#forward-diffusion-process-glossary">forward (noising) process</a> and the learned <a href="#backward-diffusion-process-glossary">backward (denoising) process</a> are typically modeled as Markov chains.</li>
                <li>Hidden Markov Models (HMMs) use an unobserved Markov chain of hidden states to generate observed data.</li>
                <li>Autoregressive models can be seen as generating sequences where each element depends on a fixed number of previous elements (a higher-order Markov property).</li>
            </ul>
        </li>
    </ul>
    Mastering the concept of Markov processes is crucial for understanding many advanced generative models, especially those dealing with sequential data or <a href="#iterative-inference-generation-glossary">iterative refinement</a>.</p>

    <h3 class="glossary-entry-title" id="mean-field-approximation-glossary">Mean Field Approximation (Variational Family)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A common simplifying assumption in <a href="#variational-inference-vi-glossary">Variational Inference (VI)</a> where the approximate <a href="#posterior-distribution-glossary">posterior distribution</a> <code>q(z)</code> over a set of <a href="#latent-variable-glossary">latent variables</a> <code>z = (z<sub>1</sub>, ..., z<sub>M</sub>)</code> is assumed to factorize into a product of independent distributions for each variable or groups of variables.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The mean field approximation assumes that the variational distribution <code>q(z)</code> can be written as <code>Π<sub>i</sub> q<sub>i</sub>(z<sub>i</sub>)</code>. This means the latent variables are treated as independent in the approximate posterior, which greatly simplifies the optimization problem in VI, even if they are dependent in the true posterior.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In <a href="#variational-inference-vi-glossary">Variational Inference</a>, we seek an approximate distribution <code>q(z; φ)</code> from a chosen variational family that best approximates the true (often intractable) <a href="#posterior-distribution-glossary">posterior</a> <code>P(z|x)</code>. The choice of this variational family is crucial.
    The <strong>mean field approximation</strong> is a specific choice for this family where the latent variables <code>z = (z<sub>1</sub>, ..., z<sub>M</sub>)</code> are assumed to be mutually independent in the approximate posterior. This means the joint variational distribution <code>q(z)</code> can be fully factorized:
    <pre>q(z; φ) = Π<sup>M</sup><sub>i=1</sub> q<sub>i</sub>(z<sub>i</sub>; φ<sub>i</sub>)</pre>
    Each <code>q<sub>i</sub>(z<sub>i</sub>; φ<sub>i</sub>)</code> is a distribution over a single latent variable <code>z<sub>i</sub></code> (or a block of variables if using a more structured mean field), parameterized by its own set of variational parameters <code>φ<sub>i</sub></code>.
    Advantages:
    <ul>
        <li><strong>Simplicity:</strong> This factorization significantly simplifies the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> calculation and its optimization, as expectations often decompose into sums or products of simpler expectations.</li>
        <li><strong>Tractability:</strong> Optimization for each <code>q<sub>i</sub></code> can sometimes be done iteratively and in closed form in traditional VI settings (Coordinate Ascent Variational Inference - CAVI).</li>
    </ul>
    Disadvantages:
    <ul>
        <li><strong>Approximation Error:</strong> If the true posterior <code>P(z|x)</code> exhibits strong dependencies between the latent variables, the mean field approximation (which ignores these dependencies) might be a poor fit, leading to a loose bound and inaccurate posterior estimates. The KL divergence <code>KL(q || P(z|x))</code> might remain large.</li>
    </ul>
    The term "naive mean field" typically refers to this fully factorized assumption. More "structured mean field" approximations might allow for some dependencies within predefined blocks of variables while maintaining independence between blocks.
    In the context of <a href="#variational-autoencoder-vae-glossary">VAEs</a>, when the encoder <code>q<sub>φ</sub>(z|x)</code> outputs parameters for a diagonal covariance Gaussian (i.e., an <a href="#isotropic-gaussian-glossary">isotropic Gaussian</a> or a Gaussian where latent dimensions are uncorrelated), this is an instance of a mean field approximation for the latent variables <code>z</code>.</p>

    <h3 class="glossary-entry-title" id="mixture-model-glossary">Mixture Model</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A probabilistic model that represents a dataset as arising from a combination (or "mixture") of several distinct underlying <a href="#probability-distribution-glossary">probability distributions</a> (components).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Mixture models are used to model populations that are composed of multiple subpopulations. Each component distribution represents a subpopulation, and the overall model is a weighted sum of these components.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Mixture models are a powerful tool for modeling heterogeneous data that is believed to be generated by several underlying processes or groups. The overall probability distribution of a data point <code>x</code> in a mixture model is given by:
    <pre>P(x) = Σ<sup>K</sup><sub>k=1</sub> π<sub>k</sub> P<sub>k</sub>(x | θ<sub>k</sub>)</pre>
    where:
    <ul>
        <li><code>K</code> is the number of mixture components (subpopulations).</li>
        <li><code>π<sub>k</sub></code> are the mixing coefficients (or weights), representing the prior probability that a data point belongs to component <code>k</code>. These weights are non-negative and sum to 1 (<code>Σ π<sub>k</sub> = 1</code>). They often follow a <a href="#categorical-distribution-glossary">Categorical distribution</a>.</li>
        <li><code>P<sub>k</sub>(x | θ<sub>k</sub>)</code> is the probability distribution of the <code>k</code>-th component, parameterized by <code>θ<sub>k</sub></code>.</li>
    </ul>
    The most common example is the <a href="#gaussian-mixture-model-gmm-glossary"><strong>Gaussian Mixture Model (GMM)</strong></a>, where each <code>P<sub>k</sub></code> is a Gaussian distribution with its own mean and covariance.
    Mixture models are widely used for:
    <ul>
        <li><strong>Clustering:</strong> Each component can represent a cluster, and data points can be assigned to the component most likely to have generated them.</li>
        <li><strong>Density Estimation:</strong> They can approximate complex, multimodal (having multiple peaks) data distributions that a single simple distribution cannot capture.</li>
        <li><strong>Unsupervised Learning:</strong> Discovering hidden structure in data.</li>
    </ul>
    In <a href="#generative-model-glossary">generative models</a> and <a href="#variational-autoencoder-vae-glossary">VAEs</a>, mixture models can be used as flexible <a href="#prior-distribution-glossary">priors</a> (e.g., a GMM prior in the latent space) or to model the <a href="#posterior-distribution-glossary">approximate posterior distribution</a>, providing greater expressiveness and the ability to represent data with multiple modes. Training mixture models typically involves the <a href="#expectation-maximization-em-algorithm-glossary">Expectation-Maximization (EM) algorithm</a> or <a href="#variational-inference-vi-glossary">variational inference</a> to learn the component parameters <code>θ<sub>k</sub></code> and mixing coefficients <code>π<sub>k</sub></code>.</p>

    <h3 class="glossary-entry-title" id="mixture-of-experts-moe-glossary">Mixture of Experts (MoE)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A method for combining multiple individual <a href="#probability-distribution-glossary">probability distributions</a> (the "experts") by taking a weighted average (a "mixture") of them.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>In an MoE model, the final <a href="#probability-distribution-glossary">distribution</a> is a convex combination of several expert distributions. Each expert contributes to the overall distribution, and the weights can be fixed or determined by a "gating network" that selects or weights experts based on the input. This often leads to a more flexible, potentially multi-modal, but possibly less sharp <a href="#consensus-distribution-glossary">consensus</a> than <a href="#product-of-experts-poe-glossary">PoE</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Mixture of Experts (MoE) approach combines multiple expert <a href="#probability-distribution-glossary">distributions</a> <code>p<sub>i</sub>(z)</code> by forming a weighted sum, rather than a product as in <a href="#product-of-experts-poe-glossary">PoE</a>:
    <pre>p<sub>MoE</sub>(z) = Σ<sup>N</sup><sub>i=1</sub> w<sub>i</sub> p<sub>i</sub>(z)</pre>
    where:
    <ul>
        <li><code>N</code> is the number of expert distributions.</li>
        <li><code>p<sub>i</sub>(z)</code> is the <code>i</code>-th expert distribution.</li>
        <li><code>w<sub>i</sub></code> are the mixing weights (or gating probabilities). These weights must be non-negative and sum to 1 (<code>Σ w<sub>i</sub> = 1</code>). They often follow a <a href="#categorical-distribution-glossary">Categorical distribution</a>.</li>
    </ul>
    The mixing weights <code>w<sub>i</sub></code> can be:
    <ul>
        <li><strong>Fixed:</strong> E.g., uniform weights <code>w<sub>i</sub> = 1/N</code>.</li>
        <li><strong>Data-dependent:</strong> Determined by a "gating network" that takes input data <code>x</code> and outputs the weights <code>w<sub>i</sub>(x)</code>, effectively deciding which expert(s) are most relevant for that input.</li>
    </ul>
    Key characteristics and applications:
    <ul>
        <li><strong>Flexibility & Multimodality:</strong> MoE can represent complex, multimodal distributions if the expert distributions are themselves peaked in different locations. The overall distribution can capture diverse patterns by "blending" the experts.</li>
        <li><strong>Robustness:</strong> Compared to PoE, MoE can be more robust to an unreliable or outlier expert, as a single expert's low probability assignment doesn't suppress the entire mixture as strongly.</li>
        <li><strong>Application in Multimodal VAEs:</strong> In <a href="#multimodal-learning-glossary">multimodal</a> <a href="#variational-autoencoder-vae-glossary">VAEs</a>, MoE can be used to form a <a href="#consensus-distribution-glossary">consensus distribution</a> for the <a href="#joint-posterior-distribution-glossary">joint posterior</a> by mixing unimodal approximate posteriors <code>q(z|x<sub>i</sub>)</code> from each modality <code>x<sub>i</sub></code>. This is particularly useful if different modalities are experts on different parts of the latent space or if only a subset of modalities is available.</li>
        <li><strong>Conditional Computation:</strong> When weights are data-dependent, MoE allows for specialized experts to handle different types of inputs, leading to more efficient models (e.g., in large language models, only a subset of experts might be activated for a given token).</li>
    </ul>
    While MoE offers flexibility, it may not combine information as "sharply" as PoE when all experts agree and provide complementary evidence for a unimodal consensus. The choice between PoE and MoE (or more advanced methods like <a href="#consensus-of-dependent-experts-code-glossary">CoDE</a>) depends on the specific assumptions about the experts and the desired properties of the combined distribution.</p>

    <h3 class="glossary-entry-title" id="multimodal-learning-glossary">Multimodal Learning</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A subfield of machine learning that focuses on building models capable of processing, relating, and learning from information derived from multiple types or "modalities" of data simultaneously (e.g., images and text, audio and video, sensor data and medical records).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Multimodal learning aims to create AI systems that can understand the world in a way that is more akin to human perception, by integrating diverse sources of information. This often involves learning shared or coordinated representations across modalities to enable tasks like <a href="#cross-modal-generation-glossary">cross-modal generation</a>, fusion, and retrieval.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Multimodal learning deals with data that comes from different channels or sources, each referred to as a modality. Each modality typically has distinct statistical properties and representations (e.g., images are grids of pixels, text is a sequence of tokens, audio is a waveform). The primary goals of multimodal learning include:
    <ul>
        <li><strong>Representation Learning:</strong>
            <ul>
                <li><strong>Shared Representation:</strong> Learning a common embedding space where data from different modalities can be projected and compared (e.g., aligning image embeddings with text embeddings).</li>
                <li><strong>Coordinated Representation:</strong> Learning separate representations for each modality but ensuring they are aligned or correlated in some meaningful way (e.g., via <a href="#canonical-correlation-analysis-cca-glossary">CCA</a>).</li>
            </ul>
        </li>
        <li><strong>Translation:</strong> Converting data from one modality to another (e.g., image captioning: image to text; text-to-image synthesis: text to image). This is a form of <a href="#cross-modal-generation-glossary">cross-modal generation</a>.</li>
        <li><strong>Fusion:</strong> Combining information from multiple modalities to perform a task more effectively than with a single modality (e.g., using both visual and audio cues for emotion recognition, or combining image, text, and tabular data for medical diagnosis). This often involves creating a <a href="#consensus-distribution-glossary">consensus distribution</a> from unimodal <a href="#posterior-distribution-glossary">posteriors</a>.</li>
        <li><strong>Co-learning:</strong> Using information from one modality to help learn or improve the model for another modality, especially when data for one modality is scarce.</li>
    </ul>
    In the context of <a href="#generative-model-glossary">generative models</a>, multimodal learning often involves building models like multimodal <a href="#variational-autoencoder-vae-glossary">VAEs</a> or GANs that can generate consistent data across multiple modalities or perform <a href="#cross-modal-generation-glossary">cross-modal generation</a>. This typically requires careful design of the <a href="#latent-variable-glossary">latent space</a> to capture shared information and mechanisms for combining evidence from different modalities, such as by learning a <a href="#joint-posterior-distribution-glossary">joint posterior distribution</a> or using consensus mechanisms like <a href="#product-of-experts-poe-glossary">PoE</a> or <a href="#mixture-of-experts-moe-glossary">MoE</a>.
    Multimodal learning is crucial for AI applications that interact with the complex, multisensory real world, including robotics, autonomous driving, human-computer interaction, virtual assistants, and multimedia content analysis.</p>

    <h3 class="glossary-entry-title" id="multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian (Normal) Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A generalization of the one-dimensional (univariate) Gaussian or normal <a href="#probability-distribution-glossary">distribution</a> to two or more <a href="#random-variable-glossary">random variables</a>, allowing for modeling correlations between them.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>A multivariate Gaussian models the joint behavior of several continuous variables. It's defined by a mean vector (specifying the center) and a covariance matrix (specifying the spread and orientation of the distribution, including correlations).</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The multivariate Gaussian (or normal) distribution extends the familiar bell curve to higher dimensions (for a D-dimensional random vector <strong>X</strong>). Instead of a single mean (μ) and variance (σ²), it is characterized by:
    <ul>
        <li>A <strong>mean vector</strong> <strong>μ</strong> (a D-dimensional vector), which represents the center of the distribution.</li>
        <li>A <strong>covariance matrix</strong> <strong>Σ</strong> (a DxD symmetric positive semi-definite matrix). The diagonal elements of <strong>Σ</strong> are the variances of individual variables, and the off-diagonal elements are the covariances between pairs of variables, describing how they vary together.</li>
    </ul>
    The probability density function (PDF) is:
    <pre>P(<strong>x</strong> | <strong>μ</strong>, <strong>Σ</strong>) = (1 / ((2π)<sup>D/2</sup> |<strong>Σ</strong>|<sup>1/2</sup>)) * exp(-1/2 (<strong>x</strong>-<strong>μ</strong>)<sup>T</sup><strong>Σ</strong><sup>-1</sup>(<strong>x</strong>-<strong>μ</strong>))</pre>
    The <a href="#log-density-of-a-gaussian-glossary">log density of this Gaussian</a> is often used in calculations. <code>|<strong>Σ</strong>|</code> is the determinant of the covariance matrix and <code><strong>Σ</strong><sup>-1</sup></code> is its inverse (the <a href="#precision-in-gaussians-glossary">precision matrix</a>).
    Contours of equal probability density form ellipses in 2D, or ellipsoids (and hyperellipsoids) in higher dimensions. If the covariance matrix is diagonal, the variables are uncorrelated. If it's an <a href="#isotropic-gaussian-glossary">isotropic Gaussian</a> (<strong>Σ</strong> = σ²<strong>I</strong>), the contours are circles/spheres.
    Multivariate Gaussians are widely used in <a href="#generative-model-glossary">generative models</a>, clustering (e.g., <a href="#gaussian-mixture-model-gmm-glossary">GMMs</a>), and as <a href="#prior-distribution-glossary">priors</a> or approximate <a href="#posterior-distribution-glossary">posteriors</a> in <a href="#variational-autoencoder-vae-glossary">VAEs</a>. They are mathematically convenient because many operations like marginalization, conditioning, and linear transformations result in another Gaussian distribution. They can also approximate a wide range of real-world data distributions when combined in <a href="#mixture-model-glossary">mixtures</a>.</p>

    <h3 class="glossary-entry-title" id="n-0-1-standard-normal-distribution-glossary">N(0,1) - Standard Normal Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>Mathematical notation for the standard normal distribution, which is a <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian distribution</a> with mean = 0 and variance = 1. The notation N(μ, σ²) represents a normal distribution with mean μ and variance σ².</p>
    <span class="entry-section-title">Explanation:</span>
    <p>N(0,1) is the "standard" or "canonical" form of the normal distribution - the classic bell curve centered at zero with unit variance. It's characterized by its symmetric bell shape where 68% of values fall within [-1, 1], 95% within [-2, 2], and 99.7% within [-3, 3].</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The standard normal distribution N(0,1) is fundamental in statistics and machine learning. The general notation N(μ, σ²) describes a normal distribution where:
    <ul>
        <li><strong>μ (mu):</strong> The mean or center of the distribution - where the peak of the bell curve is located</li>
        <li><strong>σ² (sigma squared):</strong> The variance - how spread out the distribution is</li>
    </ul>
    So N(0,1) specifically means:
    <ul>
        <li><strong>Mean = 0:</strong> The distribution is centered at zero</li>
        <li><strong>Variance = 1:</strong> The standard deviation is 1, giving it a "standard" spread</li>
    </ul>
    The standard normal distribution has the probability density function:
    <pre>p(x) = (1/√(2π)) * exp(-x²/2)</pre>
    Key properties of the standard normal (bell curve):
    <ul>
        <li><strong>68-95-99.7 Rule:</strong> Approximately 68% of values fall within 1 standard deviation of the mean [-1, 1], 95% within 2 standard deviations [-2, 2], and 99.7% within 3 standard deviations [-3, 3]</li>
        <li><strong>Symmetric:</strong> The distribution is perfectly symmetric around the mean (0)</li>
        <li><strong>Continuous:</strong> Can take any real value, but values far from the mean become exponentially less likely</li>
    </ul>
    Examples of other normal distributions:
    <ul>
        <li><strong>N(2, 1.5):</strong> Mean = 2, variance = 1.5 (centered at 2, more spread out than N(0,1))</li>
        <li><strong>N(-1, 0.5):</strong> Mean = -1, variance = 0.5 (centered at -1, less spread out than N(0,1))</li>
    </ul>
    In <a href="#variational-autoencoder-vae-glossary">VAEs</a> and <a href="#generative-model-glossary">generative models</a>, N(0,1) is commonly used as:
    <ul>
        <li><strong>Prior distribution:</strong> The target distribution that <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL regularization</a> pushes encoded distributions toward</li>
        <li><strong>Sampling distribution:</strong> During generation, we sample random points from N(0,1) and decode them into new data</li>
        <li><strong>Reparameterization:</strong> In the <a href="#reparameterization-trick-glossary">reparameterization trick</a>, we sample ε ~ N(0,1) and transform it: z = μ + σ * ε</li>
    </ul>
    The choice of N(0,1) as a standard is motivated by its mathematical convenience (simple form, well-studied properties) and its role as a "neutral" distribution that's neither too concentrated nor too spread out, making it ideal for regularization and sampling in generative models.</p>

    <h3 class="glossary-entry-title" id="nat-glossary">Nat</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A unit of information based on the <a href="#logarithm-log-natural-logarithm-ln-glossary">natural logarithm</a> (base e).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>When measuring information or entropy using natural logs, the result is in "nats" instead of "bits" (which use base 2 <a href="#logarithm-log-natural-logarithm-ln-glossary">logarithms</a>).</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In information theory, the amount of information or uncertainty (entropy) can be measured using different logarithmic bases. If you use the <a href="#logarithm-log-natural-logarithm-ln-glossary">natural logarithm</a> (ln, base e), the resulting unit is called a “nat” (natural unit of information). If you use the logarithm base 2, the unit is a “bit.” One nat is the amount of information gained when an event with probability 1/e occurs. (Similarly, one bit is the information gained when an event with probability 1/2 occurs). In probabilistic modeling and machine learning, natural logs are often used for mathematical convenience (e.g., their derivatives are simpler), so entropy, <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a>, and related quantities like the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> are often measured in nats by default. While the choice of unit doesn’t affect the underlying concepts, it’s important to be aware of which unit is being used, especially when interpreting results or comparing models. (1 nat ≈ 1.44 bits, since ln(x) = ln(2) * log<sub>2</sub>(x) and 1/ln(2) ≈ 1.44). In <a href="#variational-autoencoder-vae-glossary">VAEs</a> and most modern <a href="#generative-model-glossary">generative models</a>, nats are the standard unit for measuring information-theoretic quantities.</p>

    <h3 class="glossary-entry-title" id="noise-schedule-glossary">Noise Schedule (β<sub>t</sub>, α<sub>t</sub>, ᾱ<sub>t</sub>)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#diffusion-model-glossary">diffusion models</a>, a predefined sequence of parameters (<code>β<sub>t</sub></code>, or derived <code>α<sub>t</sub></code>, <code>ᾱ<sub>t</sub></code>) that control the amount of noise added at each step <code>t</code> of the <a href="#forward-diffusion-process-glossary">forward diffusion process</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The noise schedule determines how quickly data is corrupted into noise. <code>β<sub>t</sub></code> is the variance of noise added at step <code>t</code>. <code>α<sub>t</sub> = 1-β<sub>t</sub></code> scales the previous state, and <code>ᾱ<sub>t</sub> = Π<sup>t</sup><sub>s=1</sub>α<sub>s</sub></code> is the cumulative product, indicating the "signal rate" remaining from the original data <code>x<sub>0</sub></code> at step <code>t</code>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The noise schedule is a critical set of hyperparameters in <a href="#diffusion-model-glossary">diffusion models</a>, defining the fixed <a href="#forward-diffusion-process-glossary">forward process</a>.
    <ul>
        <li><strong><code>β<sub>t</sub></code> (Variance Schedule):</strong> A sequence of small positive values <code>β<sub>1</sub>, β<sub>2</sub>, ..., β<sub>T</sub></code>. Each <code>β<sub>t</sub></code> represents the variance of the Gaussian noise added at timestep <code>t</code>. The forward process is defined as <code>q(x<sub>t</sub> | x<sub>t-1</sub>) = N(x<sub>t</sub>; √(1-β<sub>t</sub>)x<sub>t-1</sub>, β<sub>t</sub>I)</code>. These are typically chosen to be small, often increasing from <code>β<sub>1</sub></code> (e.g., 10<sup>-4</sup>) to <code>β<sub>T</sub></code> (e.g., 0.02) over <code>T</code> steps (e.g., T=1000).</li>
        <li><strong><code>α<sub>t</sub></code>:</strong> Defined as <code>α<sub>t</sub> = 1 - β<sub>t</sub></code>. This term represents the scaling factor for the previous state <code>x<sub>t-1</sub></code> before noise is added. Since <code>β<sub>t</sub></code> is small, <code>α<sub>t</sub></code> is close to 1.</li>
        <li><strong><code>ᾱ<sub>t</sub></code> (Cumulative Alpha / Signal Rate):</strong> Defined as the cumulative product <code>ᾱ<sub>t</sub> = Π<sup>t</sup><sub>s=1</sub>α<sub>s</sub></code>. This important quantity determines how much of the original signal <code>x<sub>0</sub></code> remains in the noisy sample <code>x<sub>t</sub></code> when sampling directly via <code>q(x<sub>t</sub> | x<sub>0</sub>) = N(x<sub>t</sub>; √ᾱ<sub>t</sub>x<sub>0</sub>, (1-ᾱ<sub>t</sub>)I)</code>.
            <ul>
                <li>As <code>t</code> increases, <code>ᾱ<sub>t</sub></code> decreases from <code>ᾱ<sub>0</sub>=1</code> towards 0.</li>
                <li>The schedule is designed so that <code>ᾱ<sub>T</sub> ≈ 0</code>, ensuring that <code>x<sub>T</sub></code> is almost pure noise (i.e., <code>x<sub>T</sub> ≈ √(1-ᾱ<sub>T</sub>)ε ≈ ε</code>, where <code>ε ~ N(0,I)</code>).</li>
            </ul>
        </li>
    </ul>
    The choice of noise schedule (e.g., linear, cosine, learned) significantly impacts the training stability, sample quality, and convergence speed of diffusion models. It affects how quickly information is destroyed in the forward process and, consequently, how challenging the <a href="#backward-diffusion-process-glossary">backward (denoising) process</a> is to learn. These parameters appear throughout the mathematical formulation of diffusion models, including their <a href="#diffusion-model-objective-function-glossary">objective functions</a> and sampling procedures.</p>

    <h3 class="glossary-entry-title" id="objective-function-glossary">Objective Function (Loss Function / Cost Function)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A mathematical function that is optimized (maximized or minimized) during model training to achieve a desired outcome.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>In machine learning, the objective function defines what the model is trying to learn by quantifying its performance. Examples include maximizing <a href="#likelihood-glossary">likelihood</a>, minimizing error, or maximizing the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>. The optimization is typically done using methods like <a href="#stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The objective function (also known as a loss function or cost function, particularly when being minimized) is at the heart of any optimization problem, including those in machine learning and statistics. It provides a quantitative measure of how well a model is performing on a given task or how well it fits the data. The process of training a model involves adjusting its parameters (e.g., weights in a neural network) to find values that optimize this function (either maximize or minimize it, depending on the formulation).
    Examples:
    <ul>
        <li>In linear regression, the objective function might be the mean squared error (to be minimized).</li>
        <li>In classification, it might be the cross-entropy loss (to be minimized).</li>
        <li>In Maximum Likelihood Estimation, it's the <a href="#log-likelihood-glossary">log-likelihood</a> (to be maximized).</li>
        <li>In <a href="#variational-autoencoder-vae-glossary">VAEs</a>, the objective function is the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> (to be maximized).</li>
        <li>In <a href="#diffusion-model-glossary">Diffusion Models</a>, the objective is often a specific form of the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> or a simplified version related to <a href="#denoising-score-matching-glossary">denoising score matching</a> (see <a href="#diffusion-model-objective-function-glossary">Diffusion Model Objective Function</a>).</li>
        <li>In GANs, it's a minimax objective based on the discriminator's ability to distinguish real from fake samples. This includes <a href="#likelihood-free-objective-function-glossary">likelihood-free objectives</a>.</li>
    </ul>
    Understanding the objective function is crucial for interpreting model behavior, diagnosing issues during training, and knowing what properties the learned model is likely to have. The optimization is commonly performed using gradient-based methods like <a href="#stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent (SGD)</a> or its variants (e.g., Adam).</p>

    <h3 class="glossary-entry-title" id="posterior-distribution-glossary">Posterior Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#bayesian-inference-glossary">Bayesian inference</a>, the <a href="#probability-distribution-glossary">probability distribution</a> of parameters or <a href="#latent-variable-glossary">latent variables</a> after observing the data.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The posterior, <code>P(θ|x)</code> or <code>P(z|x)</code>, tells us what we believe about the unknown quantities (<code>θ</code> or <code>z</code>) after taking the observed data <code>x</code> into account, combining the <a href="#prior-distribution-glossary">prior</a> and the <a href="#likelihood-glossary">likelihood</a> via Bayes' theorem.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The posterior distribution is the cornerstone of <a href="#bayesian-inference-glossary">Bayesian statistics</a> and <a href="#generative-model-glossary">generative modeling</a>. It represents our updated knowledge or belief about unobserved quantities (model parameters <code>θ</code> or <a href="#latent-variable-glossary">latent variables</a> <code>z</code>) after observing the data <code>x</code>. Mathematically, it is computed using Bayes’ theorem:
    <pre>Posterior(Parameters | Data) ∝ Likelihood(Data | Parameters) × Prior(Parameters)</pre>
    or for latent variables:
    <pre>P(z|x) ∝ P(x|z)P(z)</pre>
    The posterior distribution encapsulates all information about the unobserved quantities that can be inferred from the data and the prior. In practice, the posterior can take complex forms, especially in models with many variables or non-linear relationships. For example, in a <a href="#variational-autoencoder-vae-glossary">VAE</a>, the posterior describes the distribution over the latent space <code>z</code> given an observed data point <code>x</code>, i.e., <code>P(z|x)</code>. This true posterior is typically intractable to compute exactly. Therefore, approximate inference techniques, such as <a href="#variational-inference-vi-glossary">variational inference</a>, are used to find a simpler, tractable distribution <code>q(z|x)</code> (the "approximate posterior" or "variational posterior," often modeled using a <a href="#mean-field-approximation-glossary">mean field approximation</a>) that is close to the true posterior <code>P(z|x)</code>. The quality of generative models often depends significantly on how well this approximation is performed.</p>

    <h3 class="glossary-entry-title" id="precision-in-gaussians-glossary">Precision (in Gaussians)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>The inverse of the variance (for univariate Gaussians) or the inverse of the covariance matrix (for <a href="#multivariate-gaussian-normal-distribution-glossary">multivariate Gaussians</a>).</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Precision quantifies how "sharp" or "narrow" a Gaussian distribution is. Higher precision means less spread (i.e., lower variance) and thus more certainty about the variable's value.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>In statistics, precision offers an alternative parameterization for Gaussian distributions that focuses on certainty rather than spread.
    <ul>
        <li>For a <strong>univariate Gaussian</strong> with variance σ², the precision τ is <code>τ = 1/σ²</code>.</li>
        <li>For a <a href="#multivariate-gaussian-normal-distribution-glossary"><strong>multivariate Gaussian</strong></a> with covariance matrix <strong>Σ</strong>, the precision matrix (or concentration matrix) <strong>Λ</strong> (Lambda) is <code><strong>Λ</strong> = <strong>Σ</strong><sup>-1</sup></code>.</li>
    </ul>
    While variance measures how spread out a distribution is, precision measures how tightly it is concentrated around the mean.
    <ul>
        <li>Large variance (wide distribution) ⇔ Small precision.</li>
        <li>Small variance (narrow distribution) ⇔ Large precision.</li>
    </ul>
    In multivariate settings, the precision matrix <strong>Λ</strong> is interesting because its zero entries correspond to conditional independencies between variables. Specifically, if <code>Λ<sub>ij</sub> = 0</code>, then variables <code>X<sub>i</sub></code> and <code>X<sub>J</sub></code> are conditionally independent given all other variables. This is a property not directly evident from the covariance matrix.
    Precision is particularly important in some Bayesian contexts and algorithms:
    <ul>
        <li><strong>Gaussian Belief Propagation:</strong> Operations can sometimes be simpler when parameterized by precision.</li>
        <li><a href="#product-of-experts-poe-glossary"><strong>Product of Experts (PoE)</strong></a>: When combining Gaussian "experts," their precisions add up (if they are independent). The mean of the product is a precision-weighted average of the individual means.</li>
    </ul>
    Understanding precision helps in interpreting the certainty of a model's probabilistic estimates and is useful in certain mathematical manipulations of Gaussian distributions within <a href="#generative-model-glossary">generative models</a>.</p>

    <h3 class="glossary-entry-title" id="prior-distribution-glossary">Prior Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>In <a href="#bayesian-inference-glossary">Bayesian inference</a>, the <a href="#probability-distribution-glossary">probability distribution</a> representing beliefs about unknown parameters or <a href="#latent-variable-glossary">latent variables</a> <em>before</em> observing any data.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>The prior, <code>P(θ)</code> or <code>P(z)</code>, encodes our assumptions, domain knowledge, or initial state of ignorance about a variable before evidence is considered. It's a key component in Bayes' theorem for calculating the <a href="#posterior-distribution-glossary">posterior</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A prior distribution is a fundamental concept in <a href="#bayesian-inference-glossary">Bayesian inference</a>. It expresses our beliefs about the plausible values of unobserved quantities (model parameters <code>θ</code> or <a href="#latent-variable-glossary">latent variables</a> <code>z</code>) before any data <code>x</code> is observed. Priors can be:
    <ul>
        <li><strong>Informative:</strong> Based on previous knowledge, expert opinion, or results from past studies.</li>
        <li><strong>Uninformative/Vague:</strong> Chosen to reflect a lack of specific prior knowledge, aiming to let the data speak for itself as much as possible (e.g., a uniform distribution over a wide range, or a Gaussian with very large variance).</li>
        <li><strong>Chosen for mathematical convenience:</strong> Such as conjugate priors, which simplify the calculation of the <a href="#posterior-distribution-glossary">posterior distribution</a>.</li>
    </ul>
    For example, in <a href="#variational-autoencoder-vae-glossary">VAEs</a>, the prior over the latent variables <code>P(z)</code> is often chosen to be a standard <a href="#isotropic-gaussian-glossary">isotropic Gaussian</a> distribution (<code>N(0, I)</code>). This choice acts as a regularizer, encouraging the encoder to map data points into a well-behaved, continuous, and centered latent space, which is easy to sample from for generation. In <a href="#diffusion-model-glossary">diffusion models</a>, the distribution of the noisiest state <code>p(x<sub>T</sub>)</code> is also a prior, typically <code>N(0,I)</code>. The choice of prior can significantly impact the <a href="#posterior-distribution-glossary">posterior distribution</a> and, consequently, the behavior and performance of generative models, especially when data is scarce. Understanding and appropriately choosing priors is essential for interpreting Bayesian models and for designing models that generalize well to new data.</p>

    <h3 class="glossary-entry-title" id="probability-distribution-glossary">Probability Distribution</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A function describing the likelihood of different outcomes in a random process.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Probability distributions (like the normal or Gaussian distribution) tell us how likely it is to observe each possible value of a <a href="#random-variable-glossary">random variable</a>, forming the backbone of probabilistic modeling.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Probability distributions are fundamental to statistics and machine learning because they provide a formal way to describe uncertainty and variability in data. For discrete variables, a probability distribution assigns a probability to each possible outcome (for example, the probability of each face of a die). For continuous variables, the distribution is described by a probability density function (PDF), which tells you how likely it is to observe values in a certain range (like heights of people). Common distributions include the <a href="#bernoulli-distribution-glossary">Bernoulli</a>, Binomial, <a href="#categorical-distribution-glossary">Categorical</a>, Uniform, and <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian (Normal)</a> distributions. In <a href="#generative-model-glossary">generative modeling</a>, probability distributions are used to model both the data we observe and the hidden (<a href="#latent-variable-glossary">latent</a>) variables that explain the data. Understanding how to work with these distributions—how to compute probabilities, <a href="#expectation-e-glossary">expectations</a>, and how to sample from them—is a critical skill for anyone working with probabilistic models.</p>

    <h3 class="glossary-entry-title" id="product-of-experts-poe-glossary">Product of Experts (PoE)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A method for combining multiple individual <a href="#probability-distribution-glossary">probability distributions</a> (the "experts") by multiplying their probability densities (or masses) together and then normalizing the result to form a new valid distribution.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>PoE is based on the idea that each expert provides some information about a variable. The product gives high probability only to outcomes where all (or most) experts agree (assign high probability). This typically results in a "sharper" or more confident (lower variance) combined estimate compared to individual experts.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The Product of Experts (PoE) framework provides a way to combine information from multiple sources, each modeled as a <a href="#probability-distribution-glossary">probability distribution</a> (an "expert") over a variable <code>z</code>. If we have <code>N</code> expert distributions <code>p<sub>1</sub>(z), p<sub>2</sub>(z), ..., p<sub>N</sub>(z)</code>, the combined PoE distribution <code>p<sub>PoE</sub>(z)</code> is defined as:
    <pre>p<sub>PoE</sub>(z) = (1/Z) * Π<sup>N</sup><sub>i=1</sub> p<sub>i</sub>(z)</pre>
    where <code>Z</code> is a normalization constant (the partition function) ensuring that <code>p<sub>PoE</sub>(z)</code> integrates/sums to 1.
    Key characteristics and applications:
    <ul>
        <li><strong>Sharpening Consensus:</strong> The product ensures that <code>p<sub>PoE</sub>(z)</code> is large only if all (or most) individual <code>p<sub>i</sub>(z)</code> are large. If any expert assigns a very low probability to a region of <code>z</code>, the product will also be very low there. This leads to a consensus that is often more peaked (lower variance) than any single expert.</li>
        <li><strong>Gaussian Experts:</strong> If all experts <code>p<sub>i</sub>(z)</code> are <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian distributions</a>, <code>p<sub>i</sub>(z) = N(z; μ<sub>i</sub>, Σ<sub>i</sub>)</code>, then their product <code>p<sub>PoE</sub>(z)</code> is also a Gaussian distribution (unnormalized). The <a href="#precision-in-gaussians-glossary">precision matrix</a> of the PoE Gaussian is the sum of the individual precision matrices: <code>Λ<sub>PoE</sub> = Σ<sub>i</sub> Λ<sub>i</sub></code> (where <code>Λ = Σ<sup>-1</sup></code>). The mean of the PoE Gaussian is a precision-weighted average of the individual means: <code>μ<sub>PoE</sub> = Σ<sub>PoE</sub> (Σ<sub>i</sub> Λ<sub>i</sub>μ<sub>i</sub>)</code>. This makes PoE particularly convenient for Gaussian experts.</li>
        <li><strong>Application in Multimodal VAEs:</strong> In <a href="#multimodal-learning-glossary">multimodal</a> <a href="#variational-autoencoder-vae-glossary">VAEs</a>, PoE is often used to form a <a href="#consensus-distribution-glossary">consensus distribution</a> (approximating the <a href="#joint-posterior-distribution-glossary">joint posterior</a> <code>q(z | x<sub>1</sub>, ..., x<sub>N</sub>)</code>) by taking the product of unimodal approximate posteriors <code>q(z | x<sub>i</sub>)</code> derived from each modality <code>x<sub>i</sub></code>.</li>
        <li><strong>Assumption of Conditional Independence:</strong> PoE implicitly assumes that the experts are conditionally independent given <code>z</code> when used for posterior approximation. If experts are highly correlated in their errors, PoE can become overconfident.</li>
    </ul>
    PoE is a powerful tool for information fusion, especially when experts provide complementary pieces of evidence.</p>

    <h3 class="glossary-entry-title" id="random-variable-glossary">Random Variable</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A variable whose possible values are outcomes of a random phenomenon.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>Random variables can be discrete (like the result of a dice roll) or continuous (like the height of a person), and they are the basic building blocks for <a href="#probability-distribution-glossary">probability distributions</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>A random variable is a mathematical object that represents the outcome of a random process. For example, when you flip a coin, you can define a random variable X that takes the value 1 for heads and 0 for tails. In a more complex case, you might have a random variable representing the temperature in a city on a given day, which can take any value within a range. Random variables allow us to use the machinery of probability theory to describe and analyze uncertainty. In <a href="#generative-model-glossary">generative models</a> and <a href="#variational-autoencoder-vae-glossary">VAEs</a>, random variables are used to represent both observed data (like images or words) and unobserved, hidden factors (<a href="#latent-variable-glossary">latent variables</a>) that explain the patterns in the data. Understanding random variables is key to understanding how models can generate data and how they can learn from uncertainty.</p>

    <h3 class="glossary-entry-title" id="reparameterization-trick-glossary">Reparameterization Trick</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A technique used in <a href="#variational-inference-vi-glossary">variational inference</a> (especially in <a href="#variational-autoencoder-vae-glossary">VAEs</a>) to obtain low-variance gradient estimates for an <a href="#objective-function-glossary">objective function</a> involving an <a href="#expectation-e-glossary">expectation</a> with respect to a distribution whose parameters are being learned.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>When a model needs to sample a <a href="#latent-variable-glossary">latent variable</a> <code>z</code> from a parameterized distribution <code>q<sub>φ</sub>(z|x)</code> (e.g., an encoder output), the sampling operation itself is non-differentiable with respect to <code>φ</code>. The reparameterization trick rewrites <code>z</code> as a deterministic function of <code>φ</code> and an independent noise variable <code>ε</code> (e.g., for a Gaussian, <code>z = μ<sub>φ</sub>(x) + σ<sub>φ</sub>(x) * ε</code>, where <code>ε ~ N(0,I)</code>). This allows gradients from the objective to flow back to <code>φ</code> through the deterministic transformation.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The reparameterization trick is a crucial technique for training <a href="#variational-autoencoder-vae-glossary">Variational Autoencoders (VAEs)</a> and other models that involve optimizing an <a href="#objective-function-glossary">objective function</a> containing an <a href="#expectation-e-glossary">expectation</a> where the distribution itself is parameterized by learnable parameters <code>φ</code>.
    Consider an objective of the form <code>L(φ) = E<sub>z ~ q<sub>φ</sub>(z|x)</sub>[f(z)]</code>. If we want to compute the gradient <code>∇<sub>φ</sub>L(φ)</code> to optimize <code>φ</code> using gradient descent, we face a problem: the sampling operation <code>z ~ q<sub>φ</sub>(z|x)</code> is stochastic and generally not differentiable.
    The reparameterization trick solves this by:
    <ol>
        <li>Expressing the <a href="#random-variable-glossary">random variable</a> <code>z</code> as a deterministic transformation <code>g(φ, x, ε)</code> of the parameters <code>φ</code> (and possibly input <code>x</code>) and an auxiliary independent noise variable <code>ε</code>, where <code>ε</code> is sampled from a fixed, parameter-free distribution <code>p(ε)</code> (e.g., <code>N(0,I)</code>).</li>
        <li>The transformation <code>g</code> is chosen such that <code>z = g(φ, x, ε)</code> has the desired distribution <code>q<sub>φ</sub>(z|x)</code>.</li>
    </ol>
    For example, if <code>q<sub>φ</sub>(z|x)</code> is a Gaussian distribution <code>N(z | μ<sub>φ</sub>(x), σ²<sub>φ</sub>(x)I)</code>, where <code>μ<sub>φ</sub>(x)</code> and <code>σ<sub>φ</sub>(x)</code> are outputs of an encoder network with parameters <code>φ</code>, we can reparameterize as:
    <pre>z = μ<sub>φ</sub>(x) + σ<sub>φ</sub>(x) * ε,  where ε ~ N(0,I)</pre>
    Now, the expectation becomes <code>L(φ) = E<sub>ε ~ p(ε)</sub>[f(g(φ, x, ε))]</code>.
    The gradient can be moved inside the expectation (since <code>p(ε)</code> does not depend on <code>φ</code>):
    <pre>∇<sub>φ</sub>L(φ) = E<sub>ε ~ p(ε)</sub>[∇<sub>φ</sub>f(g(φ, x, ε))]</pre>
    This gradient can be estimated using Monte Carlo sampling (often with just one sample of <code>ε</code> per data point in <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>): draw samples of <code>ε</code>, compute <code>∇<sub>φ</sub>f(g(φ, x, ε))</code> for each, and average. The gradients now flow through the deterministic function <code>g</code> to <code>φ</code>, allowing end-to-end training of the model (e.g., the encoder in a VAE) using standard backpropagation. This typically results in much lower variance gradient estimates compared to other methods like the score function estimator (REINFORCE).</p>

    <h3 class="glossary-entry-title" id="stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent (SGD)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An iterative optimization algorithm used for finding the minimum (or maximum) of an <a href="#objective-function-glossary">objective function</a>, commonly used in training machine learning models, especially deep neural networks.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>SGD updates model parameters by taking steps in the negative direction of the gradient of the objective function. Unlike standard Gradient Descent (which computes the gradient over the entire dataset), SGD computes the gradient using only a single data sample or a small "mini-batch" of samples at each step, making it much faster for large datasets.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Stochastic Gradient Descent (SGD) is a widely used optimization algorithm for training machine learning models by minimizing (or maximizing) an <a href="#objective-function-glossary">objective function</a> <code>L(θ)</code> that depends on model parameters <code>θ</code>.
    The update rule for minimizing a loss function is:
    <pre>θ<sub>new</sub> = θ<sub>old</sub> - α ∇<sub>θ</sub>L̃(θ<sub>old</sub>; x<sup>(i)</sup>, y<sup>(i)</sup>)</pre>
    where:
    <ul>
        <li><code>θ</code> are the model parameters.</li>
        <li><code>α</code> is the learning rate, a hyperparameter controlling the step size.</li>
        <li><code>L̃(θ; x<sup>(i)</sup>, y<sup>(i)</sup>)</code> is the loss computed on a single training example <code>(x<sup>(i)</sup>, y<sup>(i)</sup>)</code> or a small mini-batch of examples. <code>∇<sub>θ</sub>L̃</code> is an estimate of the true gradient <code>∇<sub>θ</sub>L</code> (which would be an average over the entire dataset).</li>
    </ul>
    Key aspects of SGD:
    <ul>
        <li><strong>Stochasticity:</strong> The gradient estimate is noisy because it's based on a subset of data. This noise can help escape shallow local minima but also makes the convergence path jittery.</li>
        <li><strong>Efficiency:</strong> For large datasets, computing the full gradient is very expensive. SGD provides a much faster (though noisier) update, allowing for many more updates in the same amount of time.</li>
        <li><strong>Mini-batches:</strong> In practice, "mini-batch SGD" is almost always used, where the gradient is computed over a small batch (e.g., 32 to 256 samples) rather than a single sample. This reduces the variance of the gradient estimate while still being computationally efficient.</li>
        <li><strong>Variants:</strong> Many variants and improvements on SGD exist, such as Momentum, AdaGrad, RMSProp, and Adam, which adapt the learning rate or incorporate momentum to improve convergence speed and stability.</li>
    </ul>
    In the context of <a href="#variational-autoencoder-vae-glossary">VAEs</a> and <a href="#diffusion-model-glossary">Diffusion Models</a>, objective functions like the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> often involve <a href="#expectation-e-glossary">expectations</a> that are estimated using Monte Carlo sampling (e.g., sampling <code>z</code> from <code>q(z|x)</code> via the <a href="#reparameterization-trick-glossary">reparameterization trick</a>). When these Monte Carlo estimates are used to compute the gradient for an SGD update, the gradient itself becomes stochastic due to both the data sub-sampling and the internal sampling of latent variables. This is a core part of how these models are trained.</p>

    <h3 class="glossary-entry-title" id="trace-trick-glossary">Trace Trick</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A mathematical identity for the trace of a product of matrices: <code>tr(ABC) = tr(CAB) = tr(BCA)</code>, useful in matrix calculus.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>This trick allows cyclic permutation of matrices within a trace operation (<code>tr</code>), which can simplify expressions or facilitate gradient calculations involving matrices. The trace of a scalar is the scalar itself.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The trace of a square matrix is the sum of its diagonal elements. The trace trick, or cyclic property of the trace, states that for matrices A, B, C such that their products are well-defined and result in square matrices where appropriate for the trace:
    <pre>tr(ABC) = tr(CAB) = tr(BCA)</pre>
    Note that the order cannot be arbitrarily changed, only cyclically permuted. This property is very useful in machine learning, particularly when dealing with expectations of quadratic forms or derivatives involving matrix-variate Gaussian distributions.
    For example, it can appear in derivations of the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> for <a href="#variational-autoencoder-vae-glossary">VAEs</a> with Gaussian posteriors (e.g., when calculating the <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a> between two Gaussians) or in the optimization of other probabilistic models involving matrix parameters.
    It helps in manipulating expressions into forms that are easier to compute or differentiate. A common application is simplifying expectations: for a random vector <code>x</code> and a constant matrix <code>A</code>, the quadratic form <code>x<sup>T</sup>Ax</code> is a scalar. Thus, <code>x<sup>T</sup>Ax = tr(x<sup>T</sup>Ax)</code>. Using the cyclic property, this can be written as <code>tr(Axx<sup>T</sup>)</code>.
    Then, <code>E[x<sup>T</sup>Ax] = E[tr(Axx<sup>T</sup>)]</code>. If expectation and trace can be swapped (which is true if A is constant), this becomes <code>tr(E[Axx<sup>T</sup>]) = tr(A * E[xx<sup>T</sup>])</code>. The term <code>E[xx<sup>T</sup>]</code> is the second moment matrix of <code>x</code>, which is related to its covariance (<code>Cov(x) = E[xx<sup>T</sup>] - E[x]E[x]<sup>T</sup></code>). This form can be much easier to work with, especially when <code>x</code> is a sample from a known distribution.</p>

    <h3 class="glossary-entry-title" id="u-net-architecture-glossary">U-Net Architecture (for Diffusion Models)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A convolutional neural network architecture characterized by its U-shaped structure, widely used for image segmentation and as the noise prediction network in <a href="#diffusion-model-glossary">diffusion models</a>.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>U-Nets consist of a contracting path (encoder) that captures context and an expansive path (decoder) that enables precise localization. Skip connections link corresponding layers in the contracting and expansive paths, allowing the decoder to use high-resolution features from the encoder, which is crucial for tasks requiring detailed output like image generation or segmentation.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>The U-Net architecture was originally developed for biomedical image segmentation but has found significant success as the core neural network in many state-of-the-art <a href="#diffusion-model-glossary">diffusion models</a> (e.g., in <a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">DDPMs</a>) for predicting the noise (or the denoised image) at each step of the <a href="#backward-diffusion-process-glossary">backward process</a>.
    Key features of a U-Net include:
    <ul>
        <li><strong>Encoder (Contracting Path):</strong> This part consists of a series of convolutional and downsampling (e.g., max pooling) layers. It progressively reduces the spatial dimensions of the input while increasing the number of feature channels, aiming to capture contextual information and learn abstract representations.</li>
        <li><strong>Bottleneck:</strong> A layer with the smallest spatial dimensions and typically the largest number of feature channels, connecting the encoder and decoder.</li>
        <li><strong>Decoder (Expansive Path):</strong> This part consists of a series of convolutional and upsampling (e.g., transposed convolutions or upsampling followed by convolution) layers. It progressively increases the spatial dimensions and aims to reconstruct a detailed output (e.g., a segmentation map or a denoised image).</li>
        <li><strong>Skip Connections:</strong> These are crucial. They concatenate or add feature maps from layers in the contracting path to the corresponding layers in the expansive path (at the same spatial resolution). This allows the decoder to access high-resolution features from the encoder, helping it to recover fine-grained details that might be lost during downsampling. This is particularly important for generating sharp and detailed images in diffusion models.</li>
        <li><strong>Conditioning (in Diffusion Models):</strong> When used in diffusion models, U-Nets are often conditioned on the timestep <code>t</code> of the diffusion process. This conditioning is typically achieved by transforming <code>t</code> into an embedding and adding or concatenating it to intermediate feature maps within the U-Net, allowing the network to adapt its denoising behavior based on the current noise level.</li>
    </ul>
    The U-Net's ability to effectively combine local detail with global context makes it highly suitable for tasks where the output needs to be spatially precise and contextually aware, such as predicting pixel-wise noise in images for diffusion models.</p>

    <h3 class="glossary-entry-title" id="unsupervised-learning-glossary">Unsupervised Learning</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A type of machine learning where algorithms learn patterns and structure from input data that has not been labeled, classified, or categorized.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>In unsupervised learning, models explore the data to find inherent structure (like clusters, <a href="#latent-variable-glossary">latent factors</a>, principal components, or density estimates) without explicit target outputs or supervisory signals provided during training.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Unsupervised learning is one of the main paradigms in machine learning, alongside supervised learning and reinforcement learning. It deals with data that lacks explicit labels or desired outputs. The primary goal is to infer the underlying structure or distribution of the data itself.
    Common tasks and techniques in unsupervised learning include:
    <ul>
        <li><strong>Clustering:</strong> Grouping similar data points together based on some similarity measure. Algorithms include K-Means, DBSCAN, and <a href="#mixture-model-glossary">mixture models</a> like <a href="#gaussian-mixture-model-gmm-glossary">GMMs</a>.</li>
        <li><strong>Dimensionality Reduction:</strong> Reducing the number of features (dimensions) of the data while preserving important information. Techniques include Principal Component Analysis (PCA), t-SNE, and autoencoders (including <a href="#variational-autoencoder-vae-glossary">VAEs</a> when viewed from a representation learning perspective).</li>
        <li><strong>Density Estimation:</strong> Learning the underlying <a href="#probability-distribution-glossary">probability distribution</a> from which the data is sampled. This is a core task for <a href="#generative-model-glossary">generative models</a>. Examples include Kernel Density Estimation (KDE), GMMs, and most deep generative models.</li>
        <li><strong>Association Rule Mining:</strong> Discovering interesting relationships or associations among variables in large datasets (e.g., "customers who buy X also tend to buy Y").</li>
        <li><strong>Anomaly Detection (Outlier Detection):</strong> Identifying data points that are significantly different from the majority of the data.</li>
        <li><strong>Feature Learning/Representation Learning:</strong> Automatically discovering useful representations (features) of the input data. This is a key aspect of deep learning and many generative models.</li>
    </ul>
    <a href="#generative-model-glossary">Generative models</a> like <a href="#variational-autoencoder-vae-glossary">VAEs</a>, GANs, and <a href="#diffusion-model-glossary">diffusion models</a> are primarily unsupervised learning methods, as they learn to model the data distribution <code>P(x)</code> without labels. Unsupervised learning is particularly valuable when labeled data is scarce, expensive, or unavailable, and it forms the foundation for many modern AI applications involving data exploration, pattern discovery, and automated feature engineering.</p>

    <h3 class="glossary-entry-title" id="variational-autoencoder-vae-glossary">Variational Autoencoder (VAE)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>A type of deep <a href="#generative-model-glossary">generative model</a> that learns a probabilistic mapping from input data to a lower-dimensional <a href="#latent-variable-glossary">latent space</a> and a mapping from the latent space back to data space, using <a href="#variational-inference-vi-glossary">variational inference</a> for training.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>VAEs have two main components:
    1. An <strong>encoder</strong> (inference network) <code>q<sub>φ</sub>(z|x)</code> that maps input data <code>x</code> to parameters (e.g., mean and variance) of a <a href="#probability-distribution-glossary">distribution</a> over latent variables <code>z</code> (the approximate <a href="#posterior-distribution-glossary">posterior</a>).
    2. A <strong>decoder</strong> (generative network) <code>p<sub>θ</sub>(x|z)</code> that maps points <code>z</code> from the latent space back to parameters of a distribution over data <code>x</code>.
    They are trained by maximizing the <a href="#evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Variational Autoencoders (VAEs) are a powerful and foundational class of deep <a href="#generative-model-glossary">generative models</a> that leverage principles from <a href="#variational-inference-vi-glossary">variational inference</a> and neural networks.
    The architecture consists of:
    <ul>
        <li><strong>Encoder (Inference Network <code>q<sub>φ</sub>(z|x)</code>):</strong>
            This network takes an input data point <code>x</code> and outputs the parameters (typically mean <code>μ<sub>φ</sub>(x)</code> and log-variance <code>log σ²<sub>φ</sub>(x)</code>) of a chosen variational <a href="#probability-distribution-glossary">distribution</a> over the <a href="#latent-variable-glossary">latent space</a> <code>z</code>. This distribution, <code>q<sub>φ</sub>(z|x)</code>, serves as an approximation to the true (but intractable) <a href="#posterior-distribution-glossary">posterior</a> <code>p(z|x)</code>. Commonly, <code>q<sub>φ</sub>(z|x)</code> is chosen to be a <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian distribution</a>, often using a <a href="#mean-field-approximation-glossary">mean field approximation</a>. The parameters <code>φ</code> are the weights of the encoder neural network.</li>
        <li><strong>Latent Space (<code>z</code>):</strong>
            A lower-dimensional, continuous space intended to capture the underlying factors of variation in the data. During training, samples <code>z</code> are drawn from <code>q<sub>φ</sub>(z|x)</code> using the <a href="#reparameterization-trick-glossary">reparameterization trick</a>. For generating new data, samples <code>z</code> are drawn from a <a href="#prior-distribution-glossary">prior distribution</a> <code>p(z)</code> (usually a standard normal <code>N(0,I)</code>). More complex VAEs might use <a href="#hierarchical-vae-glossary">hierarchical latent spaces</a>.</li>
        <li><strong>Decoder (Generative Network <code>p<sub>θ</sub>(x|z)</code>):</strong>
            This network takes a latent vector <code>z</code> as input and maps it back to the data space. It outputs the parameters of a <a href="#probability-distribution-glossary">distribution</a> over the data <code>x</code> (e.g., means for Gaussian data using <a href="#log-density-of-a-gaussian-glossary">log density of a Gaussian</a>, or probabilities for binary data if using a <a href="#bernoulli-distribution-glossary">Bernoulli likelihood</a>). The parameters <code>θ</code> are the weights of the decoder neural network.</li>
    </ul>
    <strong>Training:</strong> VAEs are trained by maximizing the <a href="#evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</a> with respect to both the encoder parameters <code>φ</code> and decoder parameters <code>θ</code> using methods like <a href="#stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent</a>. The ELBO typically consists of:
    <ol>
        <li>A <strong>reconstruction term</strong> (e.g., expected <a href="#log-likelihood-in-vaes-glossary">log-likelihood</a> <code>E<sub>q<sub>φ</sub>(z|x)</sub>[log p<sub>θ</sub>(x|z)]</code>), which encourages the decoder to accurately reconstruct the input data from its latent representation.</li>
        <li>A <strong>regularization term</strong> (e.g., <code>-D<sub>KL</sub>(q<sub>φ</sub>(z|x) || p(z))</code>), which is the <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a> between the approximate posterior <code>q<sub>φ</sub>(z|x)</code> and the prior <code>p(z)</code>. This term encourages the learned latent space to conform to the prior distribution, making it smooth and allowing for meaningful sampling.</li>
    </ol>
    VAEs are valued for their ability to learn structured latent representations, generate new data, and provide a principled probabilistic framework. They are used in tasks like image generation, data compression, anomaly detection, and drug discovery.</p>

    <h3 class="glossary-entry-title" id="variational-inference-vi-glossary">Variational Inference (VI)</h3>
    <span class="entry-section-title">Quick description:</span>
    <p>An approximate inference technique that converts a Bayesian inference problem (computing a <a href="#posterior-distribution-glossary">posterior distribution</a>) into an optimization problem.</p>
    <span class="entry-section-title">Explanation:</span>
    <p>VI aims to find a distribution (from a chosen simpler, tractable family of distributions, e.g., using a <a href="#mean-field-approximation-glossary">mean field approximation</a>) that is as "close" as possible to the true, often intractable, <a href="#posterior-distribution-glossary">posterior distribution</a> <code>P(z|x)</code>. Closeness is typically measured by minimizing <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a>, which is equivalent to maximizing the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>.</p>
    <span class="entry-section-title">Further Explanation:</span>
    <p>Variational inference is a powerful and widely used method for approximating complex, intractable <a href="#posterior-distribution-glossary">posterior distributions</a> <code>P(z|x)</code> in <a href="#bayesian-inference-glossary">Bayesian models</a>, where <code>z</code> are <a href="#latent-variable-glossary">latent variables</a> (or parameters) and <code>x</code> is observed data. The core idea is:
    <ol>
        <li><strong>Choose a Variational Family:</strong> Select a family of simpler, tractable <a href="#probability-distribution-glossary">probability distributions</a> <code>Q(z; φ)</code>, called the variational family, parameterized by variational parameters <code>φ</code>. Common choices include Gaussian distributions or <a href="#mean-field-approximation-glossary">mean-field families</a> (where <code>Q(z; φ) = Π<sub>i</sub> Q<sub>i</sub>(z<sub>i</sub>; φ<sub>i</sub>)</code>).</li>
        <li><strong>Define an Objective:</strong> Find the member of this family <code>Q(z; φ*)</code> that is "closest" to the true posterior <code>P(z|x)</code>. Closeness is typically measured by the <a href="#kl-divergence-kullback-leibler-divergence-glossary">Kullback-Leibler (KL) divergence</a> from <code>Q</code> to <code>P(z|x)</code>: <code>KL(Q(z; φ) || P(z|x))</code>.</li>
        <li><strong>Optimization:</strong> Minimize this KL divergence with respect to the variational parameters <code>φ</code>. This is equivalent to maximizing the <a href="#evidence-lower-bound-elbo-glossary"><strong>Evidence Lower Bound (ELBO)</strong></a>, <code>L(φ)</code>, because:
        <pre>log P(x) = ELBO(φ) + KL(Q(z; φ) || P(z|x))</pre>
        Since <code>log P(x)</code> is constant with respect to <code>φ</code> and <code>KL ≥ 0</code>, maximizing <code>ELBO(φ)</code> minimizes <code>KL(Q(z; φ) || P(z|x))</code>.</li>
    </ol>
    The ELBO is typically written as:
    <pre>ELBO(φ) = E<sub>Q(z;φ)</sub>[log P(x,z)] - E<sub>Q(z;φ)</sub>[log Q(z;φ)]</pre>
    (The second term is the negative entropy of Q).
    In <a href="#variational-autoencoder-vae-glossary">VAEs</a>, the variational distribution <code>Q(z;φ)</code> is parameterized by an encoder neural network (<code>q<sub>φ</sub>(z|x)</code>), and the model parameters <code>θ</code> of <code>P(x,z) = P(x|z,θ)P(z)</code> are also optimized by maximizing the ELBO using methods like <a href="#stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent</a>. VI often scales better to large datasets and complex models than sampling-based methods like MCMC, making it a cornerstone of modern deep <a href="#generative-model-glossary">generative modeling</a>.</p>


    <h2 class="exercises-title" id="reference-list-title">Reference List (Index of Glossary Terms)</h2>
    <div class="reference-list">
        <ul>
            <li><a href="#backward-diffusion-process-glossary">Backward Diffusion Process (Reverse Process)</a></li>
            <li><a href="#bayesian-inference-glossary">Bayesian Inference</a></li>
            <li><a href="#bernoulli-distribution-glossary">Bernoulli Distribution</a></li>
            <li><a href="#canonical-correlation-analysis-cca-glossary">Canonical Correlation Analysis (CCA)</a></li>
            <li><a href="#categorical-distribution-glossary">Categorical Distribution</a></li>
            <li><a href="#conditional-distribution-glossary">Conditional Distribution</a></li>
            <li><a href="#consensus-distribution-glossary">Consensus Distribution</a></li>
            <li><a href="#consensus-of-dependent-experts-code-glossary">Consensus of Dependent Experts (CoDE)</a></li>
            <li><a href="#cross-modal-generation-glossary">Cross-Modal Generation</a></li>
            <li><a href="#denoising-diffusion-probabilistic-models-ddpm-glossary">Denoising Diffusion Probabilistic Models (DDPM)</a></li>
            <li><a href="#denoising-score-matching-glossary">Denoising Score Matching</a></li>
            <li><a href="#diffusion-model-glossary">Diffusion Model</a></li>
            <li><a href="#diffusion-model-objective-function-glossary">Diffusion Model Objective Function</a></li>
            <li><a href="#evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</a></li>
            <li><a href="#expectation-e-glossary">Expectation (E)</a></li>
            <li><a href="#expectation-maximization-em-algorithm-glossary">Expectation-Maximization (EM) Algorithm</a></li>
            <li><a href="#exponent-exponential-function-glossary">Exponent (Exponential Function)</a></li>
            <li><a href="#forward-diffusion-process-glossary">Forward Diffusion Process</a></li>
            <li><a href="#gaussian-mixture-model-gmm-glossary">Gaussian Mixture Model (GMM)</a></li>
            <li><a href="#generative-model-glossary">Generative Model</a></li>
            <li><a href="#hierarchical-vae-glossary">Hierarchical VAE (Hierarchical Latent Spaces)</a></li>
            <li><a href="#integration-glossary">Integration</a></li>
            <li><a href="#iterative-inference-generation-glossary">Iterative Inference/Generation (Context of Diffusion)</a></li>
            <li><a href="#isotropic-gaussian-glossary">Isotropic Gaussian</a></li>
            <li><a href="#jensens-inequality-glossary">Jensen’s Inequality</a></li>
            <li><a href="#joint-distribution-glossary">Joint Distribution</a></li>
            <li><a href="#joint-posterior-distribution-glossary">Joint Posterior Distribution (in Multimodal Learning)</a></li>
            <li><a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence (Kullback-Leibler Divergence)</a></li>
            <li><a href="#latent-variable-glossary">Latent Variable</a></li>
            <li><a href="#likelihood-glossary">Likelihood</a></li>
            <li><a href="#likelihood-free-objective-function-glossary">Likelihood-Free Objective Function</a></li>
            <li><a href="#log-density-of-a-gaussian-glossary">Log Density of a Gaussian</a></li>
            <li><a href="#log-likelihood-glossary">Log Likelihood</a></li>
            <li><a href="#log-likelihood-in-vaes-glossary">Log Likelihood in VAEs</a></li>
            <li><a href="#logarithm-log-natural-logarithm-ln-glossary">Logarithm (log) & Natural Logarithm (ln)</a></li>
            <li><a href="#marginal-distribution-glossary">Marginal Distribution</a></li>
            <li><a href="#marginal-likelihood-glossary">Marginal Likelihood (Model Evidence)</a></li>
            <li><a href="#markov-process-markov-chain-glossary">Markov Process / Markov Chain</a></li>
            <li><a href="#mean-field-approximation-glossary">Mean Field Approximation (Variational Family)</a></li>
            <li><a href="#mixture-model-glossary">Mixture Model</a></li>
            <li><a href="#mixture-of-experts-moe-glossary">Mixture of Experts (MoE)</a></li>
            <li><a href="#multimodal-learning-glossary">Multimodal Learning</a></li>
            <li><a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian (Normal) Distribution</a></li>
            <li><a href="#n-0-1-standard-normal-distribution-glossary">N(0,1) - Standard Normal Distribution</a></li>
            <li><a href="#nat-glossary">Nat</a></li>
            <li><a href="#noise-schedule-glossary">Noise Schedule (βt, αt, ᾱt)</a></li>
            <li><a href="#objective-function-glossary">Objective Function (Loss Function / Cost Function)</a></li>
            <li><a href="#posterior-distribution-glossary">Posterior Distribution</a></li>
            <li><a href="#precision-in-gaussians-glossary">Precision (in Gaussians)</a></li>
            <li><a href="#prior-distribution-glossary">Prior Distribution</a></li>
            <li><a href="#probability-distribution-glossary">Probability Distribution</a></li>
            <li><a href="#product-of-experts-poe-glossary">Product of Experts (PoE)</a></li>
            <li><a href="#random-variable-glossary">Random Variable</a></li>
            <li><a href="#reparameterization-trick-glossary">Reparameterization Trick</a></li>
            <li><a href="#stochastic-gradient-descent-sgd-glossary">Stochastic Gradient Descent (SGD)</a></li>
            <li><a href="#trace-trick-glossary">Trace Trick</a></li>
            <li><a href="#u-net-architecture-glossary">U-Net Architecture (for Diffusion Models)</a></li>
            <li><a href="#unsupervised-learning-glossary">Unsupervised Learning</a></li>
            <li><a href="#variational-autoencoder-vae-glossary">Variational Autoencoder (VAE)</a></li>
            <li><a href="#variational-inference-vi-glossary">Variational Inference (VI)</a></li>
        </ul>
    </div>

    <h2 class="exercises-title" id="bonus-exercises">Bonus Reinforcement Exercises</h2>
    <ol>
        <li>What does the notation <code>θ* = argmax<sub>θ</sub> f(θ)</code> mean? (See: <a href="#objective-function-glossary">Objective Function</a>)</li>
        <li>If P(A) = 0.6 and P(B) = 0.3, and A and B are independent events, what is P(A and B)? (See: <a href="#joint-distribution-glossary">Joint Distribution</a>, <a href="#probability-distribution-glossary">Probability Distribution</a>)</li>
        <li>Simplify <code>log(a<sup>x</sup> / b<sup>y</sup>)</code> using properties of logarithms. (See: <a href="#logarithm-log-natural-logarithm-ln-glossary">Logarithm</a>)</li>
        <li>In <code>p(x|z, θ)</code>, which symbol(s) typically represent model parameters that are learned? (See: <a href="#likelihood-glossary">Likelihood</a>, <a href="#generative-model-glossary">Generative Model</a>)</li>
        <li>What is the primary purpose of the <a href="#reparameterization-trick-glossary">reparameterization trick</a> in training VAEs? (See also: <a href="#variational-autoencoder-vae-glossary">VAE</a>, <a href="#variational-inference-vi-glossary">Variational Inference</a>)</li>
        <li>If a <a href="#probability-distribution-glossary">distribution</a> <code>p(x)</code> integrates to 1 over its domain, what can we say about <code>p(x)</code>? (See: <a href="#integration-glossary">Integration</a>)</li>
        <li>What is the difference between a probability mass function (PMF) and a probability density function (PDF)? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>, <a href="#random-variable-glossary">Random Variable</a>)</li>
        <li>State Bayes' Theorem relating a prior, likelihood, posterior, and evidence. (See: <a href="#bayesian-inference-glossary">Bayesian Inference</a>, <a href="#prior-distribution-glossary">Prior Distribution</a>, <a href="#likelihood-glossary">Likelihood</a>, <a href="#posterior-distribution-glossary">Posterior Distribution</a>, <a href="#marginal-likelihood-glossary">Marginal Likelihood</a>)</li>
        <li>If E[X] = 5, what is E[2X + 3]? (See: <a href="#expectation-e-glossary">Expectation (E)</a>)</li>
        <li>What does <code>D<sub>KL</sub>(P||Q) = 0</code> imply about <a href="#probability-distribution-glossary">distributions</a> P and Q? (See: <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a>)</li>
        <li>Name one reason why <a href="#log-likelihood-glossary">log-likelihood</a> is often preferred over <a href="#likelihood-glossary">likelihood</a> in model optimization.</li>
        <li>What does "i.i.d." stand for in the context of data samples? (See: <a href="#unsupervised-learning-glossary">Unsupervised Learning</a>, <a href="#likelihood-glossary">Likelihood</a>)</li>
        <li>If <code>z ~ N(0,I)</code>, what is the mean of <code>z</code>? What is its covariance matrix? (See: <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>, <a href="#isotropic-gaussian-glossary">Isotropic Gaussian</a>)</li>
        <li>What is the role of the "decoder" network in a <a href="#variational-autoencoder-vae-glossary">VAE</a>? (See also: <a href="#generative-model-glossary">Generative Model</a>)</li>
        <li>For a concave function <code>f</code> (like log), how does <code>f(E[X])</code> relate to <code>E[f(X)]</code> according to <a href="#jensens-inequality-glossary">Jensen's Inequality</a>?</li>
        <li>What does <code>∝</code> mean in an equation like <code>P(A|B) ∝ P(B|A)P(A)</code>? (See: <a href="#bayesian-inference-glossary">Bayesian Inference</a>)</li>
        <li>What is the range of values for a probability? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>)</li>
        <li>How is <a href="#marginal-distribution-glossary">marginal probability</a> P(A) obtained from a <a href="#joint-distribution-glossary">joint probability</a> P(A,B) if B is discrete?</li>
        <li>What is the purpose of a <a href="#prior-distribution-glossary">prior distribution</a> in <a href="#bayesian-inference-glossary">Bayesian inference</a>?</li>
        <li>If a loss function is L(θ), and we update θ using <code>θ<sub>new</sub> = θ<sub>old</sub> - α∇L(θ)</code>, what is α called? (See: <a href="#objective-function-glossary">Objective Function</a>, <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>)</li>
        <li>True or False: <code>D<sub>KL</sub>(P||Q) = D<sub>KL</sub>(Q||P)</code>. (See: <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a>)</li>
        <li>What does "amortized inference" mean in the context of <a href="#variational-autoencoder-vae-glossary">VAEs</a>? (See: <a href="#variational-inference-vi-glossary">Variational Inference</a>)</li>
        <li>If <code>p(x|z)</code> in a VAE is a Gaussian, what common error measure does maximizing <code>log p(x|z)</code> often correspond to minimizing? (See: <a href="#log-likelihood-in-vaes-glossary">Log Likelihood in VAEs</a>, <a href="#log-density-of-a-gaussian-glossary">Log Density of a Gaussian</a>)</li>
        <li>Name two common <a href="#probability-distribution-glossary">probability distributions</a>.</li>
        <li>What is the <a href="#expectation-e-glossary">expectation</a> of a <a href="#bernoulli-distribution-glossary">Bernoulli random variable</a> with success probability <code>p</code>?</li>
        <li>What is the <a href="#integration-glossary">integral</a> of <code>p(x)dx</code> over the entire domain of x if <code>p(x)</code> is a valid PDF? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>)</li>
        <li>How does the <a href="#evidence-lower-bound-elbo-glossary">Evidence Lower Bound (ELBO)</a> relate to the <a href="#log-likelihood-glossary">log</a> <a href="#marginal-likelihood-glossary">marginal likelihood</a> <code>log p(x)</code>?</li>
        <li>If X and Y are <a href="#random-variable-glossary">random variables</a>, is E[XY] = E[X]E[Y] always true? If not, when is it true? (See: <a href="#expectation-e-glossary">Expectation (E)</a>)</li>
        <li>What does a diagonal covariance matrix imply about the correlation between variables in a <a href="#multivariate-gaussian-normal-distribution-glossary">multivariate Gaussian</a>?</li>
        <li>What mathematical operation turns a product of terms into a sum of terms? (See: <a href="#logarithm-log-natural-logarithm-ln-glossary">Logarithm</a>)</li>
        <li>In the term <code>q<sub>φ</sub>(z|x)</code>, what does the subscript <code>φ</code> represent? (See: <a href="#variational-autoencoder-vae-glossary">VAE</a>, <a href="#variational-inference-vi-glossary">Variational Inference</a>)</li>
        <li>What is "posterior collapse" in <a href="#variational-autoencoder-vae-glossary">VAEs</a>? (See: <a href="#posterior-distribution-glossary">Posterior Distribution</a>, <a href="#latent-variable-glossary">Latent Variable</a>, <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a>)</li>
        <li>What is an <a href="#isotropic-gaussian-glossary">isotropic Gaussian</a> distribution?</li>
        <li>What is the "mode" of a <a href="#probability-distribution-glossary">probability distribution</a>?</li>
        <li>If <code>p(x,z) = p(x|z)p(z)</code>, this is an application of which rule? (See: <a href="#conditional-distribution-glossary">Conditional Distribution</a>, <a href="#joint-distribution-glossary">Joint Distribution</a>)</li>
        <li>What is the derivative of <code>exp(ax)</code> with respect to <code>x</code>? (See: <a href="#exponent-exponential-function-glossary">Exponent</a>)</li>
        <li>Name one common type of regularization used in machine learning. (See: <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> for an example in VAEs where the KL term acts as regularization)</li>
        <li>What is "Monte Carlo estimation"? (See: <a href="#integration-glossary">Integration</a>, <a href="#expectation-e-glossary">Expectation (E)</a>, <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>)</li>
        <li>What type of variable (discrete or continuous) is a <a href="#categorical-distribution-glossary">Categorical distribution</a> used for?</li>
        <li>In the GAN objective, what is the generator <code>G</code> trying to achieve with respect to the discriminator <code>D</code>? (See: <a href="#generative-model-glossary">Generative Model</a>, <a href="#likelihood-free-objective-function-glossary">Likelihood-Free Objective Function</a>)</li>
        <li>What is the "variance" of a <a href="#random-variable-glossary">random variable</a> a measure of? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>)</li>
        <li>What does it mean if the <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL divergence</a> term in the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> is very large?</li>
        <li>If a model has high bias, does it typically underfit or overfit the training data? (See: <a href="#unsupervised-learning-glossary">Unsupervised Learning</a> - for general ML concepts)</li>
        <li>What is the purpose of the <code>t</code> (timestep) parameter in <a href="#diffusion-model-glossary">diffusion models</a>? (See: <a href="#forward-diffusion-process-glossary">Forward Diffusion Process</a>, <a href="#backward-diffusion-process-glossary">Backward Diffusion Process</a>)</li>
        <li>What is the "curse of dimensionality"? (Often relevant for <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a> in high-D, <a href="#generative-model-glossary">Generative Model</a>)</li>
        <li>What is the "likelihood principle" in statistics? (See: <a href="#likelihood-glossary">Likelihood</a>)</li>
        <li>If Y = aX + b, how does Var(Y) relate to Var(X)? (<code>a, b</code> are constants). (See: <a href="#random-variable-glossary">Random Variable</a>, <a href="#expectation-e-glossary">Expectation (E)</a>)</li>
        <li>What is the difference between <a href="#generative-model-glossary">generative</a> and "discriminative" models?</li>
        <li>What is the "support" of a <a href="#probability-distribution-glossary">probability distribution</a>?</li>
        <li>What is "entropy" a measure of in information theory? (See: <a href="#nat-glossary">Nat</a>, <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a>)</li>
        <li>If A and B are mutually exclusive events, what is P(A and B)? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>)</li>
        <li>What does the term <code>p(z)</code> typically represent in the <a href="#variational-autoencoder-vae-glossary">VAE</a> <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>? (See: <a href="#prior-distribution-glossary">Prior Distribution</a>)</li>
        <li>What is the mathematical inverse of the <code>exp()</code> function? (See: <a href="#logarithm-log-natural-logarithm-ln-glossary">Logarithm</a>, <a href="#exponent-exponential-function-glossary">Exponent</a>)</li>
        <li>In <code>N(μ, σ²)</code>, if <code>σ²</code> is very small, what does the distribution look like? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>, <a href="#log-density-of-a-gaussian-glossary">Log Density of a Gaussian</a>)</li>
        <li>What is a "hyperparameter"? (Relevant for any <a href="#generative-model-glossary">Generative Model</a> training)</li>
        <li>What is the "gradient" of a function of multiple variables? (See: <a href="#objective-function-glossary">Objective Function</a>, <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>)</li>
        <li>How is <a href="#precision-in-gaussians-glossary">precision</a> related to "variance" in the context of Gaussian distributions?</li>
        <li>What is a <a href="#latent-variable-glossary">latent variable</a>?</li>
        <li>What is the purpose of a "validation set" in machine learning? (Relevant for training any <a href="#generative-model-glossary">Generative Model</a>)</li>
        <li>What does "overfitting" mean? (Relevant for training any <a href="#generative-model-glossary">Generative Model</a>)</li>
        <li>If a <a href="#random-variable-glossary">random variable</a> X can only take values 0 or 1, what <a href="#probability-distribution-glossary">distribution</a> might model it? (Answer: <a href="#bernoulli-distribution-glossary">Bernoulli Distribution</a>)</li>
        <li>What is a "stationary point" of a function? (See: <a href="#objective-function-glossary">Objective Function</a>)</li>
        <li>In the <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>, what does the term <code>E<sub>q(z|x)</sub>[log p(x|z)]</code> encourage the model to do? (See: <a href="#variational-autoencoder-vae-glossary">VAE</a>)</li>
        <li>What is the sum of probabilities in any discrete <a href="#probability-distribution-glossary">probability distribution</a>?</li>
        <li>What does it mean for two vectors to be "orthogonal"? (Relevant for <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a> if covariance is diagonal)</li>
        <li>What is a "convex function"? (See: <a href="#jensens-inequality-glossary">Jensen’s Inequality</a>)</li>
        <li>What is "Maximum Likelihood Estimation" (MLE)? (See: <a href="#likelihood-glossary">Likelihood</a>, <a href="#log-likelihood-glossary">Log Likelihood</a>)</li>
        <li>What is a <a href="#markov-process-markov-chain-glossary">Markov chain</a>?</li>
        <li>How many parameters are needed to define a univariate <a href="#multivariate-gaussian-normal-distribution-glossary">Gaussian distribution</a>?</li>
        <li>What is the "identity matrix"? What does it look like? (Relevant for <a href="#isotropic-gaussian-glossary">Isotropic Gaussian</a>)</li>
        <li>What is "backpropagation" used for in training neural networks? (Used in <a href="#variational-autoencoder-vae-glossary">VAEs</a>, <a href="#diffusion-model-glossary">Diffusion Models</a>, etc., relying on <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>)</li>
        <li>What does <code>p(x; θ)</code> denote (the semicolon)? (See: <a href="#likelihood-glossary">Likelihood</a>)</li>
        <li>If P(A|B) = P(A), what can be said about events A and B? (See: <a href="#conditional-distribution-glossary">Conditional Distribution</a>)</li>
        <li>What is a <a href="#mixture-model-glossary">mixture model</a>?</li>
        <li>What is the "Jacobian matrix"? (Relevant for <a href="#reparameterization-trick-glossary">Reparameterization Trick</a> with vector transformations, and Normalizing Flows)</li>
        <li>Why might a modeler choose a Gaussian <a href="#prior-distribution-glossary">prior</a> for <a href="#latent-variable-glossary">latent variables</a>? (See also: <a href="#isotropic-gaussian-glossary">Isotropic Gaussian</a>)</li>
        <li>What is the output range of a sigmoid function? (See: <a href="#exponent-exponential-function-glossary">Exponent</a>)</li>
        <li>If a model is "non-parametric," what does that generally imply about its complexity? (See: <a href="#generative-model-glossary">Generative Model</a>)</li>
        <li>What is a <a href="#objective-function-glossary">loss function</a> (or <a href="#objective-function-glossary">objective function</a>) in machine learning?</li>
        <li>What is "annealing" in the context of optimization or model training? (Can be relevant for <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a> term in <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>)</li>
        <li>What is the "softmax" function often used for? (See: <a href="#categorical-distribution-glossary">Categorical Distribution</a>, <a href="#exponent-exponential-function-glossary">Exponent</a>)</li>
        <li>What is the difference between <code>p(x,y)</code> and <code>p(x|y)</code>? (See: <a href="#joint-distribution-glossary">Joint Distribution</a>, <a href="#conditional-distribution-glossary">Conditional Distribution</a>)</li>
        <li>What does <code>∇<sub>θ</sub> L(θ) = 0</code> signify? (See: <a href="#objective-function-glossary">Objective Function</a>)</li>
        <li>What is a "sufficient statistic"? (See: <a href="#likelihood-glossary">Likelihood</a>)</li>
        <li>If you multiply a PDF by a constant c > 0, is it still a valid PDF? Why or why not? (See: <a href="#probability-distribution-glossary">Probability Distribution</a>)</li>
        <li>What is the "covariance" between two <a href="#random-variable-glossary">random variables</a> a measure of? (See: <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>)</li>
        <li>What is "regularization" aiming to prevent in model training? (See: <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>)</li>
        <li>What is the fundamental difference between the <a href="#forward-diffusion-process-glossary">forward</a> and <a href="#backward-diffusion-process-glossary">backward processes</a> in <a href="#diffusion-model-glossary">diffusion models</a>?</li>
        <li>What is the "law of total probability"? (See: <a href="#marginal-distribution-glossary">Marginal Distribution</a>)</li>
        <li>What does it mean for a function to be "invertible"? (Relevant for <a href="#logarithm-log-natural-logarithm-ln-glossary">Logarithm</a>/<a href="#exponent-exponential-function-glossary">Exponent</a>)</li>
        <li>If <code>z = g(ε)</code> where <code>ε</code> is a random variable and <code>g</code> is a deterministic function, how can we sample <code>z</code>? (See: <a href="#reparameterization-trick-glossary">Reparameterization Trick</a>)</li>
        <li>What is the "Fisher Information Matrix"? (Related to <a href="#likelihood-glossary">Likelihood</a>, <a href="#log-likelihood-glossary">Log Likelihood</a>)</li>
        <li>What is a common way to estimate an intractable <a href="#integration-glossary">integral</a> <code>∫ f(x)p(x) dx</code>? (See: <a href="#expectation-e-glossary">Expectation (E)</a>, <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a> for use in optimization)</li>
        <li>In <code>y = Wx + b</code>, what do <code>W</code> and <code>b</code> typically represent in a neural network layer? (Used in encoders/decoders of <a href="#variational-autoencoder-vae-glossary">VAEs</a> etc.)</li>
        <li>What is "mode collapse" in GANs? (See: <a href="#generative-model-glossary">Generative Model</a>, <a href="#likelihood-free-objective-function-glossary">Likelihood-Free Objective Function</a>)</li>
        <li>What is a "conjugate prior" in <a href="#bayesian-inference-glossary">Bayesian statistics</a>? (See: <a href="#prior-distribution-glossary">Prior Distribution</a>, <a href="#posterior-distribution-glossary">Posterior Distribution</a>)</li>
        <li>If <code>log p(x) ≥ L(x)</code>, what is <code>L(x)</code> called relative to <code>log p(x)</code>? (See: <a href="#evidence-lower-bound-elbo-glossary">ELBO</a>, <a href="#marginal-likelihood-glossary">Marginal Likelihood</a>)</li>
        <li>What is the "chain rule" for differentiation? (Used in backpropagation for training any neural net based <a href="#generative-model-glossary">Generative Model</a>, via <a href="#stochastic-gradient-descent-sgd-glossary">SGD</a>)</li>
        <li>What is "empirical risk minimization"? (See: <a href="#objective-function-glossary">Objective Function</a>)</li>
        <li>If an algorithm is iterative, what does that mean? (e.g., <a href="#expectation-maximization-em-algorithm-glossary">EM Algorithm</a>, training of <a href="#diffusion-model-glossary">Diffusion Models</a> see <a href="#iterative-inference-generation-glossary">Iterative Inference/Generation</a>)</li>
        <li>What is the <a href="#trace-trick-glossary">trace trick</a> primarily used for?</li>
        <li>In <a href="#multimodal-learning-glossary">multimodal learning</a>, what does <a href="#product-of-experts-poe-glossary">Product of Experts (PoE)</a> aim to achieve with unimodal posteriors?</li>
        <li>How does a <a href="#mixture-of-experts-moe-glossary">Mixture of Experts (MoE)</a> combine expert distributions differently from PoE?</li>
        <li>What is the main goal of a <a href="#consensus-distribution-glossary">consensus distribution</a> in multimodal VAEs?</li>
        <li>What limitation of PoE does <a href="#consensus-of-dependent-experts-code-glossary">CoDE</a> try to address?</li>
        <li>What kind of relationship does <a href="#canonical-correlation-analysis-cca-glossary">Canonical Correlation Analysis (CCA)</a> find between two sets of variables?</li>
        <li>Give an example of <a href="#cross-modal-generation-glossary">cross-modal generation</a>.</li>
        <li>How is a <a href="#hierarchical-vae-glossary">Hierarchical VAE</a> different from a standard VAE?</li>
        <li>What is a <a href="#likelihood-free-objective-function-glossary">likelihood-free objective function</a>, and why might it be used?</li>
        <!-- Exercises from Slides (19 added, starting from 101) -->
        <li>Given <code>q(x<sub>t</sub>|x<sub>t-1</sub>) = N(√(1-β<sub>t</sub>)x<sub>t-1</sub>, β<sub>t</sub>I)</code> from a <a href="#diffusion-model-glossary">diffusion model</a>'s <a href="#forward-diffusion-process-glossary">forward process</a>, explain why the <a href="#joint-distribution-glossary">joint probability</a> <code>q(x<sub>1:T</sub>|x<sub>0</sub>)</code> can be written as a product <code>Π q(x<sub>t</sub>|x<sub>t-1</sub>)</code>. Which property is being used? (See: <a href="#markov-process-markov-chain-glossary">Markov Process</a>)</li>
        <li>In a <a href="#diffusion-model-glossary">diffusion model</a>, if the variance <code>β<sub>t</sub></code> in the <a href="#noise-schedule-glossary">noise schedule</a> increases with timestep <code>t</code>, what is the intuitive effect on <code>x<sub>t</sub></code>? (See: <a href="#forward-diffusion-process-glossary">Forward Diffusion Process</a>)</li>
        <li>How does the statement "each step depends only on its previous state" in a <a href="#diffusion-model-glossary">diffusion model</a> relate to the concept of a <a href="#hierarchical-vae-glossary">Hierarchical VAE</a>'s inference model?</li>
        <li>In the <a href="#forward-diffusion-process-glossary">forward diffusion process</a>, <code>x<sub>t</sub> ~ N(√(ᾱ<sub>t</sub>)x<sub>0</sub>, (1-ᾱ<sub>t</sub>)I)</code>. If <code>ᾱ<sub>T</sub> → 0</code> (from the <a href="#noise-schedule-glossary">noise schedule</a>), what does this <a href="#probability-distribution-glossary">distribution</a> for <code>x<sub>T</sub></code> approximate? (See: <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>)</li>
        <li>Explain in words what <code>ᾱ<sub>t</sub></code> (from the <a href="#noise-schedule-glossary">noise schedule</a>) represents in the context of the <a href="#forward-diffusion-process-glossary">forward diffusion process</a>.</li>
        <li>Why is it significant that no neural network is trained in the <a href="#forward-diffusion-process-glossary">forward step</a> of a <a href="#diffusion-model-glossary">diffusion model</a>?</li>
        <li>In <a href="#diffusion-model-glossary">diffusion models</a>, the distribution <code>q(x<sub>T</sub>|x<sub>0</sub>)</code> is often said to be like the <a href="#prior-distribution-glossary">prior distribution</a> in <a href="#variational-autoencoder-vae-glossary">VAEs</a>. What common distribution is this referring to? (See: <a href="#isotropic-gaussian-glossary">Isotropic Gaussian</a>)</li>
        <li>Explain the statement from the context of <a href="#diffusion-model-glossary">diffusion models</a>: "The unknown distribution of <code>x<sub>0</sub></code> should be more complex than <code>N(0,I)</code>." Why is this fundamental for <a href="#generative-model-glossary">generative modeling</a>?</li>
        <li>The <a href="#backward-diffusion-process-glossary">backward diffusion process</a> aims to estimate <code>q(x<sub>t-1</sub>|x<sub>t</sub>)</code>. If we write <code>q(x<sub>t-1</sub>|x<sub>t</sub>) = q(x<sub>t</sub>|x<sub>t-1</sub>)q(x<sub>t-1</sub>) / q(x<sub>t</sub>)</code>, what rule is this an application of? (See: <a href="#bayesian-inference-glossary">Bayesian Inference</a>)</li>
        <li>Why does the <a href="#integration-glossary">integral</a> over <code>x<sub>0</sub></code> in the derivation of the true reverse conditional make it intractable for <a href="#diffusion-model-glossary">diffusion models</a>? (See: <a href="#marginal-distribution-glossary">Marginal Distribution</a>, <a href="#marginal-likelihood-glossary">Marginal Likelihood</a>)</li>
        <li>In the <a href="#backward-diffusion-process-glossary">backward diffusion process</a>, <code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> is chosen from what common family of distributions, and what predicts its parameters? (See: <a href="#multivariate-gaussian-normal-distribution-glossary">Multivariate Gaussian</a>)</li>
        <li>What type of neural network architecture is commonly used for <code>f<sub>θ</sub>(·)</code> (the noise/mean predictor) in image-based <a href="#diffusion-model-glossary">diffusion models</a>, and why? (See: <a href="#u-net-architecture-glossary">U-Net Architecture</a>)</li>
        <li>In the <a href="#joint-distribution-glossary">joint probability</a> of the <a href="#backward-diffusion-process-glossary">reverse process</a> <code>p<sub>θ</sub>(x<sub>0:T</sub>) = p(x<sub>T</sub>) Π p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code>, what is the role of <code>p(x<sub>T</sub>)</code>? (See: <a href="#prior-distribution-glossary">Prior Distribution</a>)</li>
        <li>How is the <a href="#iterative-inference-generation-glossary">iterative generative process</a> of a <a href="#diffusion-model-glossary">diffusion model</a> similar to the generative process in a <a href="#hierarchical-vae-glossary">hierarchical VAE</a>?</li>
        <li>The <a href="#diffusion-model-objective-function-glossary">diffusion model objective function</a> (ELBO) often involves <code>log [ numerator / denominator ]</code>. What general mathematical property simplifies this? (See: <a href="#logarithm-log-natural-logarithm-ln-glossary">Logarithm</a>)</li>
        <li>The <a href="#diffusion-model-objective-function-glossary">diffusion ELBO</a> includes an <a href="#expectation-e-glossary">expectation</a> <code>E<sub>q(x₁:T|x₀)}[...]</code>. What does this expectation average over?</li>
        <li>Why is formulating an <a href="#objective-function-glossary">objective</a> as an <a href="#evidence-lower-bound-elbo-glossary">ELBO</a> useful when direct optimization of <code>log p(x<sub>0</sub>)</code> is difficult for <a href="#diffusion-model-glossary">diffusion models</a>? (See: <a href="#log-likelihood-glossary">Log Likelihood</a>)</li>
        <li>How does the <a href="#diffusion-model-objective-function-glossary">diffusion ELBO</a> encourage the learned <a href="#backward-diffusion-process-glossary">reverse process</a> <code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> to "match" the true reverse of the <a href="#forward-diffusion-process-glossary">forward process</a> <code>q(x<sub>t</sub>|x<sub>t-1</sub>)</code>? (See: <a href="#kl-divergence-kullback-leibler-divergence-glossary">KL Divergence</a>)</li>
        <li>Explain the property <code>E<sub>q(x₁:T|x₀)}[f(x<sub>a:b</sub>)] = E<sub>q(x<sub>a:b</sub>|x₀)}[f(x<sub>a:b</sub>)]</code> used in simplifying the <a href="#diffusion-model-objective-function-glossary">diffusion ELBO</a>. (See: <a href="#expectation-e-glossary">Expectation (E)</a>, <a href="#marginal-distribution-glossary">Marginal Distribution</a>)</li>
    </ol>

    <h2 class="exercises-title" id="bonus-exercises-answers">Answers to Bonus Reinforcement Exercises</h2>
    <ol>
        <li><code>θ*</code> is the value of <code>θ</code> that maximizes the function <code>f(θ)</code>.</li>
        <li><code>P(A and B) = P(A) * P(B) = 0.6 * 0.3 = 0.18</code>.</li>
        <li><code>x log(a) - y log(b)</code>.</li>
        <li><code>θ</code> (theta).</li>
        <li>To allow gradients to be backpropagated through the sampling process by making the stochasticity external (dependent on an independent noise variable) to the parameters being optimized.</li>
        <li><code>p(x)</code> is a valid probability density function (PDF).</li>
        <li>A PMF defines probabilities for discrete random variables (values sum to 1); a PDF defines probability density for continuous random variables (area under curve integrates to 1).</li>
        <li><code>P(θ|x) = [P(x|θ) * P(θ)] / P(x)</code>, or Posterior = (Likelihood * Prior) / Evidence.</li>
        <li><code>E[2X + 3] = 2E[X] + 3 = 2*5 + 3 = 13</code>.</li>
        <li>Distributions P and Q are identical (almost everywhere).</li>
        <li>It turns products of probabilities into sums, which is numerically more stable and often mathematically easier for differentiation.</li>
        <li>Independent and Identically Distributed.</li>
        <li>Mean is a vector of zeros; covariance matrix is the Identity matrix <code>I</code>.</li>
        <li>To map latent variables <code>z</code> from the latent space back to the data space, generating samples <code>x</code> (or parameters of a distribution over <code>x</code>).</li>
        <li><code>f(E[X]) ≥ E[f(X)]</code>.</li>
        <li>"Proportional to." It means the left side equals a constant (independent of the variables being related) times the right side.</li>
        <li>Between 0 and 1, inclusive.</li>
        <li><code>P(A) = ∑<sub>b</sub> P(A,B=b)</code> (sum over all possible values <code>b</code> of B).</li>
        <li>To represent our beliefs or assumptions about parameters or latent variables *before* observing any data.</li>
        <li>Learning rate.</li>
        <li>False (KL divergence is not symmetric in general).</li>
        <li>Using a single inference network (encoder) to approximate the posterior for all data points by sharing its parameters, rather than optimizing variational parameters separately for each data point.</li>
        <li>Minimizing a squared reconstruction error (or a weighted squared error if variance is also learned/predicted).</li>
        <li>Gaussian (Normal) distribution, Bernoulli distribution, Uniform distribution, Poisson distribution, Categorical distribution.</li>
        <li><code>p</code>.</li>
        <li>1.</li>
        <li>ELBO is a lower bound on <code>log p(x)</code>; i.e., <code>log p(x) ≥ ELBO</code>.</li>
        <li>Not always true. It's true if X and Y are independent.</li>
        <li>The variables are uncorrelated. (They are independent if the distribution is jointly Gaussian).</li>
        <li>Logarithm.</li>
        <li>The parameters of the inference network (encoder) <code>q</code>.</li>
        <li>A phenomenon where the VAE learns to ignore the latent variable <code>z</code>, making the approximate posterior <code>q(z|x)</code> nearly identical to the prior <code>p(z)</code> for all <code>x</code>. This often results in poor or overly general reconstructions and limited generative capability from the latent space.</li>
        <li>A multivariate Gaussian distribution where all variables have the same variance and are uncorrelated (covariance matrix is <code>σ²I</code>).</li>
        <li>The value(s) of the random variable at which the probability mass function (for discrete) or probability density function (for continuous) is maximized.</li>
        <li>Definition of conditional probability (or chain rule for two variables): <code>P(X,Z) = P(X|Z)P(Z)</code>.</li>
        <li><code>a * exp(ax)</code>.</li>
        <li>L1 regularization (Lasso), L2 regularization (Ridge/Weight Decay), or the KL divergence term in the ELBO.</li>
        <li>Estimating an expectation (often an intractable integral) by drawing samples from the relevant distribution and averaging the function values evaluated at these samples.</li>
        <li>Discrete.</li>
        <li>G tries to generate data <code>G(z)</code> that D classifies as real (i.e., <code>D(G(z))</code> is high), thereby "fooling" the discriminator.</li>
        <li>A measure of the spread or dispersion of its values around the mean.</li>
        <li>The approximate posterior <code>q(z|x)</code> is very different from (or has "strayed" far from) the prior <code>p(z)</code>.</li>
        <li>Underfit.</li>
        <li>It controls the level of noise added in the forward process or guides the denoising step in the backward process. It parameterizes the current state of corruption/denoising.</li>
        <li>Various problems that arise when working with data in high-dimensional spaces, such as data sparsity, difficulty of distance measures, and increased computational complexity.</li>
        <li>The principle that all information in the observed data relevant to the model parameters is contained in the likelihood function.</li>
        <li><code>Var(Y) = Var(aX + b) = a²Var(X)</code>.</li>
        <li>Generative models learn the joint distribution <code>p(x,y)</code> (or <code>p(x)</code>) and can generate new data; discriminative models learn the conditional distribution <code>p(y|x)</code> and focus on predicting <code>y</code> from <code>x</code>.</li>
        <li>The set of all possible values a random variable can take for which its probability (or density) is non-zero.</li>
        <li>A measure of uncertainty, randomness, or "surprise" associated with a random variable or a probability distribution.</li>
        <li><code>P(A and B) = 0</code> (since they cannot both occur).</li>
        <li>The prior distribution over latent variables, <code>P(z)</code>.</li>
        <li><code>log()</code> (natural logarithm).</li>
        <li>Very peaked, narrow, and highly concentrated around the mean <code>μ</code>.</li>
        <li>A parameter of the learning algorithm or model structure that is set before training begins and is not learned from the data directly (e.g., learning rate, number of layers, regularization strength).</li>
        <li>A vector containing all the first-order partial derivatives of the function with respect to each of its variables. It points in the direction of the steepest ascent of the function.</li>
        <li>Precision is the inverse of variance (for univariate Gaussians, <code>τ = 1/σ²</code>) or the inverse of the covariance matrix (for multivariate Gaussians, <code>Λ = Σ<sup>-1</sup></code>).</li>
        <li>A hidden variable that is not directly observed in data but is inferred by a model to explain patterns or structure in the observed variables.</li>
        <li>To tune hyperparameters of the model and to provide an unbiased evaluation of how well the model (fit on the training data) performs on unseen data during development.</li>
        <li>When a model learns the training data too well, including its noise and specific idiosyncrasies, leading to poor performance and generalization on new, unseen data.</li>
        <li>Bernoulli distribution.</li>
        <li>A point where the derivative (or gradient) of the function is zero. It can be a local minimum, local maximum, or a saddle point.</li>
        <li>It encourages the model (specifically, the decoder <code>p(x|z)</code>) to accurately reconstruct the input data <code>x</code> from latent samples <code>z</code> drawn from the approximate posterior <code>q(z|x)</code>.</li>
        <li>1.</li>
        <li>Their dot product is zero (<code>v1 · v2 = 0</code>).</li>
        <li>A function where the line segment connecting any two points on its graph lies above or on the graph. Formally, <code>f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)</code> for <code>0 ≤ λ ≤ 1</code>.</li>
        <li>A method for estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood function (or log-likelihood function) for the observed data.</li>
        <li>A sequence of possible events (states) in which the probability of each event depends only on the state attained in the previous event (Markov property).</li>
        <li>Two: the mean (<code>μ</code>) and the variance (<code>σ²</code>).</li>
        <li>A square matrix with ones on the main diagonal and zeros elsewhere. E.g., for 2x2: <code>[[1,0],[0,1]]</code>.</li>
        <li>To efficiently compute the gradients of the loss function with respect to all the weights (parameters) of the neural network, enabling gradient-based optimization.</li>
        <li>It indicates that <code>p</code> is a probability distribution (or density) of <code>x</code>, which is parameterized by <code>θ</code>. Often used to distinguish variables from parameters when discussing likelihood.</li>
        <li>A and B are independent events.</li>
        <li>A probabilistic model representing a dataset as arising from a combination (or "mixture") of several distinct underlying probability distributions (components).</li>
        <li>A matrix of all first-order partial derivatives of a vector-valued function with respect to another vector. If <code>y = f(x)</code> where <code>x</code> is n-dim and <code>y</code> is m-dim, Jacobian is an m x n matrix.</li>
        <li>It's simple (e.g., N(0,I)), mathematically tractable (KL divergence with another Gaussian often has a closed form), encourages latent codes to be centered and spread out, and acts as a regularizer.</li>
        <li>(0, 1).</li>
        <li>Its complexity (e.g., number of parameters) can grow or adapt based on the amount of data, rather than being fixed beforehand.</li>
        <li>A mathematical function that is optimized (maximized or minimized) during model training to achieve a desired outcome by quantifying model performance or fit.</li>
        <li>A technique where a parameter (like temperature in simulated annealing, or the weight of a regularization term like the KL term in β-VAEs) is gradually changed (usually decreased) during the training or optimization process.</li>
        <li>To convert a vector of K real numbers (logits) into a probability distribution over K possible discrete outcomes, ensuring outputs are between 0 and 1 and sum to 1.</li>
        <li><code>p(x,y)</code> is the joint probability of x and y occurring together. <code>p(x|y)</code> is the conditional probability of x occurring given that y has occurred.</li>
        <li><code>θ</code> is a stationary point (local minimum, maximum, or saddle point) of the loss/objective function <code>L(θ)</code>.</li>
        <li>A function of the sample data that contains all the information in the sample about a particular parameter. Once its value is known, the raw data provides no further information about the parameter.</li>
        <li>Not generally, unless <code>c=1</code>. For a PDF to be valid, its integral over the entire domain must be 1. Multiplying by <code>c</code> would make the integral <code>c</code>. It would need to be re-normalized by dividing by <code>c</code> (if <code>c > 0</code>).</li>
        <li>A measure of the joint variability of two random variables; it indicates the direction and strength of a linear relationship between them.</li>
        <li>Overfitting to the training data, thereby improving generalization to unseen data.</li>
        <li>The forward process is fixed, predefined, and progressively adds noise to data. The backward process is learned by a neural network and aims to reverse the noise addition, generating data from noise.</li>
        <li>It states that the probability of an event can be found by summing (or integrating) its conditional probabilities over a set of mutually exclusive and exhaustive events (a partition of the sample space). <code>P(A) = ∑<sub>i</sub> P(A|B<sub>i</sub>)P(B<sub>i</sub>)</code>.</li>
        <li>A function <code>f</code> is invertible if there exists another function <code>g</code> (its inverse) such that applying <code>f</code> then <code>g</code> (or vice versa) returns the original input (e.g., <code>g(f(x)) = x</code> and <code>f(g(y)) = y</code>).</li>
        <li>First sample <code>ε</code> from its distribution <code>p(ε)</code>, then compute <code>z = g(ε)</code>.</li>
        <li>A matrix that measures the expected curvature of the log-likelihood function with respect to the parameters. It indicates how sensitive the likelihood is to changes in parameters.</li>
        <li>Monte Carlo sampling: draw <code>N</code> samples <code>x<sub>i</sub></code> from <code>p(x)</code> and approximate the integral (which is an expectation) as <code>(1/N) ∑<sub>i</sub> f(x<sub>i</sub>)</code>.</li>
        <li><code>W</code> typically represents a weight matrix, and <code>b</code> represents a bias vector.</li>
        <li>A problem where the GAN generator produces only a very limited variety of samples, collapsing to a few modes (or even a single mode) of the true data distribution, instead of capturing its full diversity.</li>
        <li>In Bayesian statistics, a prior distribution is conjugate to a likelihood function if the resulting posterior distribution is in the same probability distribution family as the prior. This simplifies calculations.</li>
        <li><code>L(x)</code> is called a lower bound for <code>log p(x)</code>.</li>
        <li>A formula for computing the derivative of the composition of two or more functions. If <code>h(x) = f(g(x))</code>, then <code>h'(x) = f'(g(x)) * g'(x)</code>.</li>
        <li>A principle in statistical learning theory which states that one should choose a model (from a hypothesis class) that minimizes the average loss (empirical risk) on the training data.</li>
        <li>The algorithm proceeds in a sequence of steps or stages, where the output or state from one step is used as the input or starting point for the next. This process is repeated until a convergence criterion or a set number of iterations is met.</li>
        <li>To simplify expressions or facilitate gradient calculations involving matrices by allowing cyclic permutation of matrices within a trace operation.</li>
        <li>To form a consensus distribution (approximating the joint posterior) by taking the product of unimodal approximate posteriors derived from each modality, effectively finding where they agree.</li>
        <li>MoE combines experts by taking a weighted average of their distributions, allowing different experts to contribute differently, whereas PoE multiplies them, emphasizing unanimous agreement.</li>
        <li>To approximate the true joint posterior distribution over shared latent variables by fusing information from all available modalities in a robust and scalable way, enabling coherent inference and generation.</li>
        <li>PoE assumes conditional independence of experts given the latent variable, which may not hold in reality. CoDE attempts to model and account for these dependencies.</li>
        <li>Linear relationships, by finding pairs of linear projections (canonical variates) that maximize the correlation between the projected variables from each set.</li>
        <li>Generating a textual caption from an image (image-to-text) or creating an image from a text description (text-to-image).</li>
        <li>It uses multiple layers of latent variables organized in a hierarchy, allowing it to capture data structures and features at different levels of abstraction, compared to a standard VAE which typically has a single latent layer.</li>
        <li>An objective function used to train generative models when the likelihood P(x|θ) is intractable or unknown. It's used because it allows model training by comparing generated and real data distributions or other criteria, without direct likelihood evaluation (e.g., in GANs).</li>
        <!-- Answers for exercises from slides (101-119) -->
        <li>The joint probability can be written as a product due to the Markov property of the forward diffusion process. The chain rule of probability for a Markov chain <code>q(x<sub>1:T</sub>|x<sub>0</sub>) = q(x<sub>T</sub>|x<sub>T-1</sub>)...q(x<sub>2</sub>|x<sub>1</sub>)q(x<sub>1</sub>|x<sub>0</sub>)</code> simplifies because <code>x<sub>0</sub></code> is given (so <code>q(x<sub>1</sub>|x<sub>0</sub>)</code> is the first term in the product starting from <code>t=1</code> if we consider <code>q(x<sub>t</sub>|x<sub>t-1</sub>)</code>).</li>
        <li>As <code>β<sub>t</sub></code> increases, more noise is added at step <code>t</code>, or the "step size" in the noise space becomes larger, leading to faster corruption of the signal <code>x<sub>0</sub></code> and quicker convergence towards a pure noise distribution.</li>
        <li>Both involve a sequence of conditional dependencies forming a directed graphical model. In a hierarchical VAE's inference, a lower-level latent <code>z<sub>i</sub></code> might be inferred based on <code>z<sub>i-1</sub></code> (or data), and <code>z<sub>i+1</sub></code> based on <code>z<sub>i</sub></code>. In diffusion, <code>x<sub>t</sub></code> depends on <code>x<sub>t-1</sub></code>.</li>
        <li>If <code>ᾱ<sub>T</sub> → 0</code>, then <code>√(ᾱ<sub>T</sub>)x<sub>0</sub> → 0</code> and <code>(1-ᾱ<sub>T</sub>)I → I</code>. So, <code>x<sub>T</sub></code> approximates <code>N(0,I)</code>, a standard normal distribution.</li>
        <li><code>ᾱ<sub>t</sub></code> is the cumulative product <code>Π<sup>t</sup><sub>s=1</sub>(1-β<sub>s</sub>)</code>. It represents the proportion of the initial signal variance (or "signal rate") from <code>x<sub>0</sub></code> that remains in <code>x<sub>t</sub></code> after <code>t</code> noising steps. As <code>t</code> increases, <code>ᾱ<sub>t</sub></code> decreases, meaning less of <code>x<sub>0</sub></code> is present.</li>
        <li>It signifies that the forward process is a fixed, predefined data corruption process, not something the model learns. The learning task is entirely focused on the reverse (denoising) process.</li>
        <li>A standard Normal distribution, <code>N(0,I)</code>, which is an isotropic Gaussian.</li>
        <li>Generative modeling aims to learn a mapping from a simple distribution (like <code>N(0,I)</code>) to a complex data distribution (<code>p(x<sub>0</sub>)</code>). If <code>p(x<sub>0</sub>)</code> were also simple like <code>N(0,I)</code>, there would be little or no complex structure to learn or generate.</li>
        <li>This is an application of Bayes' Theorem, where <code>x<sub>t-1</sub></code> is the "parameter/hypothesis" and <code>x<sub>t</sub></code> is the "data/evidence".</li>
        <li>The integral is over all possible original data samples <code>x<sub>0</sub></code>, which come from an unknown and complex distribution <code>p(x<sub>0</sub>)</code>. Marginalizing over this high-dimensional, complex distribution is generally intractable.</li>
        <li><code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> is chosen from the Gaussian family. Its parameters (mean <code>μ<sub>θ</sub></code> and often variance <code>σ²<sub>θ</sub></code>) are predicted by a neural network <code>f<sub>θ</sub>(x<sub>t</sub>, t)</code>.</li>
        <li>A U-Net architecture is commonly used because its encoder-decoder structure with skip connections allows it to capture multi-scale contextual information (important for understanding what to denoise) and preserve high-resolution spatial details (important for generating sharp images).</li>
        <li><code>p(x<sub>T</sub>)</code> is the starting distribution for the reverse generative process. It's typically chosen to be a simple, known noise distribution, usually <code>N(0,I)</code>, from which we can easily sample.</li>
        <li>Both involve generating data through a sequence of conditional steps. In HVAEs, <code>z<sub>L</sub> → z<sub>L-1</sub> → ... → z<sub>1</sub> → x</code>. In diffusion, <code>x<sub>T</sub> → x<sub>T-1</sub> → ... → x<sub>0</sub></code>. Each step refines the representation/sample based on the previous one.</li>
        <li>The property of logarithms: <code>log(A/B) = log(A) - log(B)</code>.</li>
        <li>It averages over all possible noising trajectories <code>x<sub>1</sub>, ..., x<sub>T</sub></code> that could be generated from a given <code>x<sub>0</sub></code> by the forward diffusion process <code>q</code>.</li>
        <li>The ELBO is a tractable lower bound on the intractable log likelihood. By maximizing the ELBO, we indirectly maximize the log likelihood. It also converts an integration problem (for marginal likelihood) into an optimization problem.</li>
        <li>The ELBO for diffusion models can be decomposed into terms that effectively minimize the KL divergence between the learned reverse conditional <code>p<sub>θ</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code> and the true (but dependent on <code>x<sub>0</sub></code>) reverse conditional <code>q(x<sub>t-1</sub>|x<sub>t</sub>, x<sub>0</sub>)</code> at each step <code>t</code>. This forces the learned process to approximate the true reversal of the noise.</li>
        <li>This property is based on the rules of expectation and marginalization. If a function <code>f</code> only depends on a subset of random variables (<code>x<sub>a:b</sub></code>), then when taking an expectation over a larger set of variables (<code>x<sub>1:T</sub></code>) that includes this subset, the variables not in <code>f</code> can be integrated (marginalized) out first without affecting <code>f</code>. The expectation then effectively becomes only over the variables that <code>f</code> depends on, conditioned on any shared dependencies (like <code>x<sub>0</sub></code>).</li>
    </ol>
</div> <!-- End of main-container -->
</body>
</html>
